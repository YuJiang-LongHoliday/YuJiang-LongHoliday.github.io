<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>使用消逝波辅助暗场显微镜对亚波长SiO2纳米粒子阵列进行无标记成像</title>
      <link href="/2025/03/14/%E4%BD%BF%E7%94%A8%E6%B6%88%E9%80%9D%E6%B3%A2%E8%BE%85%E5%8A%A9%E6%9A%97%E5%9C%BA%E6%98%BE%E5%BE%AE%E9%95%9C%E5%AF%B9%E4%BA%9A%E6%B3%A2%E9%95%BFSiO2%E7%BA%B3%E7%B1%B3%E7%B2%92%E5%AD%90%E9%98%B5%E5%88%97%E8%BF%9B%E8%A1%8C%E6%97%A0%E6%A0%87%E8%AE%B0%E6%88%90%E5%83%8F/"/>
      <url>/2025/03/14/%E4%BD%BF%E7%94%A8%E6%B6%88%E9%80%9D%E6%B3%A2%E8%BE%85%E5%8A%A9%E6%9A%97%E5%9C%BA%E6%98%BE%E5%BE%AE%E9%95%9C%E5%AF%B9%E4%BA%9A%E6%B3%A2%E9%95%BFSiO2%E7%BA%B3%E7%B1%B3%E7%B2%92%E5%AD%90%E9%98%B5%E5%88%97%E8%BF%9B%E8%A1%8C%E6%97%A0%E6%A0%87%E8%AE%B0%E6%88%90%E5%83%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>提出了表面等离子体激元（SPP）辅助暗场显微镜，该显微镜将传统的暗场光学显微镜与SPP激发产生的渐逝场<span style="color: red;">（SPP辅助照明基板）</span>相结合，以照亮和调制样品的散射方向。研究发现，放置在石英玻璃基板上的纳米粒子阵列的信号主要是后向散射，前向散射较弱。此外，SPP辅助照明基板增加了前向散射信号，并将远场散射方向从较大的散射方向（∼50°）调整到较小的方向（∀30°），样品的前向散射信息可以有效地进入物镜的接收角，从而提高了成像对比度和分辨率极限。</li></ol><h2 id="放置在基材上的纳米粒子散射的光示意图"><a href="#放置在基材上的纳米粒子散射的光示意图" class="headerlink" title="放置在基材上的纳米粒子散射的光示意图"></a>放置在基材上的纳米粒子散射的光示意图</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202503141041538.png"></p><ol><li>放置在基材上的纳米粒子散射的光示意图。（a） 石英玻璃基板。（b） 涂有50nm厚Ag膜的石英玻璃基板。（c） SPP辅助照明基板。</li></ol><h2 id="不同基板上的独立纳米粒子的远场散射图案"><a href="#不同基板上的独立纳米粒子的远场散射图案" class="headerlink" title="不同基板上的独立纳米粒子的远场散射图案"></a>不同基板上的独立纳米粒子的远场散射图案</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202503141049066.png"></p><ol><li>在S和P偏振入射光下放置在不同基板上的独立纳米粒子的远场散射图案。（a） 石英玻璃基板。（b） 镀银石英玻璃基板。（c） SPP辅助照明基板。</li></ol><h2 id="不同基板上的SiO2纳米粒子阵列中单个纳米粒子的远场散射图案"><a href="#不同基板上的SiO2纳米粒子阵列中单个纳米粒子的远场散射图案" class="headerlink" title="不同基板上的SiO2纳米粒子阵列中单个纳米粒子的远场散射图案"></a>不同基板上的SiO2纳米粒子阵列中单个纳米粒子的远场散射图案</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202503141058788.png"></p><ol><li>在S和P偏振入射光下，组装在不同基板上的SiO2纳米粒子阵列中单个纳米粒子的远场散射图案。（a） 石英玻璃基板。（b） 镀银石英玻璃基板。（c） SPP辅助照明基板。</li></ol><h2 id="实验装置图"><a href="#实验装置图" class="headerlink" title="实验装置图"></a>实验装置图</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202503141101729.png"></p><ol><li>（a） 实验装置的图片。（b） 实验装置示意图。（c） 从基板底部测量和模拟的反射率。（d） （d-1）SPP辅助照明基板的顶面和（d-2）底面。（d-3）300 nm直径SiO2纳米粒子阵列和（d-4）独立纳米粒子的SEM图像</li></ol><h2 id="不同基底上的直径为300nm的纳米颗粒的图像"><a href="#不同基底上的直径为300nm的纳米颗粒的图像" class="headerlink" title="不同基底上的直径为300nm的纳米颗粒的图像"></a>不同基底上的直径为300nm的纳米颗粒的图像</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202503141103715.png"></p><ol><li>放置在不同基底上的直径为300nm的纳米颗粒的图像。（a） 石英玻璃基板。（b） 镀银石英玻璃基板。（c） SPP辅助照明基板。（d−f）沿相应（a−c）放大区域中线的归一化强度分布；（（a1）和（a2）、（b1）和（b2）以及（c1）和（c2）分别放大了（a-c）中的区域）</li></ol><h2 id="点扩散函数（PSF）用于量化成像系统的分辨率"><a href="#点扩散函数（PSF）用于量化成像系统的分辨率" class="headerlink" title="点扩散函数（PSF）用于量化成像系统的分辨率"></a>点扩散函数（PSF）用于量化成像系统的分辨率</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202503141115096.png"></p><ol><li>根据瑞利准则，具有物镜（100×，0.9 NA）的暗场显微镜的分辨率为0.61λ&#x2F;NA&#x3D;366 nm。我们通过传统的暗场显微镜直接观察了100 nm荧光（FL）纳米粒子，并将纳米粒子放置在三个不同的基底表面上，如图S1所示。点扩散函数（PSF）用于量化成像系统1的分辨率。图S1（b）是通过将FL纳米粒子的图像与卷积运算相结合而获得的成像系统的PSF。此时，具有不同基板的显微镜系统的分辨率分别约为363、318和272 nm。实验值与理论计算值一致。由于衍射极限，传统暗场显微镜的空间分辨率约为363nm。基板上的高频渐逝波提高了空间分辨率。</li></ol><h2 id="纳米粒子阵列图像的二维傅里叶光谱变换"><a href="#纳米粒子阵列图像的二维傅里叶光谱变换" class="headerlink" title="纳米粒子阵列图像的二维傅里叶光谱变换"></a>纳米粒子阵列图像的二维傅里叶光谱变换</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202503141117268.png"></p><ol><li>通过2D傅里叶变换获得的频谱是图像的频率分布。我们选择纳米粒子阵列的相同部分图像并进行二维傅里叶变换，如图S2所示。光谱中的亮点越多，表明图像包含更多的高频信息。对于SPP辅助照明基板的情况，纳米粒子阵列图像的空间频率更强。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Label-Free Imaging of Subwavelength SiO2 Nanoparticle Arrays Using Evanescent Wave-Assisted Dark-Field Microscopy</p><p>期刊：The Journal of Physical Chemistry C</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>Wang, Dong, et al. “Label-Free Imaging of Subwavelength Sio2 Nanoparticle Arrays Using Evanescent Wave-Assisted Dark-Field Microscopy.” The Journal of Physical Chemistry C 127.13 (2023): 6524-30. Print.</li></ul><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：D. Wang, S. Yang, M. Qi, Y. Cao, M. Zhang and Y.-H. Ye</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2023年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2025年3月14日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：无</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 衬底 </tag>
            
            <tag> 倏逝波照明 </tag>
            
            <tag> 暗场成像 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>抑制高阶衍射的二维六角形孔光栅</title>
      <link href="/2025/03/05/%E6%8A%91%E5%88%B6%E9%AB%98%E9%98%B6%E8%A1%8D%E5%B0%84%E7%9A%84%E4%BA%8C%E7%BB%B4%E5%85%AD%E8%A7%92%E5%BD%A2%E5%AD%94%E5%85%89%E6%A0%85/"/>
      <url>/2025/03/05/%E6%8A%91%E5%88%B6%E9%AB%98%E9%98%B6%E8%A1%8D%E5%B0%84%E7%9A%84%E4%BA%8C%E7%BB%B4%E5%85%AD%E8%A7%92%E5%BD%A2%E5%AD%94%E5%85%89%E6%A0%85/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>提出了一种由六边形孔组成的二维光栅，它可以完全抑制第二，第三和第四阶衍射。我们的光栅的六边形孔比正弦孔更容易制作[18，19]。</li><li>首先，我们从理论上分析了二维光栅的衍射特性与结构参数的关系，六角孔的特殊尺寸导致了我们所期望的衍射图样。</li><li>然后，我们从理论和实验上证明了二维光栅的衍射图样具有有效的抑制高阶衍射。</li></ol><h2 id="二维六角形孔光栅的理论分析和结构设计"><a href="#二维六角形孔光栅的理论分析和结构设计" class="headerlink" title="二维六角形孔光栅的理论分析和结构设计"></a>二维六角形孔光栅的理论分析和结构设计</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202503051054195.png"></p><ol><li>二维光栅由六边形孔组成。六边形孔沿x轴的边沿着为2a1，沿x轴的对角线沿着为2a，沿y轴的高度沿着为2b。（a）周期为Px和Py的正方形阵列。（B）周期为2Px和Py的三角形阵列。（c）光栅平面和衍射平面的坐标系。</li></ol><p>对于包含大量相同且类似取向的孔的膜，夫琅和费衍射图案中的光分布由下式给出：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202503051111216.png"></p><h3 id="六边形孔的正方形阵列的衍射强度"><a href="#六边形孔的正方形阵列的衍射强度" class="headerlink" title="六边形孔的正方形阵列的衍射强度"></a>六边形孔的正方形阵列的衍射强度</h3><p>如图1（a）所示的六边形孔的正方形阵列，衍射强度图案：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202503051110285.png"></p><p>Io是衍射图案的峰值辐照度。</p><h2 id="m级衍射强度与六边形形状的关系"><a href="#m级衍射强度与六边形形状的关系" class="headerlink" title="m级衍射强度与六边形形状的关系"></a>m级衍射强度与六边形形状的关系</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202503060956468.png"></p><h2 id="正方形六角形孔阵列的远场衍射强度图"><a href="#正方形六角形孔阵列的远场衍射强度图" class="headerlink" title="正方形六角形孔阵列的远场衍射强度图"></a>正方形六角形孔阵列的远场衍射强度图</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202503060957709.png"></p><h2 id="正方形六角形孔阵列的远场衍射强度图-1"><a href="#正方形六角形孔阵列的远场衍射强度图-1" class="headerlink" title="正方形六角形孔阵列的远场衍射强度图"></a>正方形六角形孔阵列的远场衍射强度图</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202503061001973.png"></p><ol><li>(b)与(a)相同，除了三角形数组。(c)正方形六角形孔阵列的0级和1级衍射的强度分布。(d)与（c）相同，除了三角形数组。(e)正方形阵列沿光轴沿着的衍射强度。(f)与（c）相同，除了三角形数组。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="用傅里叶光学来解释光栅衍射"><a href="#用傅里叶光学来解释光栅衍射" class="headerlink" title="用傅里叶光学来解释光栅衍射"></a>用傅里叶光学来解释光栅衍射</h2><p><span style="color: red;">衍射场的复振幅是孔径函数的傅里叶变换。光强则是复振幅的模平方。</span></p><ol><li>光栅的衍射现象可以用傅里叶光学来解释。当光通过光栅时，光栅的结构相当于一个周期性变化的透射函数，这会使得入射光波前被调制，产生不同级次的衍射光束。根据傅里叶变换的性质，周期性结构会生成离散的衍射级次，每个级次对应不同的空间频率成分。</li><li>在光学中，空间频率通常指的是物体结构在空间上的变化频率，高空间频率对应细节或边缘信息，低空间频率则对应整体或缓慢变化的结构。光栅本身的周期结构决定了其空间频率，衍射级次越高，对应的空间频率也越高，因为它们反映了更精细的结构变化。</li></ol><h2 id="倏逝波"><a href="#倏逝波" class="headerlink" title="倏逝波"></a>倏逝波</h2><ol><li>传统光学显微镜的分辨率受到阿贝极限的限制，大约为波长的一半，这是因为传统显微镜只能收集传播波（propagating waves），而丢失了携带高空间频率信息的倏逝波（evanescent waves）。倏逝波的强度随着距离迅速衰减，所以通常只能在物体表面附近探测到，比如在近场扫描光学显微镜（NSOM）中。但NSOM需要物理上接近样品，扫描速度慢，应用受限。</li></ol><h2 id="高阶衍射的不同利用场景"><a href="#高阶衍射的不同利用场景" class="headerlink" title="高阶衍射的不同利用场景"></a>高阶衍射的不同利用场景</h2><p>高阶衍射确实携带更高空间频率的信息，但实际是否“有用”取决于应用场景：</p><ul><li><strong>需要抑制时</strong>：避免噪声、提高能量利用率（如激光、光谱分析）。</li><li><strong>需要利用时</strong>：提取超分辨细节或增强传感灵敏度。</li></ul><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><ol><li>近年来，光子筛因其新颖的特性，如超分辨聚焦和渐逝区以外的成像而引起了人们的极大关注[21-26]。光子筛的核心思想是通过设计孔的位置来产生相长或相消干涉。在这一点上，光子筛类似于具有槽位调制的单级衍射光栅。</li></ol><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Two-dimensional gratings of hexagonal holes for high order diffraction suppression</p><p>抑制高阶衍射的二维六角形孔光栅</p><p>期刊：Optics Express</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>Liu, Ziwei, et al. “Two-Dimensional Gratings of Hexagonal Holes for High Order Diffraction Suppression.” Optics Express 25.2 (2017). Print.</li></ul><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Z. Liu, L. Shi, T. Pu, H. Li, J. Niu, G. Wang, et al.</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2017年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2025年3月5日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：无</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 光栅 </tag>
            
            <tag> 高阶衍射 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>简单孔的衍射图样</title>
      <link href="/2025/03/03/%E7%AE%80%E5%8D%95%E5%AD%94%E7%9A%84%E8%A1%8D%E5%B0%84%E5%9B%BE%E6%A0%B7/"/>
      <url>/2025/03/03/%E7%AE%80%E5%8D%95%E5%AD%94%E7%9A%84%E8%A1%8D%E5%B0%84%E5%9B%BE%E6%A0%B7/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>光的衍射分为菲涅尔衍射(近场衍射)和夫琅禾费衍射(远场衍射)。</li><li>夫琅禾费衍射是远场衍射，通常使用傅里叶变换的方法来计算。</li><li>夫琅禾费衍射中，<span style="color: red;">衍射场的复振幅是孔径函数的傅里叶变换。光强则是复振幅的模平方。</span></li><li>相位型光栅通常有更高的衍射效率，因为不吸收光能，而是通过相位延迟来调制光波。而振幅型光栅会有吸收，导致能量损失。因此，传递函数在相位型光栅中可能表现出不同的特性，比如更高的高频成分传递能力。</li><li>透过率函数t(x) &#x3D; exp(iΔφ sin(2πf x))，根据贝塞尔函数展开，可以分解为不同衍射级的复振幅，每个级次的振幅由贝塞尔函数决定。</li></ol><h2 id="夫琅禾费衍射实验原理"><a href="#夫琅禾费衍射实验原理" class="headerlink" title="夫琅禾费衍射实验原理"></a>夫琅禾费衍射实验原理</h2><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202503031040664.png" style="zoom:67%;"><h2 id="正三角形和正六边形衍射光强分布的异同"><a href="#正三角形和正六边形衍射光强分布的异同" class="headerlink" title="正三角形和正六边形衍射光强分布的异同"></a>正三角形和正六边形衍射光强分布的异同</h2><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202503031043843.png" style="zoom:67%;"><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202503031044125.png" style="zoom:67%;"><h2 id="不同三角形和四边形孔在屏上的衍射图样"><a href="#不同三角形和四边形孔在屏上的衍射图样" class="headerlink" title="不同三角形和四边形孔在屏上的衍射图样"></a>不同三角形和四边形孔在屏上的衍射图样</h2><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202503031542400.png" style="zoom:67%;"><h2 id="不同六边形孔的Fraunhofer衍射图样"><a href="#不同六边形孔的Fraunhofer衍射图样" class="headerlink" title="不同六边形孔的Fraunhofer衍射图样"></a>不同六边形孔的Fraunhofer衍射图样</h2><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202503040932270.png" style="zoom:67%;"><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="参考的中文文献："><a href="#参考的中文文献：" class="headerlink" title="参考的中文文献："></a>参考的中文文献：</h2><ol><li>对称六边形孔的Fraunhofer衍射图样模拟。</li></ol><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Diffraction patterns of simple apertures</p><p>简单孔的衍射图样</p><p>期刊：Journal of the Optical Society of America</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>Smith, Richard C., and James S. Marsh. “Diffraction Patterns of Simple Apertures.” Journal of the Optical Society of America 64.6 (1974): 798-803. Print.</li></ul><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：R. C. Smith and J. S. Marsh</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：1974年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2025年3月3日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：无</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 光栅 </tag>
            
            <tag> 高阶衍射 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>结构参数对准周期光栅高阶衍射抑制的影响</title>
      <link href="/2025/02/25/%E7%BB%93%E6%9E%84%E5%8F%82%E6%95%B0%E5%AF%B9%E5%87%86%E5%91%A8%E6%9C%9F%E5%85%89%E6%A0%85%E9%AB%98%E9%98%B6%E8%A1%8D%E5%B0%84%E6%8A%91%E5%88%B6%E7%9A%84%E5%BD%B1%E5%93%8D/"/>
      <url>/2025/02/25/%E7%BB%93%E6%9E%84%E5%8F%82%E6%95%B0%E5%AF%B9%E5%87%86%E5%91%A8%E6%9C%9F%E5%85%89%E6%A0%85%E9%AB%98%E9%98%B6%E8%A1%8D%E5%B0%84%E6%8A%91%E5%88%B6%E7%9A%84%E5%BD%B1%E5%93%8D/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>研究了孔洞形状（圆形、矩形、菱形和六边形）和孔洞位置分布对衍射性能的影响，证明了六边形孔的准三角形阵列对抑制高阶衍射非常有利。</li><li>传统的光栅表现为多级衍射，在实际的宽带应用中，高阶衍射总是与一阶衍射重叠。因此，需要复杂的展开过程来过滤杂散光。</li><li>抑制高阶衍射的另一种方法是将光栅周期减小到波长大小。据报道，在可见光波段可获得高达100%的负一阶衍射效率。</li><li>由于衍射光栅的光学特性是孔的形状和空间分布的协同作用的结果。</li><li>具有特殊形状的准周期性分布孔可以实现更多的自由度（孔的空间位置和几何形状），从而实现复杂的功能，这是传统光栅在几何控制上有限的周期性特征所无法实现的。具体地说，可以通过具有特定分布和特定孔形状的准周期阵列的光的相消干涉来实现对高阶衍射的抑制。同时，不同孔径光的相消干涉可以提高一阶衍射效率。</li><li>首先，我们研究了孔的形状和位置分布对衍射特性的影响。</li><li>然后，我们解析地证明了准三角形六方孔阵列完全抑制了二阶、三阶、四阶、五阶和六阶衍射。与一阶衍射相比，七阶衍射较小。</li><li>我们的结构在微米尺度周期内的衍射图被实验呈现，并证明了二阶、三阶、四阶、五阶和六阶衍射完全被抑制，这与理论预测和模拟结果在质量上是一致的。我们还比较了我们的准周期光栅和传统的准周期光栅的衍射模式和高阶衍射的抑制效果。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="黑白光栅与正弦光栅"><a href="#黑白光栅与正弦光栅" class="headerlink" title="黑白光栅与正弦光栅"></a>黑白光栅与正弦光栅</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202502251043085.png"></p><h2 id="振幅型与相位型正弦光栅"><a href="#振幅型与相位型正弦光栅" class="headerlink" title="振幅型与相位型正弦光栅"></a>振幅型与相位型正弦光栅</h2><ul><li><p>振幅型：光栅的透过率函数符合一个正弦sin函数的分布。</p></li><li><p>相位型：光栅对相位的调制符合一个正弦sin函数的分布。</p></li><li><p>相位型光栅通常有更高的衍射效率，因为不吸收光能，而是通过相位延迟来调制光波。而振幅型光栅会有吸收，导致能量损失。因此，传递函数在相位型光栅中可能表现出不同的特性，比如更高的高频成分传递能力。</p></li></ul><h2 id="准三角阵列光栅示意图"><a href="#准三角阵列光栅示意图" class="headerlink" title="准三角阵列光栅示意图"></a>准三角阵列光栅示意图</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202502271002623.png"></p><ol><li><p>(a)准三角形阵列光栅示意图。</p><p>(b)光栅平面和观察平面的坐标系。</p></li></ol><h2 id="孔的形状对衍射图样的影响"><a href="#孔的形状对衍射图样的影响" class="headerlink" title="孔的形状对衍射图样的影响"></a>孔的形状对衍射图样的影响</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202502271027902.png"></p><h2 id="实验装置"><a href="#实验装置" class="headerlink" title="实验装置"></a>实验装置</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202502271104887.png"></p><h2 id="光栅制作工艺"><a href="#光栅制作工艺" class="headerlink" title="光栅制作工艺"></a>光栅制作工艺</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202502271403976.png"></p><h2 id="六角形孔的准三角形阵列的衍射图样"><a href="#六角形孔的准三角形阵列的衍射图样" class="headerlink" title="六角形孔的准三角形阵列的衍射图样"></a>六角形孔的准三角形阵列的衍射图样</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202502271405563.png"></p><ol><li><p>(a)具有六边形孔的准三角形阵列的远场衍射强度图案。（b）沿着光轴的衍射强度。</p><p>插图：0级和1级衍射的强度分布。</p></li><li><p>图6（b）中的插图清楚地显示了沿光轴沿着的光强分布，仅存在0级和1级衍射，1级衍射效率为24.65%，与理论值24.26%略有差异，同时，高阶衍射已全部消失。</p></li></ol><h2 id="传统1∶1光栅的远场衍射强度图"><a href="#传统1∶1光栅的远场衍射强度图" class="headerlink" title="传统1∶1光栅的远场衍射强度图"></a>传统1∶1光栅的远场衍射强度图</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202502271410441.png"></p><ol><li>(a)传统1∶1光栅的远场衍射强度图。(b)衍射强度沿着光轴。插图：0级、1级、3级和5级衍射的强度分布。</li><li>为了比较，我们还制作并测量了一个周期为24 μm的传统1∶1光栅，其衍射特性如图7所示。此外，图7（B）中的插图清楚地显示了0、1、3和5阶衍射的强度分布。3和5阶衍射清晰可见，如图7所示。</li></ol><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><ul><li>论文中关于衍射的公式可以引用。</li></ul><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Effects of structure parameters on high-order diffraction suppression of quasi-periodic gratings</p><p>结构参数对准周期光栅高阶衍射抑制的影响</p><p>期刊：Journal of the Optical Society of America B （三区）</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>Pu, Tanchao, et al. “Effects of Structure Parameters on High-Order Diffraction Suppression of Quasi-Periodic Gratings.” Journal of the Optical Society of America B 35.4 (2018). Print.</li></ul><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：TANCHAO PU，ZIWEI LIU，LINA SHI，GUANYA WANG，JIEBIN NIU，AND CHANGQING XIE</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2018年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2025年2月25日-2025年2月27日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：无</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 光栅 </tag>
            
            <tag> 高阶衍射 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习辅助等离子体暗场显微镜超分辨率无标记成像</title>
      <link href="/2024/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BE%85%E5%8A%A9%E7%AD%89%E7%A6%BB%E5%AD%90%E4%BD%93%E6%9A%97%E5%9C%BA%E6%98%BE%E5%BE%AE%E9%95%9C%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%97%A0%E6%A0%87%E8%AE%B0%E6%88%90%E5%83%8F/"/>
      <url>/2024/12/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BE%85%E5%8A%A9%E7%AD%89%E7%A6%BB%E5%AD%90%E4%BD%93%E6%9A%97%E5%9C%BA%E6%98%BE%E5%BE%AE%E9%95%9C%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%97%A0%E6%A0%87%E8%AE%B0%E6%88%90%E5%83%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>制作了一个设计的PDF衬底，<span style="color: red;">表面等离子激元（SPPs）</span>照亮衬底上的样品。基于所设计的衬底和检测光学元件的参数，通过预训练的卷积神经网络（CNN）对样品散射光形成的暗场图像进行进一步处理。我们演示了在各种无标签对象上的分辨率提高了2.8倍。</li><li>在<span style="color: red;">离子体暗场显微镜（DAPD）</span>中，设计的等离子体衬底用于产生具有大波矢量的SPPs作为暗场照明。样品在衬底上散射的SPPs成为远场可检测的，并形成衍射受限的暗场图像，其中包含物体上的高k信息。利用预训练的神经网络对衍射受限等离子体暗场图像进行处理，得到超分辨暗场图像。</li><li>在传统的DFM中，斜向暗场照明光产生一个暗场环，不能进入物镜。物镜只收集来自物体的散射光束以形成无背景图像。因此，物体平面上的照明波矢量略大于检测数值孔径（NA），但始终小于k0，即照明光的自由空间波矢量。</li></ol><h1 id="DAPD工作原理示意图"><a href="#DAPD工作原理示意图" class="headerlink" title="DAPD工作原理示意图"></a>DAPD工作原理示意图</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412131001768.png"></p><ol><li>由于光照波矢量的限制，分辨率的提高被限制在2倍。</li><li>如图1a所示，DAPD基板由2层组成，包括发光层和SPP模式支撑层。该发光层（200 nm厚）由透明聚合物与荧光染料混合而成。聚合物中受激发的荧光染料在发射波长处直接耦合到金膜的SPP模式，在每个可能的平面方向上都有SPP k向量金膜上耦合的SPP起到照明作用，只有样品在倏逝波范围内散射，物镜才能采集到SPP。因此，所获得的图像中背景仍然是暗的，这使其成为暗场图像重要的是，在等离子体暗场衬底下，光波矢量由SPP模式的k矢量决定，k矢量总是大于自由空间k0。由于在远场中无法直接检测到SPP波，因此无论在检测中使用何种物镜，该照明方案都确保了干净的暗场背景。</li><li>DAPD工作原理示意图。(a) DAPD衬底示意图。投射在基片上的激光激发了荧光发射层（PMMA中的R6G）。R6G发射光在顶金表面耦合成SPP模式。在Au薄膜顶部的样品被spp照射。散射光被物镜检测并形成暗场图像。(b) SPP照明光、自由空间k向量（k0）、衍射极限检测范围与实际采集到的物体信息之间的傅里叶空间关系。(c)以单帧低分辨率PDF图像作为输入，通过预训练CNN重构超分辨率dpd图像。</li><li>在DAPD中，更高的空间频率信息被编码在衍射受限的图像中。与合成孔径法类似，高k光照SPP将物体上的高空间频率信息转移到有限的检测带宽中。如图1b所示，傅里叶空间中的SPP模式是一个k向量高于物镜检测距离和自由空间k0的环。在衍射极限暗场图像中检测和编码的对象信息是傅里叶空间中的甜甜圈，如图1b所示。在傅里叶空间中，照明、成像系统检测和被检测物体信息之间的关系可以用数学形式表示如下公式，式中，Od为被检测目标信息的傅里叶谱，ISPP为照明SPP模式的傅里叶谱，H为成像系统的相干传递函数。因此，将成像的有效数值孔径（NAeff）定义为NAeff &#x3D; NAobj + NASPP，其中NAobj为物镜的数值孔径，NASPP为照明SPP模式的数值孔径。因此，DAPD原始图像比正常的暗场图像包含更高分辨率的物体信息。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412131010840.png"></p><h1 id="等离子体暗场衬底设计"><a href="#等离子体暗场衬底设计" class="headerlink" title="等离子体暗场衬底设计"></a>等离子体暗场衬底设计</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412131100796.png"></p><ul><li><p>(a)等离子体暗场衬底示意图。(b)不同Au厚度和不同入射角下R6G发射反射率。PMMA的最小反射率出现在46.5°入射处，对应于SPP模式下的1.1k0</p></li><li><p>在等离子体暗场衬底中，如图S1(a)所示，PMMA中R6G的荧光发射耦合到Au薄膜的SPP模式中。设计Au膜的厚度是实现耦合效率最大化的关键。我们考虑p偏振光用于SPP模式的耦合。根据菲涅耳方程，两种材料界面处的透射系数和反射系数可计算为：</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412131101085.png"></p><ul><li>其中𝑛1和𝑛2分别为材料在界面前后的折射率，𝜃1为入射角，𝜃2为折射角，满足1 1 2 2 sin sin n n &#x3D;。因此，PMMA-Au-Air多层界面的透射和反射系数可计算为：</li></ul><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412131104915.png"></p><ul><li>式中𝑡01和𝑡12分别为PMMA-Au界面和Au-air界面处的透射系数，𝑟01和𝑟12分别为PMMA-Au界面和Au-air界面处的反射系数，1 1 01 cos z kn k &#x3D; q为k矢量的法向分量，d为Au膜厚度。考虑R6G的发射波长为~570 nm。为了使SPP耦合效率最大化，我们对Au膜的厚度进行了扫描。计算了结构的反射率和透过率，如图S2 (b)所示。SPP耦合的最佳条件对应于入射光的最小反射率，当金的厚度为40 nm时，该反射率出现在入射角≥46.4°处。考虑入射材料PMMA的折射率，SPP模式的k向量为0 0 1.5 sin 1.1 SPP i k k k &#x3D;´×»q。因此，我们在d&#x3D;40 nm处完成了Au膜的设计。</li></ul><h1 id="数据集生成与重建性能的仿真"><a href="#数据集生成与重建性能的仿真" class="headerlink" title="数据集生成与重建性能的仿真"></a>数据集生成与重建性能的仿真</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412131016234.png"></p><ol><li>数据集生成与重建性能的仿真方法。(a)生成带有随机物体的地真超分辨率图像。(b)地面真值信息的傅里叶空间处理。SPP模式照明将物体上的高k信息转移到检测波段，形成图(c)中衍射受限的PDF图像。（d, g）模拟边缘和散射粒子的低分辨率PDF图像，用于网络性能测试。（e, h）网络输出超分辨率DAPD图像。（f, j）对象基础真值。标尺&#x3D; 1 μm</li><li>在超分辨率显微镜中，获得地面真实图像尤其具有挑战性。在这项工作中，考虑到正演成像模型已知，我们基于等离子体衬底和成像系统的设计，采用仿真方法估计任意给定物体的低分辨率等离子体暗场图像，如图2a - c所示。在DAPD中，Au薄膜上的SPP模式照射样品，并通过成像系统检测散射光束。由于非相干荧光照明，PDF图像可以计算为每个SPP模式下图像的叠加。对于给定的对象，低分辨率PDF图像可以估计为式2，式中，U为目标空间信息的傅里叶谱，H为成像系统的相干传递函数，kspp，i为SPP模式在不同方向的k向量，m为SPP模式的总数，n为加性高斯白噪声（信噪比(S&#x2F; n) &#x3D; 35）。通过总结不同SPP模式照明产生的图像强度来计算最终的PDF图像。利用MATLAB R2020b对仿真图像数据集进行了数值生成和处理。如图2b所示，m越大，估计越准确。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412131022247.png"></p><h1 id="数据集仿真中的光照近似"><a href="#数据集仿真中的光照近似" class="headerlink" title="数据集仿真中的光照近似"></a>数据集仿真中的光照近似</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412131109549.png"></p><ol><li>为了模拟等离子体暗场图像，我们<span style="color: red;">首先在MATLAB中以数值方式生成地面真值图像，创建类似于暗场图像典型边缘特征的结构（如线和点）。然后使用傅里叶变换将这些真实图像转换到傅里叶空间中。得到的傅里叶谱根据相应的照明k向量进行移位。然后将相干传递函数应用于对象的位移傅立叶谱。通过对所有照明角度的傅里叶光谱求和并进行傅里叶反变换，我们获得了无噪声的等离子体暗场图像。最后，在图像中加入不同信噪比的高斯噪声作为训练数据。</span></li><li>在PDF数据模拟中，SPP照度被视为不同角度的照度，独立计算。计算的照明角度(m)的个数影响着PDF模拟的精度。我们对比了不同m下的模拟PDF图像，如图S2所示。当m&#x3D;5时，有效相干传递函数（CTF）将无法覆盖傅里叶空间中的整个环，从而在仿真图像中引入不希望的强度变化。当m&#x3D;10时，傅里叶空间中的CTF环是完整的。然而，环内强度的不连续仍然会影响模拟图像的强度均匀性。当m增大到30时，傅里叶空间的不连续大大减弱，PDF图像质量得到改善。当我们继续增大m时，模拟的质量并没有明显的提高，因此在本研究中我们使用m&#x3D;30</li><li>m对模拟等离子体暗场图像的影响。（a-d）不同光照角数（m&#x3D;5、10、30和50）下等离子体暗场模拟的有效相干传递函数。（e-h）对应的仿真PDF图像。标尺：1 μm</li></ol><h1 id="网络模型与损失函数"><a href="#网络模型与损失函数" class="headerlink" title="网络模型与损失函数"></a>网络模型与损失函数</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412131111409.png"></p><ol><li>3000对模拟图像用于训练，批大小设置为15。输入图像被裁剪为256 × 256的大小。在使用模拟数据集进行训练后，我们使用以前未见过的模拟图像测试网络性能。如图2d−1所示，衍射受限的PDF图像被成功地重构为物体的超分辨率暗场图像。利用结构与相似指数度量（SSIM）和峰值信噪比（PSNR）比较网络输出和地物真实度对重建图像进行评价，SSIM为0.9885±0.008，PSNR为33.049±0.019 dB。</li></ol><h1 id="傅里叶频谱和分辨率分析"><a href="#傅里叶频谱和分辨率分析" class="headerlink" title="傅里叶频谱和分辨率分析"></a>傅里叶频谱和分辨率分析</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412131243630.png"></p><ol><li>图像的傅里叶分析。(a)衍射极限PDF图像的傅里叶变换。(b)超分辨率dpd图像的傅里叶变换。(c)同一COS-7细胞的网络输出图像的傅里叶环相关。</li><li>为了证实DAPD分辨率的提高，我们对PDF原始图像和网络输出的DAPD图像的傅里叶谱进行了分析，分别如图S5(a)和S5(b)所示。将图像中的轴转换成相对于自由空间横向部分的空间频率，其中NAobj 为成像系统的数值孔径，<a href="https://zhidao.baidu.com/question/820167486774546132.html">λ</a>为光的波长。图S5(a)中的黑色圆圈对应0.1 × k，图S5(b)中的黑色圆圈对应0.8 k。我们还使用公式计算了图S5 (c)中PDF图像与DAPD图像之间的傅立叶环相关性（FRC），其中F1为PDF图像的傅里叶变换，F2为dpd图像的傅里叶变换，ri∈r 为图像频域半径为i的环。本实验采用0.6NA物镜，荧光发射峰在~570 nm处。1&#x2F;7 FRC阈值显示分辨率为170 nm，与衍射极限相比，分辨率提高了2.8倍。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412131250005.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412131253304.png"></p><h1 id="200-nm聚苯乙烯微球"><a href="#200-nm聚苯乙烯微球" class="headerlink" title="200 nm聚苯乙烯微球"></a>200 nm聚苯乙烯微球</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412131033256.png"></p><ol><li>我们通过成像直径为200 nm的聚苯乙烯珠（Invitrogen， USA）来证明DAPD的分辨率提高。成像前，将微珠稀释10倍，滴投于DAPD底物上。底物在倒置显微镜（IX83型，Olympus）下成像。我们使用多模光纤引导的532 nm激光作为照明，激发聚甲基丙烯酸甲酯（PMMA）中的罗丹明6G （R6G）。多模光纤连接在一个旋转的偏心电机上，以消除照明中的斑点。然后用40×&#x2F;0.6NA物镜采集散射信号，然后在相机前放大4倍。考虑kspp≈1.1k0，我们得到DAPD显微镜的NAeff≈1.7。采用532 nm陷波滤波器（Semrock）和以588 nm为中心的20 nm带通滤波器（Semrock）阻断激光传输。图3所示的图像是用CCD相机（iXon 897, Andor）记录的，并通过训练好的神经网络进行处理。预计低分辨率图像的分辨率为~ 500 nm。图3c ~ h中的网络输出图像表明，可以清晰地分辨出直径为~ 200nm的聚苯乙烯珠。如图3j所示，双高斯拟合应用于珠子的高分辨率输出图像，显示190 nm的中心到中心距离，对应于~ 2.8倍的分辨率提高，符合理论估计。</li><li>利用200 nm聚苯乙烯微球对dpd进行了实验论证。（a, b, e, f）神经网络处理前的低分辨率PDF图像。（c, d, g, h）超分辨率dpd重建图像。(i)图(a)和(c)中以一对箭头线标记的目标的横截面轮廓。(j)图(e)和(g)中以一对箭头线标记的目标的横截面轮廓。黑线表示原始的低分辨率PDF图像。红色圆圈表示输出图像像素。红色实线表示两个高斯函数的拟合曲线，蓝色虚线表示每个头的单个高斯曲线。标尺&#x3D; 1 μm。</li></ol><h1 id="COS-7细胞"><a href="#COS-7细胞" class="headerlink" title="COS-7细胞"></a>COS-7细胞</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412131035636.png"></p><ol><li>虽然网络在训练过程中没有遇到COS-7细胞，但仍然取得了令人称道的重建效果。图4b−d和图4h−j显示了COS-7细胞未经处理的等离子体暗场（PDF）图像，由于衍射极限，这些图像显得模糊。相比之下，网络的输出如图4e−g和4k−m所示，可以清楚地分辨出靠近衬底表面的细胞特征，小到200nm的结构也可以清晰地识别出来。其他数据，包括傅立叶谱比较和傅立叶环相关结果，如图S5所示。为了说明DAPD实现的分辨率增强，我们还将DAPD重建结果与使用Lucy−Richardson反卷积算法获得的结果进行了比较，如图S6所示。</li><li>用细胞实验数据训练网络性能。(a)单个COS-7单元的PDF图像。左上为衍射极限PDF图像，右下为网络处理后的DAPD图像。（b−d, h−j）用40x &#x2F;0.6NA物镜获得的衍射受限PDF图像。（e−g, k−m） DAPD超分辨图像。比例尺&#x3D;面板(a)为2 μm，其他面板均为1 μm。所有的横截面轮廓都被放大了两倍，以便更好地可视化。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Deep Learning Assisted Plasmonic Dark-Field Microscopy for SuperResolution Label-Free Imaging</p><p>深度学习辅助等离子体暗场显微镜超分辨率无标记成像</p><p>期刊：Nano Lett</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>Lei, M., et al. “Deep Learning Assisted Plasmonic Dark-Field Microscopy for Super-Resolution Label-Free Imaging.” Nano Lett 24.49 (2024): 15724-30. Print.</li></ul><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Ming Lei，Junxiang Zhao，Ayse Z. Sahan，Jie Hu，Junxiao Zhou，Hongki Lee，Qianyi Wu，Jin Zhang，and Zhaowei Liu</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2024年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年12月13日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：无。</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> U-Net </tag>
            
            <tag> 傅里叶 </tag>
            
            <tag> 衬底 </tag>
            
            <tag> 暗场显微镜 </tag>
            
            <tag> 傅里叶环相关(FRC) </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于去相关分析的无参数图像分辨率估计</title>
      <link href="/2024/12/11/%E5%9F%BA%E4%BA%8E%E5%8E%BB%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90%E7%9A%84%E6%97%A0%E5%8F%82%E6%95%B0%E5%9B%BE%E5%83%8F%E5%88%86%E8%BE%A8%E7%8E%87%E4%BC%B0%E8%AE%A1/"/>
      <url>/2024/12/11/%E5%9F%BA%E4%BA%8E%E5%8E%BB%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90%E7%9A%84%E6%97%A0%E5%8F%82%E6%95%B0%E5%9B%BE%E5%83%8F%E5%88%86%E8%BE%A8%E7%8E%87%E4%BC%B0%E8%AE%A1/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>本文提出了一种基于图像部分相位自相关的超分辨图像分辨率评估方法。该算法是无模型的，不需要任何用户定义的参数。我们展示了它在各种成像模式上的性能，包括衍射限制技术。最后，我们展示了如何使用我们的方法来优化超分辨率显微镜的图像采集和后处理。</li><li>1982年，Van Heel和Saxton利用同一物体的两张独立图像，独立提出了傅里叶环相关（FRC），用于电子显微镜图像的分辨率估计。</li></ol><h1 id="图像去相关分析工作流程"><a href="#图像去相关分析工作流程" class="headerlink" title="图像去相关分析工作流程"></a>图像去相关分析工作流程</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412111047865.png"></p><ol><li>图像去相关分析工作流程。a，图像与其傅里叶滤波归一化版本的相互关系。b，互相关系数作为掩模半径的函数。c、输入图像的高通滤波和分辨率估计。d，为图像和分辨率估计计算的所有去相关函数图。绿色，无高通滤波的去相关函数；灰色、高通滤波去相关函数；蓝十字，局部极大值；黑色，最高频率峰值的去相关函数。垂直虚线，截止频率kc，比例尺，5µm。</li></ol><h1 id="共聚焦和STED"><a href="#共聚焦和STED" class="headerlink" title="共聚焦和STED"></a>共聚焦和STED</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412111052256.png"></p><ol><li>共聚焦和STED。用Abberior Star 635P免疫标记固定COS-7细胞中的微管。a,b，共聚焦图像(a)及其去相关分析(b)。绿线，高通滤波前的去相关函数；洋红色线，a的傅里叶变换绝对值的对数径向平均值；灰线表示所有高通滤波去相关函数；蓝色到黑色的线，去相关函数与细化掩模半径和高通滤波范围。蓝色叉，都是局部最大值。虚线竖线，截断kc（为了可读性，我们在所有后续分析中使用了相同的颜色和样式表示）。运费到付,互相关。c,d，与选定微管的线轮廓相同结构的STED图像(c)及其相应的去相关分析(d)。e,f，两个不同细胞的连续STED图像(e)和四个不同细胞的共四个STED序列的信噪比估计和分辨率（平均值和标准差）(f)随时间的函数。g， STED图像作为STED功率的函数。下面板显示了相应的傅立叶空间与指示的截止频率。h，分辨率作为STED功率的函数（每个STED功率下不同细胞的五幅图像的平均值和标准差）。比例尺，5µm。图像采集和样本细节见补充表1。</li></ol><h1 id="广角成像和结构照明显微镜"><a href="#广角成像和结构照明显微镜" class="headerlink" title="广角成像和结构照明显微镜"></a>广角成像和结构照明显微镜</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412111103578.png"></p><ol><li>广角成像和结构照明显微镜。a， phalloidin Atto 488标记的固定U2OS细胞中肌动蛋白网络的伪宽视场（WF）图像（SIM序列的平均值）（由T. Huser提供）。b、相应的去相关分析。c，选取截面对a进行SIM重构。d，相应的去相关分析。e，用mitotracker标记的U2OS细胞线粒体网络的Pseudo-和SIM重建。f，扇区分辨率估计（白色虚线）和平均分辨率（白色实心圆）。g，在70 × 70像素重叠20像素的贴图中估计e的局部分辨率。h，局部分辨率直方图如图g所示，虚线为中位数分辨率。比例尺，5µm。图像采集和样本细节见补充表1。</li></ol><h1 id="单分子定位显微镜"><a href="#单分子定位显微镜" class="headerlink" title="单分子定位显微镜"></a>单分子定位显微镜</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412111323639.png"></p><ol><li>(a)基于16000帧，Abberior Flip 565标记的COS-7细胞微管的伪wf、标准偏差（STD）和STORM图像。b, a.c的去相关分析，a.d的FRC分析，去相关分辨率（黑线）和FRC分辨率（橙线）作为帧数的函数。e，随机选取15个微管截面的线轮廓。f，标记距离为40nm的HiRes 40R纳米直尺的GATTAquant PAINT图像。g，标记为f的截面的放大图像，以及去相关和FRC分辨率估计。h， HiRes 40R分子的十行谱图，分辨率高于40 nm。图像采集和样本细节见补充表1。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：基于去相关分析的无参数图像分辨率估计</p><p>Parameter-free image resolution estimation based on decorrelation analysis</p><p>期刊：NaTure Methods</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>Descloux, A., K. S. Grussmayer, and A. Radenovic. “Parameter-Free Image Resolution Estimation Based on Decorrelation Analysis.” Nat Methods 16.9 (2019): 918-24. Print.</li></ul><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：A. Descloux , K. S. Grußmayer   and A. Radenovic  </p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2019年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年12月11日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：<a href="https://github.com/Ades91/ImDecorr">https://github.com/Ades91/ImDecorr</a> (作者)</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 去相关分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Image J</title>
      <link href="/2024/12/10/Image-J/"/>
      <url>/2024/12/10/Image-J/</url>
      
        <content type="html"><![CDATA[<h1 id="图像RGB分解"><a href="#图像RGB分解" class="headerlink" title="图像RGB分解"></a>图像RGB分解</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412100949309.png"></p><h2 id="把其中的一个颜色图像取出来"><a href="#把其中的一个颜色图像取出来" class="headerlink" title="把其中的一个颜色图像取出来"></a>把其中的一个颜色图像取出来</h2><ul><li>右击图片，选择Duplicate。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412100952042.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412100953804.png"></p><h2 id="灰度图添加颜色"><a href="#灰度图添加颜色" class="headerlink" title="灰度图添加颜色"></a>灰度图添加颜色</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412131328192.png"></p><h1 id="光强图"><a href="#光强图" class="headerlink" title="光强图"></a>光强图</h1><ul><li>点击Live还可以实时更新。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412100943616.png"></p><h1 id="3D-View"><a href="#3D-View" class="headerlink" title="3D View"></a>3D View</h1><h2 id="灰度图像的3D-View"><a href="#灰度图像的3D-View" class="headerlink" title="灰度图像的3D View"></a>灰度图像的3D View</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412100956811.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412100957510.png"></p><h2 id="3D图像"><a href="#3D图像" class="headerlink" title="3D图像"></a>3D图像</h2><ul><li>选择样品，Fly Brain。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412101017473.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412101018980.png"></p><h1 id="Threshold"><a href="#Threshold" class="headerlink" title="Threshold"></a>Threshold</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412101003328.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412101004863.png"></p>]]></content>
      
      
      <categories>
          
          <category> 软件操作 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>基于卷积神经网络的深度学习生成的六边形衍射光栅用于抑制高阶衍射</title>
      <link href="/2024/12/04/%E5%9F%BA%E4%BA%8E%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%9F%E6%88%90%E7%9A%84%E5%85%AD%E8%BE%B9%E5%BD%A2%E8%A1%8D%E5%B0%84%E5%85%89%E6%A0%85%E7%94%A8%E4%BA%8E%E6%8A%91%E5%88%B6%E9%AB%98%E9%98%B6%E8%A1%8D%E5%B0%84-1/"/>
      <url>/2024/12/04/%E5%9F%BA%E4%BA%8E%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%9F%E6%88%90%E7%9A%84%E5%85%AD%E8%BE%B9%E5%BD%A2%E8%A1%8D%E5%B0%84%E5%85%89%E6%A0%85%E7%94%A8%E4%BA%8E%E6%8A%91%E5%88%B6%E9%AB%98%E9%98%B6%E8%A1%8D%E5%B0%84-1/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>光栅的±1阶衍射在光谱分析中有广泛的应用。当入射光是非单色时，传统衍射光栅产生的高阶衍射总是叠加在有用的一阶衍射上，使后续的光谱解码变得复杂。</li><li>本文采用基于深度学习算法的卷积神经网络设计了具有正弦透射率的单阶衍射光栅，即六边形衍射光栅（HDGs）。训练后的卷积神经网络可以准确地检索出HDGs的结构参数。仿真和实验结果证实了HDGs能有效抑制三阶以上的高阶衍射。三阶衍射强度由一阶衍射强度的20%降低到小于背景衍射强度。</li><li>为了消除高阶衍射引起的干扰，大大提高光谱分析的精度，衍射光栅只需要有零阶衍射和一阶衍射，需要消除高阶衍射。</li></ol><h1 id="光栅示意图"><a href="#光栅示意图" class="headerlink" title="光栅示意图"></a>光栅示意图</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412090959304.png"></p><ol><li>HDGs在x和y方向的周期分别为Px &#x3D; 20µm和Py &#x3D; 20µm。</li><li>六边形结构左侧两点沿x方向的水平距离为a，六边形结构中间两点沿x方向的水平距离为2 * b，六边形结构在y方向的高度c等于周期Py。</li></ol><h2 id="光栅的透射和衍射"><a href="#光栅的透射和衍射" class="headerlink" title="光栅的透射和衍射"></a>光栅的透射和衍射</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412091049182.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412091047687.png" alt="image-20241209104739446"></p><h2 id="HDGs的不同结构参数模拟"><a href="#HDGs的不同结构参数模拟" class="headerlink" title="HDGs的不同结构参数模拟"></a>HDGs的不同结构参数模拟</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412091052367.png"></p><ul><li>根据式(5)，对不同a和b的HDGs进行数值模拟，如图2所示。远场衍射强度分布与HDGs的结构参数a和b有关，可以通过调整结构参数来改变远场衍射强度分布。换句话说，选择合适的结构参数可以将HDGs的透射率转换为正弦函数，使得远场衍射强度分布只存在零级衍射和一级衍射，迫使HDGs获得单级衍射的特性。</li></ul><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412091056114.png"></p><ol><li>深度卷积神经网络框架由四个卷积层和三个全连接层组成。在这里，黄色矩形正方形和蓝色圆圈分别代表卷积神经元和完全连接神经元。输入为观测平面上的轴向衍射强度，输出为结构a和b的两个参数。</li></ol><h1 id="实验步骤"><a href="#实验步骤" class="headerlink" title="实验步骤"></a>实验步骤</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412091108711.png"></p><ol><li>为了验证上述数值模拟结果，我们对HDGs的性能进行了验证实验。我们采用标准的平面制造技术获得了HDGs和TGs。HDGs的结构参数为λ &#x3D; 632.8 nm, a &#x3D; 2.1µm, b &#x3D; 4.7µm, Px &#x3D; 20µm, Py &#x3D; 20µm， HDGs和TGs的面积 &#x3D; 20 mm × 20 mm。TGs的周期与HDGs的周期相同。</li><li>制作工艺可以简单地概括为以下步骤。首先，利用电子束蒸发技术在石英表面沉积了一层150nm的铬层。然后，在铬层上旋转涂覆300 nm厚的正光刻胶，并在150°C热板上烘烤2分钟。随后，设计的光栅图案通过激光直写光刻系统（DESIGN write LAZER 2000）转移到光刻胶上。接下来，显影后，以光刻胶作为掩膜进行铬的湿法蚀刻。最后，用丙酮去除残留的光刻胶，清洗后得到HDGs。图7(a)和7(b)显示了制备的tg和HDGs的光学显微镜图像。观察到的光栅结构与仿真设计一致，说明标准平面加工工艺对结构形貌具有良好的控制效果。</li><li>(a)传统光栅和(b)六边形衍射光栅的显微图像。(c)实验装置示意图。从左到右依次为激光源、光束扩展器、光圈、光栅和CCD。</li><li>图7(c)显示了定制的光学设置，以验证光栅的衍射特性。He-Ne激光器产生波长为λ D 632.8 nm的相干激光束。随着光束的传播，在其传输路径上通过波束扩展器使光束的面积增大。然后，扩展后的光束穿过膜片，照射在所制备的光栅上。最后，利用Lumenera LW230电荷耦合器件（CCD）相机记录光栅的远场衍射强度分布。</li></ol><h1 id="光栅的远场衍射图"><a href="#光栅的远场衍射图" class="headerlink" title="光栅的远场衍射图"></a>光栅的远场衍射图</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412100949020.png"></p><ol><li>由于HDGs和CCD的周期小，以及强度衍射的远场分布对称，我们取了HDGs和TGs的一半衍射图像进行分析。图8(a)和图8(b)为CCD记录的衍射强度分布。对于TGs，在观测平面上有零级衍射、一级衍射和三级衍射。而HDGs只存在零阶衍射和一阶衍射，高阶衍射被有效抑制。HDGs和TGs的轴向强度分布如图8(c)所示。由于CCD的灰度值上限为255，因此零级衍射和一级衍射的强度实际上是过曝光的。可以清楚地看到，三阶衍射光强可以从所需一阶衍射光强的20%降低到背景光强，并且HDGs的背景噪声强度与TGs的背景噪声强度基本相同，说明高阶衍射传递的强度对背景噪声影响很小。实验结果表明，通过设计合理的六边形结构参数，HDGs可以有效地抑制不需要的高阶衍射。</li><li>捕获的(a)传统光栅和(b)六边形衍射光栅的远场衍射图。(c)传统光栅（黑线）与六角形衍射光栅（红线）的轴向水平衍射强度对比。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="锯齿形光栅（Blazed-Grating）"><a href="#锯齿形光栅（Blazed-Grating）" class="headerlink" title="锯齿形光栅（Blazed Grating）"></a>锯齿形光栅（Blazed Grating）</h2><p>通过设计光栅表面的刻线形状（如锯齿形），可以使光栅在某个特定的方向上最大化一阶衍射效率，同时抑制高阶衍射的产生。</p><ul><li>锯齿形状的倾角与入射光波长和光栅常数相匹配时，一阶衍射的效率最高。</li><li>高阶衍射会由于不符合相位条件而被抑制。</li><li>优化刻线形状还能减少光能量的散射和不必要的高阶分布。</li></ul><h2 id="光栅常数"><a href="#光栅常数" class="headerlink" title="光栅常数"></a>光栅常数</h2><p><strong>调节光栅周期</strong>：选择一个适合的光栅周期（光栅常数），让高阶衍射角超出光学系统的接受范围，从而有效避免高阶衍射进入光谱分析仪。</p><ul><li>根据衍射公式 mλ&#x3D;d(sin⁡θ+sin⁡ϕ)，合理设计光栅周期 d。</li><li>例如，缩小光栅周期 d会导致高阶衍射光线分布得更宽，从而被系统排除。</li></ul><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Hexagonal diffraction gratings generated by convolutional neural network-based deep learning for suppressing high-order diffractions</p><p>基于卷积神经网络的深度学习生成的六边形衍射光栅用于抑制高阶衍射</p><p>期刊：Journal of the Optical Society of America A</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>Hu, Huakui, et al. “Hexagonal Diffraction Gratings Generated by Convolutional Neural Network-Based Deep Learning for Suppressing High-Order Diffractions.” Journal of the Optical Society of America A 41.10 (2024). Print.</li></ul><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Huakui Hu，Jiangtao Ding，Weifeng Wu，Huajie Xu，AND Hailiang Li</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2024年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年12月5日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：无。</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 光栅 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>U2-Net：深入使用嵌套U结构进行显著目标检测</title>
      <link href="/2024/12/03/U2-Net%EF%BC%9A%E6%B7%B1%E5%85%A5%E4%BD%BF%E7%94%A8%E5%B5%8C%E5%A5%97U%E7%BB%93%E6%9E%84%E8%BF%9B%E8%A1%8C%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
      <url>/2024/12/03/U2-Net%EF%BC%9A%E6%B7%B1%E5%85%A5%E4%BD%BF%E7%94%A8%E5%B5%8C%E5%A5%97U%E7%BB%93%E6%9E%84%E8%BF%9B%E8%A1%8C%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>U2 -Net，用于显著目标检测（SOD）。</li><li>U2 -Net的架构是一个两层嵌套的u型结构。该设计具有以下优点：<span style="color: red;">(1)由于在我们提出的残差u块（RSU）中混合了不同大小的接受域，它能够从不同尺度捕获更多的上下文信息；(2)由于在这些残差u块中使用池化操作，它增加了整个架构的深度，而不会显著增加计算成本。这种架构使我们能够从头开始训练深度网络，而无需使用图像分类任务中的主干。</span></li><li>大多数SOD网络的设计都有一个共同的模式[18,27,41,6]，即注重充分利用现有主干提取的深度特征，如Alexnet[17]、VGG[35]、ResNet[12]、ResNeXt[44]、DenseNet[15]等。然而，<span style="color: red;">这些主干最初都是为图像分类而设计的。它们提取的特征是语义的代表，而不是局部细节和全局对比信息，这是显著性检测所必需的。</span></li><li>关于SOD的网络架构还有一些问题。首先，它们往往过于复杂。这部分是由于在现有主干中添加了额外的特征聚合模块，以从这些主干中提取多级显著性特征。其次，现有主干通常通过牺牲特征图的高分辨率来实现更深层次的体系结构。为了在可承受的内存和计算成本下运行这些深度模型，特征图在早期阶段被缩小到较低的分辨率。例如，在ResNet和DenseNet[15]的早期层，使用步幅为2的卷积，然后使用步幅为2的maxpooling，将特征映射的大小减少到四分之一。</li><li>U2 -Net是为SOD设计的两层嵌套u型结构，不使用任何来自图像分类的预训练主干。它可以从零开始训练，以获得有竞争力的表现。其次，这种新颖的架构可以在不显著增加内存和计算成本的情况下，使网络更深入，获得高分辨率。这是通过嵌套的u结构实现的：在底层，我们<span style="color: red;">设计了一种新的残差u块（RSU），它能够在不降低特征映射分辨率的情况下提取阶段内的多尺度特征；</span>在顶层，有一个类似U-Net的结构，其中每个阶段都由一个RSU块填充。两级配置导致嵌套的u结构（见图5）。我们的U2 -Net （176.3 MB）在六个公共数据集上实现了与最先进（SOTA）方法的竞争性能，并在1080Ti GPU上实时运行（30 FPS，输入大小为320×320×3）。为了便于在计算和内存受限的环境中使用我们的设计，我们提供了U2 -Net的一个小版本，称为U2 -Net (4.7 MB)。</li></ol><h1 id="各种模型大小的比较"><a href="#各种模型大小的比较" class="headerlink" title="各种模型大小的比较"></a>各种模型大小的比较</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412030940483.png"></p><ol><li>U2 -Net的模型尺寸和性能与其他最先进的SOD模型的比较。maxFβ测量在数据集ECSSD[46]上计算。红星表示我们的U2 -Net (Ours) (176.3 MB)，蓝星表示我们的小版本U2 -Net (Oursy) （4.7 MB）。</li></ol><h1 id="Residual-U-blocks"><a href="#Residual-U-blocks" class="headerlink" title="Residual U-blocks"></a>Residual U-blocks</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412031327512.png"></p><ol><li>现有卷积块和我们提出的残差u块RSU的说明：(a)普通卷积块PLN， (b)类残差块RES， (c)类稠密块DSE， (d)类初始块INC和(e)我们的残差u块RSU。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412031329708.png"></p><ol><li>剩余块与我们的RSU的比较。</li><li>受U-Net的启发，我们提出了一种新的残差u块RSU来捕捉阶段内的多尺度特征。RSU-L(Cin；M;Cout)如图2(e)所示，其中L为编码器层数，Cin、Cout分别为输入、输出通道，M为RSU内层通道数。因此，我们的RSU主要由三个部分组成：(i)输入卷积层，将输入特征映射x （H×W ×Cin）变换为通道为Cout的中间映射F1(x)。这是一个用于局部特征提取的普通卷积层。（ii）高度为L的类似U- net的对称编码器-解码器结构，以中间特征映射F1(x)为输入，学习提取和编码多尺度上下文信息U（F1(x)）。U表示U- net型结构，如图2(e)所示。L越大，剩余u块（RSU）越深，池化操作越多，接受域范围越大，局部和全局特征越丰富。配置此参数可以从任意空间分辨率的输入特征映射中提取多尺度特征。从逐渐下采样的特征图中提取多尺度特征，通过逐级上采样、级联和卷积编码成高分辨率特征图。这个过程减轻了大尺度直接上采样造成的精细细节损失。（iii）通过求和F1(x) +U（F1(x)）融合局部特征和多尺度特征的残差连接。</li><li>RSU与残差块在设计上的主要区别在于，RSU用类似U- net的结构代替了普通的单流卷积，并用权值层变换后的局部特征代替了原始特征：HRSU (x) &#x3D; U(F1(x))+F1(x)，其中U表示图2(e)所示的多层U结构。这种设计变化使网络能够直接从每个残差块中提取多个尺度的特征。更值得注意的是，由于u结构的计算开销很小，因为大多数操作都应用在下采样的特征映射上。</li></ol><h1 id="U2-Net的架构"><a href="#U2-Net的架构" class="headerlink" title="U2 -Net的架构"></a>U2 -Net的架构</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412031335327.png"></p><ol><li>U2 -Net架构说明。主要架构是一个类似u网的编码器-解码器，其中每个阶段由我们新提出的残差u块（RSU）组成。例如，en1基于图2(e)所示的RSU块。表1最后两行给出了各阶段RSU模块的详细配置。</li><li>我们的U2 -Net是一个两层嵌套u型结构，如图5所示。它的顶层是一个由11级组成的大u型结构（图5中的立方体），每个级都由一个配置良好的剩余u块（RSU）填充（底层u型结构）。因此，嵌套的u型结构使得阶段内多尺度特征的提取和阶段间多层次特征的聚合更加高效。</li><li>U2 -Net主要由三部分组成：(1)六级编码器，(2)五级解码器，(3)与解码器级和最后一个编码器级相连的显著性图融合模块。</li><li>(i)在编码器阶段En 1、En 2、En 3和En 4，我们分别使用RSU-7、RSU-6、RSU-5和RSU-4残差u块。如前所述，“7”、“6”、“5”和“4”表示RSU块的高度(L)。L通常根据输入特征图的空间分辨率进行配置。对于高度和宽度较大的特征图，我们使用更大的L来捕获更多的大比例尺信息。En 5和En 6中特征图的分辨率相对较低，进一步降低这些特征图的采样会导致丢失有用的上下文。因此，在en5和en6阶段，使用RSU-4F，其中“F”表示RSU是一个扩展版本，其中我们用扩展卷积替换池化和上采样操作（见图5）。这意味着RSU- 4f的所有中间特征映射与其输入特征映射具有相同的分辨率。</li><li>（ii）对于En 6，解码器级具有与其对称编码器级相似的结构。在de5中，我们还使用了扩展版剩余u块RSU-4F，这与编码器阶段En 5和En 6中使用的相似。每个解码器级都将其前一级和对称编码器级的上采样特征映射的拼接作为输入，如图5所示。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412031345315.png"></p><h1 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h1><p>为了验证U2 -Net的有效性，在以下三个方面进行了烧蚀研究：i)基本块，ii)架构和iii)主干。所有消融研究都遵循相同的实施设置。</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412031358985.png"></p><h1 id="图像分隔的结果"><a href="#图像分隔的结果" class="headerlink" title="图像分隔的结果"></a>图像分隔的结果</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202412031404047.png"></p><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><p>无。</p><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：U2 -Net: Going Deeper with Nested U-Structure for Salient Object Detection</p><p>U2 -Net：深入使用嵌套U结构进行显著目标检测</p><p>期刊：Pattern Recognition</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>Qin, Xuebin, et al. “U2-Net: Going Deeper with Nested U-Structure for Salient Object Detection.” Pattern Recognition 106 (2020): 107404. Print.</li></ul><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Xuebin Qin，Zichen Zhang，Chenyang Huang，Masood Dehghan，Osmar R. Zaiane and Martin Jagersand</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2020年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年12月3日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：<a href="https://github.com/xuebinqin/U-2-Net">https://github.com/xuebinqin/U-2-Net</a> (作者)</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> U-Net </tag>
            
            <tag> 显著目标检测(SOD) </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习使结构照明显微镜具有低光照水平和增强的速度</title>
      <link href="/2024/11/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%BB%93%E6%9E%84%E7%85%A7%E6%98%8E%E6%98%BE%E5%BE%AE%E9%95%9C%E5%85%B7%E6%9C%89%E4%BD%8E%E5%85%89%E7%85%A7%E6%B0%B4%E5%B9%B3%E5%92%8C%E5%A2%9E%E5%BC%BA%E7%9A%84%E9%80%9F%E5%BA%A6/"/>
      <url>/2024/11/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%BB%93%E6%9E%84%E7%85%A7%E6%98%8E%E6%98%BE%E5%BE%AE%E9%95%9C%E5%85%B7%E6%9C%89%E4%BD%8E%E5%85%89%E7%85%A7%E6%B0%B4%E5%B9%B3%E5%92%8C%E5%A2%9E%E5%BC%BA%E7%9A%84%E9%80%9F%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>使用分辨率缩放误差（RSE）和分辨率缩放皮尔逊系数（RSP）来量化恢复质量。</li><li>使用基于去相关分析的方法量化了每种方法的分辨率。</li><li>从给定的细胞内结构训练的模型在用于检查其他结构时产生了显著的伪影（补充图8）。我们应用迁移学习，以最大限度地减少将预训练网络适应其他结构时的工作量（方法）。</li><li>使用深度学习以更少的输入图像和更低的强度和&#x2F;或更短的曝光产生高质量的SIM图像，定量地表明深度学习可以达到与传统SIM重建算法相当的分辨率。</li><li>使用Fiji &#x2F; ImageJ 中的Image Decorrleation Analysis插件估算每张裁剪图像的分辨率，并使用默认参数设置。</li></ol><h1 id="U-Net网络架构"><a href="#U-Net网络架构" class="headerlink" title="U-Net网络架构"></a>U-Net网络架构</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411211044960.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411211046455.png"></p><ul><li>两个u - net通过跳层连接堆叠在一起。以15张低光照条件下的SIM原始图像作为输入，以正常光照条件下的SIM重建图像作为地面真值对网络进行训练。</li></ul><h1 id="使用U-Net进行超分辨率成像"><a href="#使用U-Net进行超分辨率成像" class="headerlink" title="使用U-Net进行超分辨率成像"></a>使用U-Net进行超分辨率成像</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411211022414.png"></p><ol><li>a）使用15或3张SIM原始数据图像作为输入，并使用15张图像的相应SIM重建作为地面真值来训练U-Net。Θ：正弦图案照明的角度；ψ：图案照明的相位。b）不同亚细胞结构的重建结果。所示为15张SIM原始数据图像的平均投影（第一列）、传统SIM重建算法的重建结果（第二列）、U-Net-SIM15输出（第三列）、U-Net-SIM3输出（第四列）以及每张图像中沿虚线的线条轮廓（第五列）。在线形图中，平均值显示在右y轴上，所有其他平均值共享左y轴。R表示分辨率。所示为从补充表1所示的测试数据集中随机选取的代表性图像。训练数据集收集自至少三个独立的实验。c）估计不同方法所达到的分辨率（源数据作为源数据文件提供）。MT微管（n &#x3D; 204）；抗利尿激素。粘连（n &#x3D; 32）；水户。线粒体（61例）；的行为。f -肌动蛋白（n &#x3D; 85）。一个平均水平;S SIM重构；U15 U-Net-SIM15;U3 U-Net-SIM3。图中显示框须图，异常值用圆点表示（方法）。比例尺：1 μm。</li></ol><h1 id="在极端弱光条件下的超分辨率成像"><a href="#在极端弱光条件下的超分辨率成像" class="headerlink" title="在极端弱光条件下的超分辨率成像"></a>在极端弱光条件下的超分辨率成像</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411211026568.png"></p><ol><li>a）两个u - net通过跳层连接堆叠。以低光照条件下拍摄的15张SIM原始数据图像作为输入，以正常光照条件下相应的SIM重建图像作为地面真值，对scU-Net进行训练。b）不同亚细胞结构的重建结果(第一行：微管；第二排：粘接剂；第三行：线粒体；第四行：F-actin)。所示为15个SIM原始数据的平均投影（第一列），传统SIM重建算法的重建结果（第二列），U-Net-SIM15输出（第三列），su - net输出（第四列），以及正常光照条件下SIM重建的真实情况（第五列）。所示为从补充表1所示的测试数据集中随机选取的代表性图像。训练数据集收集自至少三个独立的实验。局部放大显示修复质量。比例尺：1 μm。</li></ol><h1 id="网络架构和培训细节"><a href="#网络架构和培训细节" class="headerlink" title="网络架构和培训细节"></a>网络架构和培训细节</h1><ul><li>U-Net-SIM15和U-Net-SIM3、UNet-SNR和U-Net-SRRF共享相似的网络架构（补充图1），它们只是输入或输出（地真值）数据集的通道数不同(U-Net-SIM15: Cin &#x3D; 15, Cout &#x3D; 1；U-Net-SIM3: Cin &#x3D; 3, Cout &#x3D; 1；U-Net-SNR: Cin &#x3D; 15, Cout &#x3D; 15；U-Net-SRRF: Cin &#x3D; 5, Cout &#x3D; 1；Cin和Cout分别是输入和输出的通道数)。对于UNet-SNR，我们以弱光条件下获取的15张SIM原始数据图像作为输入，正常光照条件下的相同样本作为地面真值。</li></ul><h1 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h1><ul><li>直接将在一个特定结构上训练的模型应用到其他结构上可能会产生重要的伪影（补充图8），这意味着每个目标都需要一个独特的模型。理论上，我们需要准备<del>1000个训练样本，并在消费级显卡（NVIDIA GTX-1080 GPU）上训练网络2-3天（</del>2000个epoch），以获得我们测试的每个结构的工作模型。我们采用迁移学习（transfer learning）来减少对新结构进行成像的工作量。简而言之，我们将从预训练网络中获得的参数初始化一个新网络，并开始在一个不同的结构上使用较小的训练样本大小（200个裁剪补丁）进行再训练。我们验证了迁移学习在恢复不同结构方面的有效性。即使减少了训练工作量（200次），新模型产生的结果与使用更大的数据集和更大的训练工作量训练的模型相当（补充图9）。</li></ul><h1 id="scU-Net用于活细胞成像"><a href="#scU-Net用于活细胞成像" class="headerlink" title="scU-Net用于活细胞成像"></a>scU-Net用于活细胞成像</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411211047238.png"></p><ol><li>a）活细胞微管重建结果。显示了一个代表性的时间点。U-Net-SIM15缺失的结构通过scU-Net恢复（白色箭头）。第一个面板显示了15张SIM原始图像的平均投影；第二张图为SIM重构图；第三个面板显示U-Net-SIM15输出；第四个面板显示scU-Net输出。N &#x3D; 3，来自三个独立实验。b）所示为a中白虚线框所示区域的放大视图。scUNet很好地恢复了单个微管（白色三角形）的动力学。c） scU-Net揭示了微管-线粒体相互作用的动力学。第一行：15张SIM原始图像的平均投影；第二行：SIM重构；第三行：scU-Net输出。N &#x3D; 3，来自三个独立实验。比例尺：1 μm。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="不同结构的数据集的大小"><a href="#不同结构的数据集的大小" class="headerlink" title="不同结构的数据集的大小"></a>不同结构的数据集的大小</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411211041914.png"></p><h2 id="U-Net-SIM性能的定量评估"><a href="#U-Net-SIM性能的定量评估" class="headerlink" title="U-Net-SIM性能的定量评估"></a>U-Net-SIM性能的定量评估</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411211043179.png"></p><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：深度学习使结构照明显微镜具有低光照水平和增强的速度</p><p>Deep learning enables structured illumination microscopy with low light levels and enhanced speed</p><p>期刊：Nature Communications</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>Jin, L., et al. “Deep Learning Enables Structured Illumination Microscopy with Low Light Levels and Enhanced Speed.” Nat Commun 11.1 (2020): 1934. Print.</li></ul><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Luhong Jin，Bei Liu，Fenqiang Zhao，Stephen Hahn，Bowei Dong，Ruiyan Song，Timothy C. Elston，Yingke Xu &amp; Klaus M. Hahn</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2020年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年11月21日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：<a href="https://github.com/drbeiliu/DeepLearning">https://github.com/drbeiliu/DeepLearning</a> </p><p><a href="http://hahnlab.com/tools/">http://hahnlab.com/tools/</a></p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> SIM </tag>
            
            <tag> U-Net </tag>
            
            <tag> 去相关分析 </tag>
            
            <tag> 迁移学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>物理先验引导深度学习用于SIM重建：建模对象到图像的退化</title>
      <link href="/2024/11/19/%E7%89%A9%E7%90%86%E5%85%88%E9%AA%8C%E5%BC%95%E5%AF%BC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8ESIM%E9%87%8D%E5%BB%BA%EF%BC%9A%E5%BB%BA%E6%A8%A1%E5%AF%B9%E8%B1%A1%E5%88%B0%E5%9B%BE%E5%83%8F%E7%9A%84%E9%80%80%E5%8C%96/"/>
      <url>/2024/11/19/%E7%89%A9%E7%90%86%E5%85%88%E9%AA%8C%E5%BC%95%E5%AF%BC%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%94%A8%E4%BA%8ESIM%E9%87%8D%E5%BB%BA%EF%BC%9A%E5%BB%BA%E6%A8%A1%E5%AF%B9%E8%B1%A1%E5%88%B0%E5%9B%BE%E5%83%8F%E7%9A%84%E9%80%80%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ul><li>提出了一种以<span style="color: red;">光学成像物理过程为导向的物像平面退化网络（OIDN）</span>。具体来说，提出的OIDN将目标到图像平面的退化过程嵌入到重建网络中，以提供明确的指导。OIDN利用一组受物理先验约束的可学习点扩展函数（PSF）参数，成功地将传统的图像到图像的数据模式映射转化为高度符合SIM成像光学过程的物体到图像的平面退化映射。</li><li>基于深度学习的SIM重建算法存在两个主要问题：1)重建结果可信度有限：基于DL的重建算法被表示为“黑盒”，使得神经网络如何从有限的输入中产生预期结果变得不清楚，甚至导致错误的重建结构的出现；2)重构模型的泛化性低：这是由于端到端深度学习方法的泛化性与训练样本的大小和种类密切相关。</li><li>提出了一种称为目标到图像平面退化网络（OIDN）的方法。由于SIM系统[31]中照明模式和点扩展函数（PSF）的固有性质，PSF信息隐式编码在微观标本[32]的图像中，这些信息可以用于设计符合SIM物理模型的网络。<span style="color: red;">OIDN基于SIM成像系统的物理退化模型，利用原始输入图像、噪声分量和可学习的PSF构建了一个接近真实的退化过程。</span>此外，为了严格按照SIM成像过程的原理对网络进行优化，利用物理先验来约束PSF的生成，从而在有限的训练数据下获得具有高泛化性的模型。</li></ul><h1 id="空间物理退化过程"><a href="#空间物理退化过程" class="headerlink" title="空间物理退化过程"></a>空间物理退化过程</h1><ul><li>正是PSF的卷积运算导致了高频信息的丢失。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411191026979.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411191026978.png"></p><h1 id="频率物理退化过程"><a href="#频率物理退化过程" class="headerlink" title="频率物理退化过程"></a>频率物理退化过程</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411191049606.png"></p><h1 id="SIM重构过程的物理模型"><a href="#SIM重构过程的物理模型" class="headerlink" title="SIM重构过程的物理模型"></a>SIM重构过程的物理模型</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411191055761.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411191056583.png"></p><h1 id="物像平面退化网络"><a href="#物像平面退化网络" class="headerlink" title="物像平面退化网络"></a>物像平面退化网络</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411191059550.png"></p><ol><li>该过程从3×3卷积层开始，该层将输入的原始图像D(r)转换为多维特征空间。随后，这些特征被输入到一个由四个自适应动态域融合组（ADDG）组成的模块中，每个组都配备了剩余连接。该设计策略性地规避了在过深网络中可能出现的梯度消失问题，从而使网络能够更有效地集中于高频特征的学习。每个ADDG模块由四个自适应动态域融合块（ADDB）、一个3 × 3卷积层和一个短跳接组成。ADDB采用双分支结构，构建动态权值机制减少频域信息损失，采用全局平均池化提取信号整体趋势，全局最大池化重点捕获重要局部特征，实现频域特征的自适应调整。</li></ol><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>模型的性能不仅取决于其结构，而且还受损失函数的选择的显著影响。认识到这一点，我们<span style="color: red;">制定了一个全面的总损失函数，有效地利用输入图像中固有的高频和低频信息。具体来说，它包括三个不同的组成部分：空间域损耗、频率域损耗和PSF损耗。</span></p><p><span style="color: red;">损失函数示意图看图1右下角！！！</span></p><h2 id="空间损失函数"><a href="#空间损失函数" class="headerlink" title="空间损失函数"></a>空间损失函数</h2><p>由均方误差（MSE）损失[22]和结构相似指数度量（SSIM）损失[37]组合而成的空间域损失函数保证了像素精度，平衡了预测动态范围，并增强了输出图像的结构相似性。若将S(r)定义为SR模型的输出，将S(r)‘ 定义为相应的GT，将（w, h）定义为输出图像的像素大小，则目标函数可表示为：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411191412385.png"></p><p>其中λ是用于平衡SSIM和MSE相对贡献的标量权重，在本文中大多数情况下设置为0.1。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>&#96;&#96;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss_mse_ssim</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    ssim_para = <span class="number">1e-1</span></span><br><span class="line">    mse_para = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># nomolization</span></span><br><span class="line">    x = y_true</span><br><span class="line">    y = y_pred</span><br><span class="line">    <span class="comment"># x = norm(x)</span></span><br><span class="line">    <span class="comment"># y = norm(y)</span></span><br><span class="line">    MSE_loss = nn.MSELoss()</span><br><span class="line">    MSE_loss = MSE_loss.to(device)</span><br><span class="line">    <span class="comment"># SSIMloss = SSIM()</span></span><br><span class="line">    <span class="comment"># SSIMloss = SSIMloss.to(device)</span></span><br><span class="line"></span><br><span class="line">    mse_loss = mse_para * MSE_loss(x, y)</span><br><span class="line">    ssim_loss = ssim_para * (<span class="number">1</span> - pytorch_msssim.ssim(x, y))</span><br><span class="line">    mse_ssim_loss = mse_loss + ssim_loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> mse_ssim_loss</span><br></pre></td></tr></table></figure><h2 id="频率损失函数"><a href="#频率损失函数" class="headerlink" title="频率损失函数"></a>频率损失函数</h2><p>为了增强网络学习和获取频域信息的能力，在频域对预测输出图像和GT图像施加约束。在本研究中，我们采用Charbonnier Loss作为频率损失函数的主成分。这种选择是战略性的，因为Charbonnier Loss以其在管理异常值方面的有效性而闻名，异常值是图像处理中的一个关键方面[38]。此外，它还显著提高了模型在图像生成任务中的收敛速度和整体性能。利用Charbonnier Loss符合我们在精度和计算效率之间取得平衡的目标，从而优化模型产生高质量图像重建的能力。频域损失函数可以表示为：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411191423131.png"></p><h3 id="Charbonnier-损失函数"><a href="#Charbonnier-损失函数" class="headerlink" title="Charbonnier 损失函数"></a>Charbonnier 损失函数</h3><p>&#96;&#96;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fft2d</span>(<span class="params"><span class="built_in">input</span></span>):</span><br><span class="line">    <span class="comment"># fft.fftn 是 PyTorch 的多维快速傅里叶变换函数。</span></span><br><span class="line">    <span class="comment"># 参数 dim=(2, 3) 表示对输入张量的第 2 和第 3 维（即高度和宽度）进行二维傅里叶变换。</span></span><br><span class="line">    <span class="comment"># 参数 norm=&#x27;ortho&#x27; 表示使用正交归一化方式，确保傅里叶变换是能量守恒的。</span></span><br><span class="line">    fft_out = fft.fftn(<span class="built_in">input</span>, dim=(<span class="number">2</span>, <span class="number">3</span>), norm=<span class="string">&#x27;ortho&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> fft_out</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">CharbonnierLoss</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="comment"># 定义 Charbonnier 损失的平滑参数</span></span><br><span class="line">    <span class="comment"># epsilon 是 Charbonnier 损失的一个平滑参数，用于避免梯度不稳定问题</span></span><br><span class="line">    epsilon = <span class="number">1e-3</span></span><br><span class="line">    epsilon2 = epsilon*epsilon</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对输入进行二维傅里叶变换</span></span><br><span class="line">    <span class="comment"># 对输入的真实值和预测值分别应用二维傅里叶变换，将其从时域信号转换为频域信号。</span></span><br><span class="line">    <span class="comment"># fft2d 函数假定是一个支持 PyTorch 张量的自定义或已导入的函数</span></span><br><span class="line">    y_true_fft = fft2d(y_true)</span><br><span class="line">    y_pred_fft = fft2d(y_pred)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取频域的实部和虚部</span></span><br><span class="line">    <span class="comment"># 从傅里叶变换结果中提取真实值和预测值的实部和虚部，用于后续计算。</span></span><br><span class="line">    yt_real = y_true_fft.real</span><br><span class="line">    yt_imag = y_true_fft.imag</span><br><span class="line">    yp_real = y_pred_fft.real</span><br><span class="line">    yp_imag = y_pred_fft.imag</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算频域损失</span></span><br><span class="line">    <span class="comment"># 分别计算实部和虚部的 Charbonnier 损失，使用 torch.sqrt 和 epsilon2 保持数值稳定。</span></span><br><span class="line">    <span class="comment"># 最终将实部和虚部的损失相加，得到频域损失。</span></span><br><span class="line">    <span class="comment"># 对所有频域值求平均，返回最终的标量损失值。</span></span><br><span class="line">    real = torch.sqrt(torch.<span class="built_in">pow</span>((yt_real - yp_real), <span class="number">2</span>) + epsilon2)</span><br><span class="line">    imag = torch.sqrt(torch.<span class="built_in">pow</span>((yt_imag - yp_imag), <span class="number">2</span>) + epsilon2)</span><br><span class="line">    value = real + imag</span><br><span class="line">    <span class="keyword">return</span> torch.mean(value)</span><br></pre></td></tr></table></figure><h2 id="PSF损失函数"><a href="#PSF损失函数" class="headerlink" title="PSF损失函数"></a>PSF损失函数</h2><p>PSF模块定义了可学习PSF的最终输出，如图1右上角所示。PSF损失函数被设计用来表征最终输出H ‘ (r)与两个物理约束之间的间隙，促使PSF从正确的分布中学习，式中r≡(x, y)为空间位置向量，n为可学习PSF的大小</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411191431970.png"></p><p>在这种情况下，使用CE损耗和RS损耗分别量化理想PSF与能量一致性和旋转对称分布的偏差。通过不同维度的区分，可以通过评估不同空间背景下的特征来实现对PSF质量的更全面评估。这种方法对于确保PSF在图像重建过程中的准确性和有效性至关重要。</p><h2 id="总损失函数"><a href="#总损失函数" class="headerlink" title="总损失函数"></a>总损失函数</h2><p>考虑空域、频域和PSF的约束，总损失函数可表示为：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411191437098.png"></p><p>在损失函数中，α和β是平衡参数。在本研究中，它们分别固定为0.01和1 × 10−3。选择这些值是为了平衡损失函数中不同分量的贡献，确保图像重建过程的各个方面之间的最佳权衡。</p><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><p>&#96;&#96;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss = args.weight*L1_loss(output, gt_images)+(<span class="number">1</span>-args.weight)*torch.<span class="built_in">abs</span>(<span class="number">1</span>-ssim_loss(output, gt_images))+ (<span class="number">1e-3</span>) * loss_psf + <span class="number">0.01</span> * CharbonnierLoss(gt_images, output)</span><br><span class="line"><span class="comment"># 此处空间损失函数由论文中的MSE+SSIM换成了L1+SSIM</span></span><br></pre></td></tr></table></figure><h2 id="损失函数的消融实验"><a href="#损失函数的消融实验" class="headerlink" title="损失函数的消融实验"></a>损失函数的消融实验</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411191838108.png"></p><h1 id="三种模型的SIM重构能力评价结果"><a href="#三种模型的SIM重构能力评价结果" class="headerlink" title="三种模型的SIM重构能力评价结果"></a>三种模型的SIM重构能力评价结果</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411191810577.png"></p><ol><li>研究中使用的测试图像根据其荧光激发水平分为高、中、低三类。这种分类是至关重要的，因为它确保了成像结果的保真度与SIM通常应用的真实场景一致。该方法可以更准确地评估每种方法在接近实际使用情况下的性能，为其在SIM重建中的实际适用性和有效性提供有价值的见解。</li><li>为了进一步评价三种方法在重建分辨率方面的性能，我们使用去相关分析（DA）来评估超分辨率图像的分辨率。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411191815517.png"></p><ol start="3"><li>比较HiFi-SIM、DFCAN、APCAN和OIDN在四种类型样本上的评价指标：F-actin（样本量&#x3D; 16）、ER（样本量&#x3D; 18）、CCPs（样本量&#x3D; 19）和MT（样本量&#x3D; 20）。</li></ol><h2 id="基于去相关分析的SR图像分辨率评价"><a href="#基于去相关分析的SR图像分辨率评价" class="headerlink" title="基于去相关分析的SR图像分辨率评价"></a>基于去相关分析的SR图像分辨率评价</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411191818214.png"></p><ol><li>对来自APCAN、DFCAN、OIDN和GT的图像（奇列）进行去相关分析（偶列）。绿色曲线表示未经高通滤波的去相关函数；黑色曲线为最高频率峰值的去相关函数；蓝色的点表示去相关函数的局部最大值。比例尺：1nm</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411191821934.png"></p><ol start="2"><li>低荧光信噪比下不同方法重建SIM图像的比较。每行代表一种类型的标本(第一行：F-actin；第二行：MT；第三行：ER；第四行：ccp)。对于每种类型的标本，两个代表性区域被放大以提供更清晰的视图。比例尺：1µm</li></ol><h1 id="可信度实验比较"><a href="#可信度实验比较" class="headerlink" title="可信度实验比较"></a>可信度实验比较</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411191824310.png"></p><ol><li>(a)不同方法重构的SIM结构对比。(b)利用rFRC评价评价不同方法重建的超分辨率图像。</li><li>滚动傅立叶环相关（rFRC）[40]方法用于测量傅立叶域中两个信号之间的距离，对强度变化和微运动不敏感。它通过定义最可靠的频率分量来量化重建误差。rFRC通常用于评估重建不确定性至超分辨率尺度，通过局部图像质量的显示更详细地说明图像的重建质量，可以精确识别不同模型输出图像中可靠性较低的区域。</li><li>经过详细计算，APCAN的平均分辨率为100.95 nm，而DFCAN的平均分辨率为102.86 nm。相比之下，OIDN的平均分辨率为99.93 nm，表现出显著的性能提升。</li><li>相比之下，GT的平均分辨率为82.55 nm，可视化如图5(b)所示。OIDN的最大全像分辨率为154.72 nm，较APCAN （568.19 nm）和DFCAN （505.05 nm）有显著提高，更接近GT （139.23 nm）。这表明OIDN重建的图像不确定度更低，重建质量更可靠。</li></ol><h1 id="概括性实验比较"><a href="#概括性实验比较" class="headerlink" title="概括性实验比较"></a>概括性实验比较</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411191832848.png"></p><ol><li>(a) CCPs、MT和ER样本的泛化能力测试，从左至右显示。(b)跨数据集验证评估指标的比较雷达。</li></ol><h1 id="理论PSF与可学PSF的比较结果"><a href="#理论PSF与可学PSF的比较结果" class="headerlink" title="理论PSF与可学PSF的比较结果"></a>理论PSF与可学PSF的比较结果</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411191835007.png"></p><ol><li>(a)理论和理想PSF的目视比较。(b) CE、RS和OIDN超分辨率重建图像的PSNR、SSIM和NRMSE评价结果。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="滚动傅立叶环相关（rFRC）-40-方法"><a href="#滚动傅立叶环相关（rFRC）-40-方法" class="headerlink" title="滚动傅立叶环相关（rFRC）[40]方法"></a>滚动傅立叶环相关（rFRC）[40]方法</h2><p><a href="https://www.nature.com/articles/s41377-023-01321-0#code-availability">参考论文</a></p><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Physical prior-guided deep learning for SIM reconstruction: modeling object-to-image degradation</p><p>物理先验引导深度学习用于SIM重建：建模对象到图像的退化</p><p>期刊：Optics Express</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>Jin, Zitong, et al. “Physical Prior-Guided Deep Learning for Sim Reconstruction: Modeling Object-to-Image Degradation.” Optics Express 32.21 (2024). Print.</li></ul><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：ZITONG JIN，JUNKANG DAI，BOWEN LIU，ZHIXIANG WEI，ZHENAN FU，HUAIAN CHEN，AND YI JIN</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2024年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年11月19日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：<a href="https://github.com/USTCdjk/OIDN/tree/master">https://github.com/USTCdjk/OIDN/tree/master</a> (作者)</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> SIM </tag>
            
            <tag> 物理先验 </tag>
            
            <tag> 点扩散函数(PSF) </tag>
            
            <tag> OIDN </tag>
            
            <tag> 去相关分析 </tag>
            
            <tag> 傅里叶环相关(FRC) </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>迈向稳健的超分辨成像：模式照明傅立叶平面摄影的低阶近似方法</title>
      <link href="/2024/11/18/%E8%BF%88%E5%90%91%E7%A8%B3%E5%81%A5%E7%9A%84%E8%B6%85%E5%88%86%E8%BE%A8%E6%88%90%E5%83%8F%EF%BC%9A%E6%A8%A1%E5%BC%8F%E7%85%A7%E6%98%8E%E5%82%85%E7%AB%8B%E5%8F%B6%E5%B9%B3%E9%9D%A2%E6%91%84%E5%BD%B1%E7%9A%84%E4%BD%8E%E9%98%B6%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95/"/>
      <url>/2024/11/18/%E8%BF%88%E5%90%91%E7%A8%B3%E5%81%A5%E7%9A%84%E8%B6%85%E5%88%86%E8%BE%A8%E6%88%90%E5%83%8F%EF%BC%9A%E6%A8%A1%E5%BC%8F%E7%85%A7%E6%98%8E%E5%82%85%E7%AB%8B%E5%8F%B6%E5%B9%B3%E9%9D%A2%E6%91%84%E5%BD%B1%E7%9A%84%E4%BD%8E%E9%98%B6%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li><span style="color: red;">图案照明傅立叶压印术（piFP）</span>是结构照明成像和傅立叶压印算法的优雅结合，具有超越所采用光学器件衍射极限成像的能力。无伪影的piFP超分辨率重建要求光照模式的高度稳定性。</li><li>在存在环境扰动、强度波动和指向源不稳定的情况下，会发生不可预测的模式变化，导致图像重建质量下降。为了解决这一问题，我们提出了一种基于低秩近似的<span style="color: red;">高效鲁棒piFP算法（LRA-piFP）</span>，该算法放宽了对照明模式稳定性的要求。</li><li><span style="color: red;">计算成像（CI）</span>技术使成像能够通过散射介质，和绕过光学的衍射限制。CI的原理是共同设计物理编码模型和计算解码算法，允许从间接测量中重建图像。在CI中，解逆图像重建往往需要对物理模型有准确的了解。然而，在许多实际情况下，理论模型与现实物理系统之间存在差异，导致模型不匹配，从而带来令人不快的伪影，降低重建质量。因此，考虑模型不匹配对提高CI的可靠性和适用性至关重要。</li><li>为了实现高质量的图像重建，<span style="color: red;">结构照明显微镜（SIM）</span>需要准确估计初始相位和调制深度等照明参数。即使很小的参数错误也会导致重大的重建伪影由于这个原因，已经提出了许多算法来估计照明参数。另一个例子是<span style="color: red;">傅里叶平面摄影（FP）</span>，这是一种新颖的CI技术，可以绕过所用光学器件的衍射极限。</li><li>由于<span style="color: red;">频率混合效应</span>，每个原始图像包含超出成像镜头截止频率的信息，并且可以恢复超分辨率样本图像以及照明模式。</li><li>piFP的重建过程与FP有共同的根源，FP在空间和傅里叶空间之间迭代切换，从一组低分辨率测量中恢复高分辨率图像。因此，FP的许多算法扩展也适用于piFP。</li><li>在piFP中，光照模式在样品上被顺序扫描，而其分布假定是平稳的。然而，导致模式变化的原因有很多，包括环境扰动、光源方向不稳定和功率，导致模型不匹配，从而阻碍图像的准确重建。这种光照模式的不匹配在piFP实现中很常见，但据我们所知，这仍然是一个未解决的问题。为了解决这个问题，我们提出了一种基于低秩近似的高效鲁棒piFP算法，称为LRA-piFP，在不做任何事先假设的情况下对模式变化进行建模。LRA-piFP方法通过在每个扫描位置重建不同的模式来放松平稳模式假设。在迭代重建过程中，所有扫描位置的模式被投影到由截断奇异值分解（SVD）确定的低维空间中。在低维空间中，每个扫描位置的模式被估计为几个特征模的线性组合，而不必要的和有噪声的模式被拒绝。低秩近似对CI技术来说并不新鲜；它背后的思想是“降维”，它已被用于传统的平面摄影（CP）来模拟时间探针的变化，在FP中用于考虑瞳孔的空间变化，在相位检索中用于抑制噪声，和在SIM中用于估计照明参数。</li><li>第二节描述了piFP的前向模型和我们用于恢复具有不同模式的高质量超分辨率图像的LRA-piFP算法；第三节给出了LRA-piFP方法的仿真结果；第四节通过在远程成像中使用USAF靶标和在光学显微镜中使用Siemens星靶来量化图像质量的改善，并将LRA-piFP与原始piFP以及blind-SIM的性能进行比较。</li></ol><h3 id="频率混合效应（Frequency-Mixing-Effect）"><a href="#频率混合效应（Frequency-Mixing-Effect）" class="headerlink" title="频率混合效应（Frequency Mixing Effect）"></a>频率混合效应（Frequency Mixing Effect）</h3><ul><li><p><strong>频率混合效应</strong>是由于成像系统的截止频率（或称空间频率的上限）所引起的现象。在成像系统中，镜头的成像能力是有限的，无法捕捉到超过某个频率的高频信息。超出镜头截止频率的频率信息可能会混合或丢失，导致图像模糊或细节丢失。</p></li><li><p>在超分辨率显微镜技术中，尤其是结构光显微镜（SIM），这一效应是非常常见的。因为照明模式的设计（如使用不同的照明频率）有助于恢复原始图像中的高频信息。通过对多幅不同频率的图像进行组合处理，可以恢复被丢失的高频细节，从而提高图像的空间分辨率。</p></li></ul><h2 id="piFP的前向模型"><a href="#piFP的前向模型" class="headerlink" title="piFP的前向模型"></a>piFP的前向模型</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411182108890.png"></p><ol><li>图样照明傅立叶平面照相术原理。(a)实验设置。(b)高分辨率物体。(c)光照模式不均匀。(d)物体和图案的乘法。(e) (d)的傅里叶谱。(f)光学传递函数。(g)低分辨率原始图像。FT和IFT分别表示傅里叶变换和傅里叶反变换。</li><li>piFP方法使用非均匀强度模式对样品进行扫描，并通过成像透镜捕获相应的低分辨率图像。在这里，强度模式在数字微镜设备（DMD）上播放，并通过投影透镜投射到物体上。与SIM类似，照明图案和物体之间的混频效应导致高频采样信息转移到低频通带。因此，每个捕获的图像Ik(x)包含的信息超出了成像透镜的衍射极限。</li><li>图1(b) -1 (g)总结了正向过程，式中，x为空间坐标，xk为图像的偏移量，O(x)为物体，P（x−xk）为第k个光照模式，psf为集体光学确定的强度点扩散函数，“⋅”为逐元乘法。⋆是卷积算子，Eq.(1)中的卷积运算通常通过快速傅里叶变换在傅里叶域中计算。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411182113761.png"></p><ol start="4"><li>(1)式假设模式运动时物体是静止的，这与CP中探针-物体的相互作用类似。为了建立静止模式模型，我们可以将Eq.(1)等效表示为物体相对于模式的运动。令x ‘ &#x3D; x−xk，则方程修改为式2，其中Ik（x′+ xk）为移位图像，可以根据偏移量xk对捕获图像进行简单的平移。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411182123285.png"></p><h2 id="重建图像"><a href="#重建图像" class="headerlink" title="重建图像"></a>重建图像</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411182137103.png"></p><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Toward robust super-resolution imaging: A low-rank approximation approach for pattern-illuminated Fourier ptychography</p><p>迈向稳健的超分辨成像：模式照明傅立叶平面摄影的低阶近似方法</p><p>期刊：APL Photonics</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>Zhang, Junhao, et al. “Toward Robust Super-Resolution Imaging: A Low-Rank Approximation Approach for Pattern-Illuminated Fourier Ptychography.” APL Photonics 9.6 (2024). Print.</li></ul><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Junhao Zhang，Weilong Wei，Kaiyuan Yang，Qiang Zhou，Haotong Ma，Ge Ren，Zongliang Xie</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2024年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年11月18日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：无</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SIM </tag>
            
            <tag> 图案照明傅立叶压印术（piFP） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于物理先验卷积网络的荧光显微图像超分辨率重建</title>
      <link href="/2024/11/18/%E5%9F%BA%E4%BA%8E%E7%89%A9%E7%90%86%E5%85%88%E9%AA%8C%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%9A%84%E8%8D%A7%E5%85%89%E6%98%BE%E5%BE%AE%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E9%87%8D%E5%BB%BA/"/>
      <url>/2024/11/18/%E5%9F%BA%E4%BA%8E%E7%89%A9%E7%90%86%E5%85%88%E9%AA%8C%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%9A%84%E8%8D%A7%E5%85%89%E6%98%BE%E5%BE%AE%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E9%87%8D%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>提出了一个<span style="color: red;">基于物理先验的卷积超分辨率网络(PCSR)</span>，它结合了一个基于物理的损失项和一个基于维纳滤波器的初始优化过程，直接使用低分辨率图像创建优秀的超分辨率图像。实验结果表明，通过在有限的数据集上进行训练，PCSR可以实现100 ms的快速重建时间和10 nm的高空间分辨率，从而实现高时空分辨率、低细胞光毒性光照和高可及性的亚细胞研究。</li><li>为了减轻对高质量数据的依赖，我们提出了物理卷积超分辨率网络（PCSR）从低分辨率WF图像重建SR图像。PCSR由物理反演网络（PIN）和超分辨率网络（SRN）组成。SRN采用先进的网络结构，精确获得高质量的荧光图像。PIN旨在加强输入，促进SRN的训练，并产生改进的结果。具体来说，PIN模拟了荧光成像过程，并逆转了由PSF引起的模糊效应。这种方法将深度学习与物理建模相结合。</li><li>受荧光图像的稀疏性和连续性的物理先验性的启发，我们设计了一种新的由稀疏性损失项和连续性损失项组成的损失函数。为了优化被称为稀疏性和连续性（SC）损失的损失函数，我们用神经网络代替过去传统的方法来实现稀疏性和连续性的重建。SC损失函数会显著影响SR网络的结果。</li><li>研究了从单个LR荧光图像直接重建到SR图像的可能性。首先探索了PCSR对微管WF图像进行SR重建的可行性。然后，我们研究了物理先验对SR重建的影响。此外，我们还证明了PCSR在重建其他活细胞结构中的普遍性。实验结果表明，从单幅LR图像中获得SR图像 时，PCSR可以实现100 ms的快速重建时间和10 nm的高空间分辨率，为快速、准确和可获取的活细胞SR成像提供了可能。</li></ol><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411181020422.png"></p><ol><li>物理卷积超分辨率网络(PCSR)包括物理反演网络(PIN)和超分辨率网络(SRN)。</li><li>LR图像首先经过基于维纳滤波的物理反演网络（PIN），再经过超分辨率卷积网络（SRN），实现荧光显微镜图像的超分辨率重建。最后，PCSR计算网络输出与接地真值（GT）之间的损失，然后通过反向传播优化PCSR参数。</li></ol><h3 id="物理反演网络-PIN"><a href="#物理反演网络-PIN" class="headerlink" title="物理反演网络(PIN)"></a>物理反演网络(PIN)</h3><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411181024044.png"></p><ol><li>PIN包括伪逆算子求解器(POS)和傅立叶卷积运算(FCO)。PIN的输入由低分辨率（LR）图像和相应系统的点扩散函数（PSF）组成。</li><li>背景噪声的存在会降低图像的重建质量。这个问题可以通过维纳滤波器进行优化[34]，在一定程度上提高重构质量。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411181106877.jpg"></p><h3 id="维纳滤波"><a href="#维纳滤波" class="headerlink" title="维纳滤波"></a>维纳滤波</h3><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411181049730.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411181050761.png"></p><h3 id="Hadamard-Product"><a href="#Hadamard-Product" class="headerlink" title="Hadamard Product"></a>Hadamard Product</h3><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411181415458.png"></p><h3 id="GELU"><a href="#GELU" class="headerlink" title="GELU"></a>GELU</h3><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411181102993.png"></p><ul><li>GELU表示高斯误差线性单元，它是神经网络的高性能激活函数，是常用的整流线性单元（ReLU）激活函数的光滑变体。</li></ul><h3 id="超分辨率网络-SRN"><a href="#超分辨率网络-SRN" class="headerlink" title="超分辨率网络(SRN)"></a>超分辨率网络(SRN)</h3><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411181110113.png"></p><ul><li>在PIN之后，SRN负责SR重建（图3）。它包含了从低分辨率（LR）输入到SR输出学习复杂映射的卷积层。SRN包括编码器网络和解码器网络的对称结构。</li><li>RN的结构。SRN是一种实现超分辨率重构的卷积网络，包括编码器网络和解码器网络。SRN包括三个基本模块，分别是SR模块、下采样模块和上采样模块。</li></ul><h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><ol><li>考虑到高分辨率的荧光显微镜图像变得更加稀疏，细胞结构应该是连续的，尽管在不同的显微镜系统下可能具有不同的外部形状[46]，荧光显微镜图像，如微管，应该遵循物理先验知识的稀疏性和连续性。因此，我们提出在损失函数内使用L1范数和Hessian矩阵作为正则化项来影响PCSR的学习机制。这种新的损失函数被称为<span style="color: red;">稀疏性和连续性损失(SC)</span>。式中，MSE为均方误差，||||1为L1范数，H为Hessian矩阵，λ，µ为正则化系数。</li><li>L1范数表示为||||1，表示每个元素绝对值的和。使用L1范数作为正则化项的目的是产生低像素值，最终导致稀疏解决方案。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411181314097.png"></p><ol start="3"><li>Hessian矩阵，用H表示，是描述图像在不同方向上二阶导数的二维矩阵。Hessian矩阵通常用于提取边缘，降低噪声，使图像更加连续。这是因为图像的二阶导数对像素值突然变化的区域高度敏感。如果图像的像素值是均匀或一致的，则其黑森矩阵的每个元素趋向于低。通过降低Hessian矩阵的L1范数来平滑图像中的像素变化，具体情况如下：</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411181327522.png"></p><ol start="4"><li>通过将稀疏性和连续性的物理先验知识作为正则化项纳入损失函数中，神经网络的结果将呈现出稀疏性和连续性的特征。这些特征与我们生成SR显微镜图像的目标一致。</li><li>工作中进一步使用的损失函数如下：</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411181326314.png"></p><h2 id="PCSR实现细胞微管结构的超分辨率重建"><a href="#PCSR实现细胞微管结构的超分辨率重建" class="headerlink" title="PCSR实现细胞微管结构的超分辨率重建"></a>PCSR实现细胞微管结构的超分辨率重建</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411181330610.png"></p><ul><li><p>（a, b） PCSR实现了从WF图像到STORM超分辨率图像的超分辨率重建（时间分辨率约为100 ms，空间分辨率约为10 nm）。b中的第一列：微管的WF图像。第二列：相同视场下微管的STORM图像。第三列：网络输出的SR图像。</p><p>SR与GT的保真度较高。(c) b中蓝色框框区域放大。(d)左：c中蓝色虚线方向的强度分布。右：c中橙色虚线方向的强度分布。FWHM约为10 nm。比例尺：200nm。</p></li></ul><h2 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h2><ol><li>所提出的深度学习网络是在pytorch 1.10和python 3.8环境下进行的。使用的GPU服务器配置8张NVIDIA Tesla v10 - 16g显卡和48个CPU核。训练前，所有数据归一化为8位格式。此外，还对图像进行了水平和垂直翻转等随机数据增强技术。采用Adamw优化算法进行反向传播和参数更新[50]。参数更新的周期设置为400，批大小为16。整个训练过程大约花了2个小时。首先，学习率设置为0.001。在epoch 200时下降。Adamw是由Adam优化器[51]衍生而来的增强算法。它将权重衰减计算与梯度更新计算分离，减轻了权重衰减对梯度更新的影响。<span style="color: red;">为了正确确定SC损耗项的系数，我们计算了GT的L1范数及其Hessian矩阵的L1范数。</span></li></ol><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><h3 id="PCSR与其他网络的比较"><a href="#PCSR与其他网络的比较" class="headerlink" title="PCSR与其他网络的比较"></a>PCSR与其他网络的比较</h3><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411181343748.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411181345093.png"></p><ol><li>PCSR与其他网络的比较。(a) WF图像、GT、各网络输出。(b) a中的区域被放大。观察到GT中相邻微管的存在，PCSR成功地推断了细节。(c) b. PCSR中沿蓝线的强度分布推断出与GT相似的强度分布。(d)各网络输出的PSNR和SSIM箱形图。比例尺：200nm</li></ol><h2 id="损失函数和物理反演网络对PCSR性能的影响"><a href="#损失函数和物理反演网络对PCSR性能的影响" class="headerlink" title="损失函数和物理反演网络对PCSR性能的影响"></a>损失函数和物理反演网络对PCSR性能的影响</h2><h3 id="SC损耗对网络输出的影响"><a href="#SC损耗对网络输出的影响" class="headerlink" title="SC损耗对网络输出的影响"></a>SC损耗对网络输出的影响</h3><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411181351474.png"></p><ul><li>荧光显微镜图像具有固有的物理先验的稀疏性和连续性。为了利用这些先验，我们在PCSR的训练中加入了一个新的SC损失函数。在SC损耗中，连续性损耗项可以提高图像的连续性和平滑性，但会在一定程度上降低图像的分辨率，而稀疏性损耗项可以通过添加高频细节来提高分辨率，使图像变得清晰。通过将SC损失与其他常见损失函数（包括MSE损失和SSIM损失）进行比较，评估SC损失的影响。</li><li>SC损耗改善了网络结果。(a)从左至右：WF图像，GT，网络输出SR使用SC损耗，MSE损耗，SSIM损耗。（b, c） a中的框框区域被放大。与GT类似，SR成功地区分了相邻的微管，并产生了清晰的细节和连续的结构。比例尺：200nm。</li><li>图6展示了使用不同损失函数的网络输出的可视化比较。用SC损失训练的PCSR成功地区分了相邻的微管，产生了与GT非常相似的清晰连续的结构。相比之下，用MSE损失训练的网络无法分辨紧密间隔的微管，导致特征模糊和不清晰。使用SSIM损失训练的网络虽然比MSE损失获得更高的PSNR和SSIM，但引入了严重的伪影，影响了图像的整体质量。</li><li>将SC损失纳入PCSR训练中，大大提高了超分辨率荧光显微镜图像的质量。通过有效地结合稀疏性和连续性原则，SC损失提高了网络产生高保真重建的能力，优于传统的损失函数。这些结果证明了将物理先验整合到损失函数中对于基于深度学习的超分辨率成像的价值。</li></ul><h3 id="物理反演网络提高了WF图像的质量"><a href="#物理反演网络提高了WF图像的质量" class="headerlink" title="物理反演网络提高了WF图像的质量"></a>物理反演网络提高了WF图像的质量</h3><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411181354631.png"></p><ul><li>物理反演网络提高了WF图像的质量。(a)微管WF图像和PIN结果。PIN显著提高了WF图像的质量。(b) a中橙色区域被放大。(c) PCSR的损失函数表明PIN促进了网络的训练。蓝线表示添加PIN码(P)，橙线表示不添加PIN码（NP）。P的损失函数收敛曲线下降并快速稳定收敛，而NP的损失函数收敛曲线振荡剧烈。比例尺：200nm。</li><li>利用维纳滤波器加入PIN，显著提高了PCSR的训练效率和稳定性。PIN通过减少伪影和平滑输入来提高WF图像的初始质量，从而促进更有效的网络训练。图7显示了PIN对图像质量和训练动态的影响。随着PIN的加入，网络损失函数收敛速度更快、更稳定，如图7(c)中的蓝线所示。在没有PIN的情况下，损失函数表现出严重的振荡，这表明在训练稳定性方面存在挑战。总体而言，PIN的集成提高了重建质量，加快了训练过程，强调了其在提高网络性能方面的重要性。</li></ul><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><ol><li>将物理先验纳入深度学习的第一种策略是逐步展开基于模型的算法，并用卷积层取代迭代过程。它结合了神经网络的高性能和物理模型的可解释性。第二种策略是设计包含物理先验的损失函数来影响网络的学习机制。它会显著影响神经网络的输出，而神经网络通常用于特定的应用。第三种策略是考虑正演成像物理模型，然后设计相应的逆解算法并与网络相结合。它可以优化网络输入，进一步方便网络训练。换句话说，物理先验的结合是提高神经网络性能和减少对数据依赖的有力工具。</li></ol><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Super resolution reconstruction of fluorescence microscopy images by a convolutional network with physical priors</p><p>基于物理先验卷积网络的荧光显微图像超分辨率重建</p><p>期刊：Biomedical Optics Express</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>Cai, Qiangyu, et al. “Super Resolution Reconstruction of Fluorescence Microscopy Images by a Convolutional Network with Physical Priors.” Biomedical Optics Express 15.11 (2024). Print.</li></ul><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：QIANGYU CAI，JUN LU，WENTING GU，DI XIAO，BOYI LI，LEI XU，YUANJIE GU，BIQIN DONG，AND XIN LIU</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2024年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年11月18日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：无</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> U-Net </tag>
            
            <tag> 傅里叶 </tag>
            
            <tag> 物理先验 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>高分辨率体内4D-OCT鱼眼成像，使用3D-UNet和多级残留解码器</title>
      <link href="/2024/11/13/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E4%BD%93%E5%86%854D-OCT%E9%B1%BC%E7%9C%BC%E6%88%90%E5%83%8F%EF%BC%8C%E4%BD%BF%E7%94%A83D-UNet%E5%92%8C%E5%A4%9A%E7%BA%A7%E6%AE%8B%E7%95%99%E8%A7%A3%E7%A0%81%E5%99%A8/"/>
      <url>/2024/11/13/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E4%BD%93%E5%86%854D-OCT%E9%B1%BC%E7%9C%BC%E6%88%90%E5%83%8F%EF%BC%8C%E4%BD%BF%E7%94%A83D-UNet%E5%92%8C%E5%A4%9A%E7%BA%A7%E6%AE%8B%E7%95%99%E8%A7%A3%E7%A0%81%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li><span style="color: red;">光学相干断层扫描（OCT）</span>主要应用在眼科学中，不自主眼动是影响图像采集的一个重要问题。</li><li>提出了一种端到端深度学习方法来实现实时4D-OCT成像。我们首先以高扫描速率收集欠采样图像，然后应用基于DL的上采样方法来提高图像分辨率。</li></ol><h2 id="4D-OCT系统"><a href="#4D-OCT系统" class="headerlink" title="4D-OCT系统"></a>4D-OCT系统</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411150928296.png"></p><ol><li>图1表示了我们提出的方法的工作流程。首先，内部构建的OCT系统对体内鱼眼的高速干涉图信号进行采样，并将其传输到处理计算机，如图1(a)所示。</li><li>第一种提出的方法使用双2D-UNet (D2DUNet)，其中包括一个Y- unet和一个Z- unet，沿Y和Z轴顺序插值图像，如图1(c)所示。该方法被称为常规方法，因为Y-UNet和Z-UNet都是基于二维图像的CNN方法，广泛用于OCT轴向和横向方向的图像上采样任务。第二种方法使用一个3D-UNet沿两个轴同时对图像大小进行上采样，如图1(d)所示。</li></ol><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411150941063.png"></p><ol><li>网络的上部路径是一个三维上采样层，使用线性插值将图像尺寸沿Y和Z方向增加4倍。下面的路径是一个类似unet的架构，带有一个产生高频特征的多级剩余解码器（MRD）。</li><li>每一层编码器和解码器的基块为残差块，残差块包括两个3 × 3 × 3卷积层、两个整流线性单元（ReLU）层和残差运算，利用残差学习[29]在优化过程中改善梯度流。</li><li>我们没有在我们的网络中使用批归一化（batch normalization， BN）层，因为有研究表明，批归一化会导致高分辨率图像重建精度[30]的降低。</li><li>低分辨率和高分辨率图像通常具有相似的低频信息，这些信息来自于上路径。下路径主要用来学习高频特征。上路径的结果提供了高分辨率体积的粗略估计，下路径主要用于添加组织的更精细的细节。</li></ol><h2 id="多级剩余解码器-MRD"><a href="#多级剩余解码器-MRD" class="headerlink" title="多级剩余解码器(MRD)"></a>多级剩余解码器(MRD)</h2><ol><li>与仅依赖最后一层的信息作为输出的经典U-Net相比，我们提出的网络利用从各个层提取的信息来增强性能。每个U-Net电平的输出后跟一个带有4个特征通道的残差块，结果标记为R1 ~ R5。如果我们将解码器的每个电平的输出定义为D1 ~ D5，我们得到D5 &#x3D; R5。</li><li>其中Up是一个线性上采样层，将两个图像的尺寸增加2倍。值得注意的是，Di引导解码过程准确还原了各种尺度空间的细节，对揭示组织边界有很大贡献[32]。</li></ol><h2 id="重建效果"><a href="#重建效果" class="headerlink" title="重建效果"></a>重建效果</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411150953739.png"></p><ol><li>(a)由(b)双三次插值沿xy平面（b扫描）的输入下采样OCT重建结果，(c) 2D-UNet, (d) 2D-UNet与MRD， (e) 3D-UNet, (f)提出的方法（3D-UNet with MRD）， (g)地面真值。插图：放大的视图，突出水晶透镜的边缘。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411150955328.png"></p><ol><li>图5显示了使用不同方法在xz平面上的OCT图像。图5（a-e）分别表示输入图像和双三次插值、2D-UNet、2D-UNet with MRD、3D-UNet和本文方法的图像重建结果，图5(g)为比较的基本真值。如红色箭头所示，我们提出的方法可以显示比其他方法更清晰的晶界。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><p>无。</p><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：高分辨率体内4D-OCT鱼眼成像，使用3D-UNet和多级残留解码器</p><p>High-resolution in vivo 4D-OCT fish-eye imaging using 3D-UNet with multi-level residue decoder</p><p>期刊：Biomedical Optics Express</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>Zuo, R., et al. “High-Resolution in Vivo 4d-Oct Fish-Eye Imaging Using 3d-Unet with Multi-Level Residue Decoder.” Biomed Opt Express 15.9 (2024): 5533-46. Print.</li></ul><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：RUIZHI ZUO，SHUWEN WEI，YANING WANG，KRISTINA IRSCH，JIN U. KANG</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2024年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年11月13日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：无</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> U-Net </tag>
            
            <tag> 光学相干断层扫描（OCT） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Learning Note</title>
      <link href="/2024/11/12/Deep-Learning-Note/"/>
      <url>/2024/11/12/Deep-Learning-Note/</url>
      
        <content type="html"><![CDATA[<h1 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h1><h2 id="什么叫平滑操作？"><a href="#什么叫平滑操作？" class="headerlink" title="什么叫平滑操作？"></a>什么叫平滑操作？</h2><p>平滑操作的核心思想是利用周围像素的值来调整目标像素，从而减小像素值的剧烈变化，使图像看起来更加平滑和柔和，即就是让周围的像素点和自己差别不要太大。</p><h2 id="数学中的卷积"><a href="#数学中的卷积" class="headerlink" title="数学中的卷积"></a>数学中的卷积</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411121521238.png"></p><h2 id="图像的卷积操作"><a href="#图像的卷积操作" class="headerlink" title="图像的卷积操作"></a>图像的卷积操作</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411121523934.png"></p><h3 id="不同卷积核-滤波器-filter-的作用"><a href="#不同卷积核-滤波器-filter-的作用" class="headerlink" title="不同卷积核(滤波器 filter)的作用"></a>不同卷积核(滤波器 filter)的作用</h3><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411121524621.png"></p><h2 id="卷积的三层含义"><a href="#卷积的三层含义" class="headerlink" title="卷积的三层含义"></a>卷积的三层含义</h2><ol><li>不稳定输入，稳定输出，求系统存量。</li><li>周围像素点如何产生影响。</li><li>提取图像的特征。</li></ol><h1 id="傅里叶变换"><a href="#傅里叶变换" class="headerlink" title="傅里叶变换"></a>傅里叶变换</h1><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411130922806.png"><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411130923749.png"><h2 id="构成波的基本单位是三角函数"><a href="#构成波的基本单位是三角函数" class="headerlink" title="构成波的基本单位是三角函数"></a>构成波的基本单位是三角函数</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411130926103.png"></p><h2 id="傅里叶变换的一般操作"><a href="#傅里叶变换的一般操作" class="headerlink" title="傅里叶变换的一般操作"></a>傅里叶变换的一般操作</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411130928391.png"></p><h2 id="时域-空间域-、频域-变换域"><a href="#时域-空间域-、频域-变换域" class="headerlink" title="时域(空间域)、频域(变换域)"></a>时域(空间域)、频域(变换域)</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411131014523.png"></p><h2 id="傅里叶变换的时域图、振幅谱、相位谱"><a href="#傅里叶变换的时域图、振幅谱、相位谱" class="headerlink" title="傅里叶变换的时域图、振幅谱、相位谱"></a>傅里叶变换的时域图、振幅谱、相位谱</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411130929136.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411130936670.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411130937068.png"></p><h3 id="相位谱详细解释"><a href="#相位谱详细解释" class="headerlink" title="相位谱详细解释"></a>相位谱详细解释</h3><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411131058494.png" style="zoom:50%;"><ul><li>鉴于正弦波是周期的，我们需要设定一个用来标记正弦波位置的东西。在图中就是那些小红点。小红点是距离频率轴最近的波峰，而这个波峰所处的位置离频率轴有多远呢？为了看的更清楚，我们将红色的点投影到下平面，投影点我们用粉色点来表示。当然，这些粉色的点只标注了波峰距离频率轴的距离，并不是相位。</li></ul><h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/19763358">傅里叶分析之掐死教程</a></li></ol><h1 id="机器学习中的知识点"><a href="#机器学习中的知识点" class="headerlink" title="机器学习中的知识点"></a>机器学习中的知识点</h1><h2 id="黑盒、白盒、单元、负载实验"><a href="#黑盒、白盒、单元、负载实验" class="headerlink" title="黑盒、白盒、单元、负载实验"></a>黑盒、白盒、单元、负载实验</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411251108117.png"></p><h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><h3 id="IoU（Intersection-over-Union）"><a href="#IoU（Intersection-over-Union）" class="headerlink" title="IoU（Intersection over Union）"></a>IoU（Intersection over Union）</h3><p>IoU(Intersection over Union)是一个比率，用于评估两个区域的重叠程度。</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411151331541.png"></p><h1 id="GitHub的使用"><a href="#GitHub的使用" class="headerlink" title="GitHub的使用"></a>GitHub的使用</h1><h2 id="使用requirements-txt配置环境"><a href="#使用requirements-txt配置环境" class="headerlink" title="使用requirements.txt配置环境"></a>使用requirements.txt配置环境</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411201327844.png"></p><p>cd命令进入指定文件夹内。</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411210924961.png"></p>]]></content>
      
      
      <categories>
          
          <category> 笔记资料 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>基于自监督深度学习的图像融合改进相关性超分辨率显微图像</title>
      <link href="/2024/11/11/%E5%9F%BA%E4%BA%8E%E8%87%AA%E7%9B%91%E7%9D%A3%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E6%94%B9%E8%BF%9B%E7%9B%B8%E5%85%B3%E6%80%A7%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E5%BE%AE%E5%9B%BE%E5%83%8F/"/>
      <url>/2024/11/11/%E5%9F%BA%E4%BA%8E%E8%87%AA%E7%9B%91%E7%9D%A3%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E6%94%B9%E8%BF%9B%E7%9B%B8%E5%85%B3%E6%80%A7%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E5%BE%AE%E5%9B%BE%E5%83%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>本文提出了一种用于强度和<span style="color: red;">超分辨率光学波动成像（SOFI）</span>显微图像融合的深度学习方法。</li><li>超分辨率光学波动成像（SOFI）是基于分析荧光探针自然波动的时间相关性。因此，它通常被用作更灵活的宽场模式，并且不需要样品中的任何特殊制备或发射器的稀疏性。原则上，SOFI的分辨率受到分析波动的顺序的限制。然而，通常情况下，由于发射器之间波动的可变性，SOFI仅用于二阶，从而产生高达两倍的分辨率提高。在这些限制下，SOFI已被证明是一种强大的工具，用于成像广泛的生物结构，包括微管、线粒体和染色质。</li><li>开发了一个相当简单的基于物理的深度学习方法来解决基于波动的成像中的图像融合问题。该方法是通用的，并结合了点扩展函数（psf）的物理限制。</li></ol><h2 id="融合强度和SOFI图像的网络架构"><a href="#融合强度和SOFI图像的网络架构" class="headerlink" title="融合强度和SOFI图像的网络架构"></a>融合强度和SOFI图像的网络架构</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411111338026.png"></p><ol><li>我们的目标是将两个信号，高信噪比强度图像和颗粒状SOFI图像相结合，以产生融合图像，平滑从SOFI获得的颗粒状图像，同时保持其高分辨率。在这两个场景中，如果没有Ground Truth或与成像对象信噪比相似的更高分辨率参考，则不能采用直接监督。</li><li>该网络由7层卷积神经网络（CNN）组成，采用ReLU激活函数引入非线性。网络的输入是强度和SOFI图像，输出是融合后的图像。融合后的图像被送入两个独立的线性CNN，每个CNN有3个隐藏层。对网络中不同的图像采用不同的损失项。</li><li>由于没有对预测信号的直接监督，将其馈送到两个独立的网络中，每个网络的目的是对预测的融合图像应用线性退化，目的是重建两个原始输入信号。</li><li>退化可以通过线性变换（也存在加性噪声）进行建模。</li></ol><h2 id="仿真结果及优化"><a href="#仿真结果及优化" class="headerlink" title="仿真结果及优化"></a>仿真结果及优化</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411111413650.png"></p><ol><li>二维微管表面的模拟图像，以及与图像右下角GT相比较的相应PSNR。(a)地真二值图像。(b)分布在GT图像上的发射体强度图像。(c)由求和得到图像(b)的时间序列得到的二阶SOFI图像。(d)将(b)和(c)作为输入图像输入网络，对图像(b)和(c)进行训练后得到的融合图像。(e) GT、强度、SOFI和融合图像的径向空间频率分量。(f)强度、SOFI和融合图像相对于GT图像的FRC。</li><li>各种模拟图像的空间频率含量如图2(e)所示。融合后的图像在达到噪声水平之前呈现出比SOFI图像更高的频率内容信号。并且在某一点上，在GT频率含量最高的地方，它超过了强度图像。</li><li>图2(f)给出了图像相对于地面真值的傅立叶环相关性（FRC）。这可以作为不同图像的频率内容与GT图像相似度的指标。融合后的图像具有较高的相关值，在所有图像中与图像结构的原始频率内容最为相似。</li></ol><h2 id="用共聚焦装置拍摄的多张微管图像"><a href="#用共聚焦装置拍摄的多张微管图像" class="headerlink" title="用共聚焦装置拍摄的多张微管图像"></a>用共聚焦装置拍摄的多张微管图像</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411111453992.png"></p><ol><li>(a)微管的ISM图像。(b)同一场景的SOFISM图像，图像噪声较大。(c)在(a)和(b)上训练的网络的融合图像。(d) (a)(b)(c)的横截面，用绿色虚线标记，显示了SOFISM区分ISM无法分辨的特征的能力，并且表明融合后的图像也可以区分这些特征，信噪比更好。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411111457019.png"></p><ol><li>该网络对用相同设置拍摄的另一对微管图像进行训练。(a)网络训练的强度图像。(b)网络训练的SOFI图像。(c)将图3所示的强度和SOFI图像作为输入，训练后的网络得到的融合图像。(d)在同一场景ISM和SOFISM图像（与图3 (c)相同）上训练的网络得到的融合图像。(e)图3 (c)(d)、(a)、(b)中包含的空间频率信息。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="缩放因子（Scaling-Factor）"><a href="#缩放因子（Scaling-Factor）" class="headerlink" title="缩放因子（Scaling Factor）"></a>缩放因子（Scaling Factor）</h2><p>在图像处理和超分辨率重建（Super-Resolution, SR）任务中，<strong>缩放因子</strong>通常用来表示<strong>输入低分辨率（LR）图像和输出高分辨率（HR）图像之间的空间分辨率差异</strong>。典型的缩放因子有 2×、4×、8× 等，表示图像在横向和纵向的尺寸被放大2倍、4倍或8倍。</p><ul><li><p><strong>解释</strong>：</p><ul><li><strong>缩放因子 &#x3D; 2</strong>：假设原始低分辨率图像的大小为 100 × 100 像素，经过放大后，高分辨率图像的大小为 200 × 200 像素。</li><li><strong>缩放因子 &#x3D; 4</strong>：如果低分辨率图像为 100 × 100 像素，则高分辨率图像为 400 × 400 像素。</li></ul></li><li><p><strong>公式</strong>：</p><p>HR 图像尺寸 &#x3D; LR 图像尺寸 × 缩放因子</p></li></ul><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><p>无。</p><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Improving correlation based super-resolution microscopy images through image fusion by self-supervised deep learning</p><p>基于自监督深度学习的图像融合改进相关性超分辨率显微图像</p><p>期刊：OE</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>Beck, Lior M., et al. “Improving Correlation Based Super-Resolution Microscopy Images through Image Fusion by Self-Supervised Deep Learning.” Optics Express 32.16 (2024). Print.</li></ul><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：LIOR M. BECK，ASSAF SHOCHER，URI ROSSMAN，ARIEL HALFON，MICHAL IRANI，AND DAN ORON</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2024年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年11月11日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：无。</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 自监督学习 </tag>
            
            <tag> 波动成像(SOFI) </tag>
            
            <tag> 图像融合 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FDTD</title>
      <link href="/2024/11/01/FDTD/"/>
      <url>/2024/11/01/FDTD/</url>
      
        <content type="html"><![CDATA[<p><span style="color: red;">解决方法：</span></p><h1 id="监视器E、H、P、T参数"><a href="#监视器E、H、P、T参数" class="headerlink" title="监视器E、H、P、T参数"></a>监视器E、H、P、T参数</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410101048270.png"></p><h1 id="Mesh的仿真优先级大于FDTD"><a href="#Mesh的仿真优先级大于FDTD" class="headerlink" title="Mesh的仿真优先级大于FDTD"></a>Mesh的仿真优先级大于FDTD</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410101420656.png"></p><h1 id="页面网格的显示与隐藏"><a href="#页面网格的显示与隐藏" class="headerlink" title="页面网格的显示与隐藏"></a>页面网格的显示与隐藏</h1><p>网格显示：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411011002383.png"></p><p>网格隐藏：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411011011706.png"></p><h1 id="实例计算反射率，透过率，吸收率"><a href="#实例计算反射率，透过率，吸收率" class="headerlink" title="实例计算反射率，透过率，吸收率"></a>实例计算反射率，透过率，吸收率</h1><h2 id="透过率为负，怎么修改为正"><a href="#透过率为负，怎么修改为正" class="headerlink" title="透过率为负，怎么修改为正"></a>透过率为负，怎么修改为正</h2><p>如图仿真得到的结果透射率为负数：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411061431820.png"></p><p>这样修改：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411061434573.png"></p><h2 id="怎么将两条曲线显示在一起？"><a href="#怎么将两条曲线显示在一起？" class="headerlink" title="怎么将两条曲线显示在一起？"></a>怎么将两条曲线显示在一起？</h2><p>如下图选择将什么参数添加到visualizer 1中：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411061439698.png"></p><p>添加成功：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411061441476.png"></p><h2 id="编辑脚本计算吸收率"><a href="#编辑脚本计算吸收率" class="headerlink" title="编辑脚本计算吸收率"></a>编辑脚本计算吸收率</h2><p>代码：</p><p>&#96;f &#x3D; getdata(“R”,”f”);</p><p>T &#x3D; transmission(“T”);</p><p>R &#x3D; -transmission(“R”);</p><p>A &#x3D; 1-T-R;</p><p>plot(c&#x2F;f*1e6,A,T,R,”wavelength um”,”T,R”);</p><p>legend(“A”,”T”,”R”);&#96;</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411061457195.png"></p>]]></content>
      
      
      <categories>
          
          <category> 软件操作 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>L-Edit</title>
      <link href="/2024/10/29/L-Edit/"/>
      <url>/2024/10/29/L-Edit/</url>
      
        <content type="html"><![CDATA[<h1 id="格点距离"><a href="#格点距离" class="headerlink" title="格点距离"></a>格点距离</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410141509171.png" alt="image-20241014150946079"></p><h1 id="图形的重复复制"><a href="#图形的重复复制" class="headerlink" title="图形的重复复制"></a>图形的重复复制</h1><p>  <img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410211007673.png" alt="image-20241021100619637"></p><h1 id="添加数字、字母"><a href="#添加数字、字母" class="headerlink" title="添加数字、字母"></a>添加数字、字母</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410211025332.png" alt="image-20241021102514041"></p><h1 id="文件的导出与检查"><a href="#文件的导出与检查" class="headerlink" title="文件的导出与检查"></a>文件的导出与检查</h1><h2 id="导出GDSII文件"><a href="#导出GDSII文件" class="headerlink" title="导出GDSII文件"></a>导出GDSII文件</h2><p>1.填写两个参数，随意填写。(不填写这两个参数会导致导出的GDSII文件为空)</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410291343769.png"></p><p>2.点击File—Export Mask Data—GDSII，导出版图。</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410291347559.png"></p><p><strong>参考博客：</strong><a href="https://zhuanlan.zhihu.com/p/354214410">Tanner L-Edit 系列教程：05 导出GDSII文件</a></p><h2 id="当样品为圆时报错Circle-is-too-small"><a href="#当样品为圆时报错Circle-is-too-small" class="headerlink" title="当样品为圆时报错Circle is too small"></a>当样品为圆时报错Circle is too small</h2><p>当设计的样品为这样时：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410291111020.png"></p><p>导出GDSII文件出现以下报错时：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410291112821.png"></p><p><span style="color: red;">解决方法：</span></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410291114969.png" alt="image-20241029111402602"></p><p>再次导出GDSII文件时：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410291115507.png"></p><p><strong>参考博客：</strong><a href="https://blog.csdn.net/weixin_42510019/article/details/112523862">gds文件 导出_Ledit导出GDS时，圆变成了多边形应该怎么办？</a></p><h1 id="怎么在板子上面挖孔？"><a href="#怎么在板子上面挖孔？" class="headerlink" title="怎么在板子上面挖孔？"></a>怎么在板子上面挖孔？</h1><p>如图，要在淡绿色面板上挖出左边红色类型的孔洞。</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411011437717.png"></p><p>1.使其位置重叠，并全部选中。</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411011439734.png"></p><p>2.如下步骤。</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411011456373.png"></p><p>3.挖孔成功。</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411011457729.png"></p><p>参考视频：<a href="https://www.bilibili.com/video/BV1jS4y1472N/?spm_id_from=333.337.search-card.all.click&vd_source=990e26b127a5aaac7fadaf302599f42a">使用L-Edit画掩模版的快速上手教程</a></p><h1 id="掩模版的制作"><a href="#掩模版的制作" class="headerlink" title="掩模版的制作"></a>掩模版的制作</h1><h2 id="一维光栅和样品的制备"><a href="#一维光栅和样品的制备" class="headerlink" title="一维光栅和样品的制备"></a>一维光栅和样品的制备</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410281522043.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410281523800.png"></p><h2 id="两种制作方法"><a href="#两种制作方法" class="headerlink" title="两种制作方法"></a>两种制作方法</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410311415166.png"></p><h2 id="用NMP去除多余的铬"><a href="#用NMP去除多余的铬" class="headerlink" title="用NMP去除多余的铬"></a>用NMP去除多余的铬</h2><p>NMP(N-甲基吡咯烷酮)高级溶剂，是选择性强和稳定性好的极性溶剂，高精密电子、电路板、锂电池的优良清洗剂。</p><ol><li>将制作好的光栅放置在倒有NMP溶液的烧杯中(半小时左右，时间越长越好)。</li><li>将整个烧杯放置在超声波仪器中(频率50)，进行去铬。</li></ol><h2 id="当设计图案重叠但放大时却没有什么问题时"><a href="#当设计图案重叠但放大时却没有什么问题时" class="headerlink" title="当设计图案重叠但放大时却没有什么问题时"></a>当设计图案重叠但放大时却没有什么问题时</h2><p>设计的图案是这样：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411042136025.png"></p><p>但流片时却出现这样的情况：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411042139606.png"></p><p><span style="color: red;">解决方法：</span></p><ol><li>先检查自己复制粘贴的时候有没有一个图案有很多层。</li><li>GDSII文件导出时，导出要使用的那个层，不要把所有的层都导出。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411042145843.png"></p><h2 id="顶点数超出了GDSII规范所允许的范围"><a href="#顶点数超出了GDSII规范所允许的范围" class="headerlink" title="顶点数超出了GDSII规范所允许的范围"></a>顶点数超出了GDSII规范所允许的范围</h2><p>GDSII格式对于多边形的顶点数有一个上限，而在你的设计中，某个多边形的顶点数量超出了GDSII的处理能力：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411061942813.png"></p><p><span style="color: red;">解决方法：</span></p><p>导出GDSII文件时勾选</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411061943315.png"></p><p>参考博客：<a href="https://zhuanlan.zhihu.com/p/354214410"><a href="https://www.zhihu.com/column/c_1350370264887336960">Tanner L-Edit 系列教程</a></a></p><h2 id="二维光栅样品制备"><a href="#二维光栅样品制备" class="headerlink" title="二维光栅样品制备"></a>二维光栅样品制备</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411261023817.png"></p>]]></content>
      
      
      <categories>
          
          <category> 软件操作 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>用于荧光显微镜成像的无配准3D超分辨率生成深度学习网络</title>
      <link href="/2024/10/28/%E7%94%A8%E4%BA%8E%E8%8D%A7%E5%85%89%E6%98%BE%E5%BE%AE%E9%95%9C%E6%88%90%E5%83%8F%E7%9A%84%E6%97%A0%E9%85%8D%E5%87%863D%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E7%94%9F%E6%88%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C/"/>
      <url>/2024/10/28/%E7%94%A8%E4%BA%8E%E8%8D%A7%E5%85%89%E6%98%BE%E5%BE%AE%E9%95%9C%E6%88%90%E5%83%8F%E7%9A%84%E6%97%A0%E9%85%8D%E5%87%863D%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E7%94%9F%E6%88%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>图像超分辨率（SR）方法为从低分辨率（LR）图像中恢复HR图像提供了一种有效的方法。这些方法需要像素级配准的LR和HR图像，这对准确的图像配准提出了挑战。为了解决这些问题，我们提出了一种新的无配准图像SR方法：直接对未配准的LR和HR体积神经元图像进行SR训练和预测。该网络建立在CycleGAN框架和基于注意机制的3D UNet基础上。</li><li>提出了一种<span style="color: red;">无配准的超分辨率生成深度学习网络（RFSRGDN）</span>，实现了无需图像配准的3D脑显微镜光学图像的无监督超分辨率重建。该网络基于CycleGAN框架，将超分辨率视为LR&#x2F;HR图像域自适应，用于无监督超分辨率训练。构建了基于注意机制的三维U-Net网络作为骨干网络，对神经元区域进行准确的聚焦。我们将3D感知损失与GAN损失相结合来捕获HR图像内容特征。</li></ol><h2 id="无配准图像超分辨率流程"><a href="#无配准图像超分辨率流程" class="headerlink" title="无配准图像超分辨率流程"></a>无配准图像超分辨率流程</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410311314374.png"></p><ol><li>低分辨率（LR）和高分辨率（HR）图像数据集由两种成像系统从同一脑组织的相似区域获得。</li><li>在训练阶段，我们分别在LR和HR数据集中随机裁剪图像块。HR图像的大小是LR图像的3 × 3 × 3倍。然后将随机裁剪的LR&#x2F;HR图像直接输入到RFSRGDN中。训练完成后，我们将LR图像输入到RFSRGDN中的超分辨率（SR）网络中，得到超分辨率图像。</li></ol><h2 id="RFSRGDN网络结构"><a href="#RFSRGDN网络结构" class="headerlink" title="RFSRGDN网络结构"></a>RFSRGDN网络结构</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410311319859.png"></p><ol><li>(A)训练阶段，G(AB)生成假B（SR）图像，然后将假B （SR）图像与真实HR图像一起输入D(B)。D(B)产生GAN损失和感知损失。F(BA)使用假的B （SR）图像生成周期A图像。周期A图像要求与真实LR图像一致。HR图像的处理方法相同。忽略D(A)的知觉丧失。在推理阶段，GAB将真实的LR图像转换为SR图像。(B) RFSRGDN中超分辨率网络的基本结构。</li><li>RFSRGDN建立在CycleGAN框架之上。CycleGAN可以在不需要像素级映射的情况下实现不同图像域的变换。我们利用CycleGAN将低分辨率（LR）图像域转换为高分辨率（HR）图像域，从而实现无配准的图像超分辨率（SR）。</li><li>RFSRGDN由四个网络组成：超分辨率生成网络GAB、退化网络FBA、低分辨率图像鉴别网络DA和高分辨率图像鉴别网络DB（图2 (A)）。GAB是用于图像超分辨率的目标超分辨率网络。FBA可以将HR图像降级为LR图像。训练DA来区分LR&#x2F;退化图像。DB被训练来区分HR&#x2F;SR图像。FBA和GAB的训练导致生成的退化和SR图像接近真实的HR和LR图像，使得DA和DB难以区分。这种训练权衡将推动GAB生成高质量的SR图像。</li><li>GAB采用基于注意机制的三维UNet网络作为头部网络，然后利用3 × 3 × 3三线性上采样层和2个卷积层输出结果。由于体积神经元图像的前景比相对较小，引入注意机制捕获感兴趣区域的信息。GAB的详细结构如图2(B)所示。注意机制的设计在文献[14]中有描述。FBA、DA和DB都是全卷积网络结构。FBA包括6个卷积层和1个最大池化层。DA包括5个卷积层和3个最大池化层。DB包括11个卷积层和5个最大池化层。所有网络中卷积层的核大小均为3 × 3 × 3。</li></ol><h2 id="数据集信息"><a href="#数据集信息" class="headerlink" title="数据集信息"></a>数据集信息</h2><ol><li>我们选择了1张1000 × 1000 × 450体素的LR图像和3张1800 × 1100 × 400体素的HR图像作为训练数据集。高分辨率和低分辨率图像都在皮层区域附近被裁剪。将低分辨率和高分辨率图像随机裁剪为48 × 48 × 48体素和144 × 144 × 144体素大小的图像块，送入RFSRGDN进行训练，不进行配准。</li><li>训练的初始学习率设置为5 × 10−4，使用步长调度器进行学习率迭代，gamma值为0.9。采用Adam优化器对超分辨率生成网络和图像退化网络进行了优化。整个网络是使用PyTorch构建的。培训在Intel I7 12700F和NVIDIA RTX 3090平台上进行。我们训练网络超过300次，大约14小时，以达到超分辨率。</li></ol><h2 id="方法比较"><a href="#方法比较" class="headerlink" title="方法比较"></a>方法比较</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410311350108.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410311354843.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410311359881.png"></p><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><p>无</p><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><p>无</p><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：用于荧光显微镜成像的无配准3D超分辨率生成深度学习网络</p><p>Registration-free 3D super-resolution generative deep-learning network for fluorescence microscopy imaging</p><p>期刊：OL</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>Zhou, H., et al. “Registration-Free 3d Super-Resolution Generative Deep-Learning Network for Fluorescence Microscopy Imaging.” Opt Lett 48.23 (2023): 6300-03. Print.</li></ul><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Hang Zhou，Yuxin Li， Bolun Chen，Hao Yang，Maoyang Zou，Wu Wen，Yayu Ma，Min Chen</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2023年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年10月28日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：<a href="https://github.com/artzers/RFSRGDN">https://github.com/artzers/RFSRGDN</a> (作者)</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
            <tag> U-Net </tag>
            
            <tag> 无监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>两种光照条件下RGB图像的无监督光谱重建</title>
      <link href="/2024/10/27/%E4%B8%A4%E7%A7%8D%E5%85%89%E7%85%A7%E6%9D%A1%E4%BB%B6%E4%B8%8BRGB%E5%9B%BE%E5%83%8F%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E5%85%89%E8%B0%B1%E9%87%8D%E5%BB%BA/"/>
      <url>/2024/10/27/%E4%B8%A4%E7%A7%8D%E5%85%89%E7%85%A7%E6%9D%A1%E4%BB%B6%E4%B8%8BRGB%E5%9B%BE%E5%83%8F%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E5%85%89%E8%B0%B1%E9%87%8D%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>无监督光谱重建（Unsupervised spectral reconstruction， SR）的目的是在不加注释的情况下，从相应的RGB图像中恢复<span style="color: red;">高光谱图像(HSI)</span>。</li><li>从两种照明下捕获的一对RGB图像重建HSI，显着提高了重建精度。</li><li>首先构建了基于两种光照的SR迭代模型。通过展开求解该SR模型的近端梯度算法，提出了一个可解释的无监督深度网络。网络中各模块具有精确的物理意义，使网络具有优越的性能和良好的泛化能力。在两个公共数据集和我们的真实世界图像上的实验结果表明与最先进的方法相比，所提出的方法在视觉和定量上都有显著提高。</li><li>基于双光照的SR问题迭代模型。基于近端梯度下降（PGD）算法，提出一种可解释无监督网络结构DSR-Net来求解该模型。在PGD中，采用嵌入自编码器作为近端算子，而不是传统的软线程算子。从而增强了HSI深度先验的探测能力，有利于图像重建。此外，DSR-Net是通过展开物理成像模型进行专门设计的，确保每个网络模块具有明确的物理含义，具有良好的可解释性和泛化能力。</li></ol><h2 id="DSR-Net网络架构"><a href="#DSR-Net网络架构" class="headerlink" title="DSR-Net网络架构"></a>DSR-Net网络架构</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410271528030.png"></p><h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410271532707.png"></p><ul><li>我们首先进行消融实验，以评估在SR中引入照明的影响。如图2(a)所示，在性能上取得的巨大收益证实了增加照明的有效性。这也说明了增加光照是提高光谱恢复的一种方便有效的方法。在图2(b)中，无论增加多少照明，我们的方法总是在其他方法中表现最好，这表明了我们的双照明SR的优越性。</li></ul><h2 id="评估深度网络的性能"><a href="#评估深度网络的性能" class="headerlink" title="评估深度网络的性能"></a>评估深度网络的性能</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410271537093.png"></p><ul><li>为了保证实验的公平性，所有的深度网络都使用相同的损失函数进行训练，确保所有的方法都能从两幅RGB图像中恢复光谱。竞争方法在CAVE和Harvard数据集上的平均定量结果如表1所示。值得注意的是，所提出的深度网络约有27万个参数，得到了最好的结果，证明了我们的网络的实用性。具体来说，与SAM、PSNR、SSIM和RMSE的次优结果相比，我们的CAVE数据集分别提高了27.12%、4.81%、0.41%和10.00%。同样，在哈佛数据集上，我们的表现比第二好的HSCNN+高出12.02%</li></ul><h2 id="误差图与重构谱"><a href="#误差图与重构谱" class="headerlink" title="误差图与重构谱"></a>误差图与重构谱</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410271541888.png"></p><ul><li>在图3中给出了相应的误差图和重构谱。图3(a)描绘了SAM误差图和平均绝对误差（MAE）图，SAM和MAE值列在相应图像的右下角。SAM和MAE误差反映了重建图像与地面真实值的光谱相似性和值差异。我们的方法在图3(a)中显示了更好的边缘细节和整体结构，验证了我们的方法的优越性。</li><li>在图3(b)中，我们随机选取光滑区域，绘制重建hsi的平均光谱曲线。同时给出了重建光谱与地面真值的相关系数（Corrs）。我们的方法更接近真实情况，其相关系数最高，证明了所提出方法的有效性。</li></ul><h2 id="真实世界的图像"><a href="#真实世界的图像" class="headerlink" title="真实世界的图像"></a>真实世界的图像</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410271546015.png"></p><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><ol><li>其中Fubara等[10]直接学习一组<span style="color: red;">光谱基函数</span>，通过U-Net从RGB图像中预测权重，重建了HSI。由于RGB图像中光谱信息的严重缺乏，在没有任何约束的情况下，从单幅RGB图像中学习光谱基函数会造成严重的光谱失真，为了缓解这一问题，许多研究者利用了HSI中相邻波段之间的高结构相似性（SSIM）的固有特性，尽管有了很大的改进，但RGB图像严重的光谱信息丢失使得以无监督的方式从单个RGB图像重建HSI变得困难。</li><li>现有的多种基于图像的SR方法仍然需要在大量高光谱图像(HSI)的监督下训练其网络。更重要的是，如果没有物理成像模型的指导，现有的SR网络往往是黑盒子，缺乏透明度，这阻碍了它们的性能和泛化。</li></ol><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：两种光照条件下RGB图像的无监督光谱重建</p><p>Unsupervised spectral reconstruction from RGB images under two lighting conditions</p><p>期刊：OL</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>Cao, X., et al. “Unsupervised Spectral Reconstruction from Rgb Images under Two Lighting Conditions.” Opt Lett 49.8 (2024): 1993-96. Print.</li></ul><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Xuheng Cao，Yusheng Lian，Zilong Liu，Jin Li，Kaixuan Wang</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2024年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年10月27日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：无。</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无监督学习 </tag>
            
            <tag> 光谱重建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于双重学习和深傅立叶通道关注的光场图像超分辨率</title>
      <link href="/2024/10/25/%E5%9F%BA%E4%BA%8E%E5%8F%8C%E9%87%8D%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%82%85%E7%AB%8B%E5%8F%B6%E9%80%9A%E9%81%93%E5%85%B3%E6%B3%A8%E7%9A%84%E5%85%89%E5%9C%BA%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
      <url>/2024/10/25/%E5%9F%BA%E4%BA%8E%E5%8F%8C%E9%87%8D%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%B7%B1%E5%82%85%E7%AB%8B%E5%8F%B6%E9%80%9A%E9%81%93%E5%85%B3%E6%B3%A8%E7%9A%84%E5%85%89%E5%9C%BA%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ul><li>提出了一种利用双重学习和深度FCA的新型DLISN方法。双重学习模块适应配对和非配对数据，减少对高分辨率图像的依赖。深度FCA机制提高了处理复杂场景的性能。</li><li><span style="color: red;">Disentangling LF Image SR Network (DLISN，解缠LF图像SR网络)</span>。</li></ul><h2 id="网络结构-DLISN"><a href="#网络结构-DLISN" class="headerlink" title="网络结构(DLISN)"></a>网络结构(DLISN)</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410251055329.png"></p><h3 id="傅里叶通道注意力机制"><a href="#傅里叶通道注意力机制" class="headerlink" title="傅里叶通道注意力机制"></a>傅里叶通道注意力机制</h3><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410251107474.png"></p><ul><li>FCA机制解决了低分辨率（LR）和高分辨率（HR）图像在功率谱覆盖上的显著差异，这种差异可能超过了空间结构的差异。为了在连接特征后纳入FCA。</li><li>这里，FFT（·）表示快速傅里叶变换。功率运算涉及一个参数γ，其中小于1的值用于抑制高频信息，增强低频信息，大于1的值用于强调高频信息，抑制低频信息。γ的选择可以根据具体任务进行定制，我们的工作设置为γ到1。</li></ul><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410251343937.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410251344361.png"></p><h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410251337832.png"></p><ul><li>我们还通过选择性地从网络中移除DLISN中的双模块和FCA机制来证明DLISN的有效性。我们引入了一个不使用双重学习的网络（即模型1）和一个不使用df块的网络（即模型2）。表3显示了比较的PSNR结果。</li><li>我们比较了模型1和模型0的性能，以验证DF块的有效性。如表3所示，模型1的平均PSNR比模型0提高了0.09 dB。这些实验表明，我们的DF块捕获了更多有利于SR的信息。</li><li>我们比较了模型2和模型0的性能，以验证双重学习的有效性。如表3所示，模型2的平均PSNR比模型0提高了0.01 dB。这些实验证明了双模块的有效性。</li></ul><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="双重学习"><a href="#双重学习" class="headerlink" title="双重学习"></a>双重学习</h2><p>双重学习（Dual Learning）是一种机器学习方法，主要用于增强模型在某些任务上的表现，如机器翻译和图像处理。它的基本思想是通过双向学习来提高模型的性能，通常涉及以下两个方面：</p><ol><li><strong>正向学习</strong>：模型从源域（例如，某种语言或图像类型）学习到目标域（例如，另一种语言或图像类型）的映射。</li><li><strong>反向学习</strong>：同时，模型也从目标域学习回源域的映射。这种双向的学习过程可以相互促进，从而提高最终的学习效果。</li></ol><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><ol><li>基于深度学习的超分辨率（SR）技术在提高光场成像（LF）图像分辨率方面显示出相当大的优势。然而，获取丰富的结构信息和重建复杂纹理细节的固有挑战仍然存在，特别是在空间和角度信息错综复杂交织的情况下。</li><li>傅立叶通道注意（FCA）机制有助于提取与不同结构相关的高频信息，有助于提高空间分辨率。</li></ol><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：基于双重学习和深傅立叶通道关注的光场图像超分辨率</p><p>Light field image super-resolution based on dual learning and deep Fourier channel attention</p><p>期刊：OL</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>Ma, Jian, et al. “Light Field Image Super-Resolution Based on Dual Learning and Deep Fourier Channel Attention.” Optics Letters 49.11 (2024). Print.</li></ul><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Jian Ma, Zhipeng Li</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2024年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年10月25日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：无</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 双重学习 </tag>
            
            <tag> 傅里叶 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>元学习下基于多区域深度特征融合的多类型图像质量评估</title>
      <link href="/2024/10/17/%E5%85%83%E5%AD%A6%E4%B9%A0%E4%B8%8B%E5%9F%BA%E4%BA%8E%E5%A4%9A%E5%8C%BA%E5%9F%9F%E6%B7%B1%E5%BA%A6%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E7%9A%84%E5%A4%9A%E7%B1%BB%E5%9E%8B%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BC%B0/"/>
      <url>/2024/10/17/%E5%85%83%E5%AD%A6%E4%B9%A0%E4%B8%8B%E5%9F%BA%E4%BA%8E%E5%A4%9A%E5%8C%BA%E5%9F%9F%E6%B7%B1%E5%BA%A6%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88%E7%9A%84%E5%A4%9A%E7%B1%BB%E5%9E%8B%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BC%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>本文提出了一种基于<span style="color: red;">元学习（MMQA）</span>下多区域深度特征融合的多类型图像质量评估方法。首先，我们利用结构信息的差异来筛选出显著和非显著区域。其次，设计深度多流网络，模拟<span style="color: red;">人类视觉系统(HVS)</span>，综合考虑和融合显著区域、非显著区域和整个图像中与质量相关的不同特征。第三，应用元学习，在面对新的图像类型时，快速学习和更新模型的参数。</li><li>设计了一个基于显著和非显著区域的深度多流网络来模拟HVS。该方法旨在用一种较轻的区域增强方法来提高模型性能。</li><li>为了模拟人类的学习风格，提出了一个元学习框架来总结先验知识。该模型可以利用先验知识快速处理多类型任务。</li><li>该方法可以满足多种类型的图像质量评估任务，具有泛化和鲁棒性的优点。该方法在多类型任务中的准确性也相当高。</li></ol><h2 id="非参考图像质量评估-NR-IQA"><a href="#非参考图像质量评估-NR-IQA" class="headerlink" title="非参考图像质量评估(NR-IQA)"></a>非参考图像质量评估(NR-IQA)</h2><ol><li>非参考图像质量评估（Non-Reference Image Quality Assessment，简称NR-IQA）是一种无需参考图像（即未失真图像）的图像质量评估方法。与需要参考图像的全参考（Full-Reference，FR-IQA）方法不同，NR-IQA方法仅依赖失真的图像本身，评估图像的质量，无需原始的“金标准”图像。</li></ol><h2 id="Deep-meta-learning"><a href="#Deep-meta-learning" class="headerlink" title="Deep meta-learning"></a>Deep meta-learning</h2><p>元学习(Meta-Learning)，也被称为“学习如何学习”，是一种机器学习方法，它的目标是通过学习一些通用的知识和策略，使模型能够更快、更高效地适应新任务。元学习的核心思想是利用以往学习到的经验，来提高模型在新任务上的学习效率，尤其是当新任务的数据量较少时。</p><h3 id="元学习的基本原理"><a href="#元学习的基本原理" class="headerlink" title="元学习的基本原理"></a>元学习的基本原理</h3><p>元学习通常包括两个学习过程：</p><ol><li><strong>元层次学习（Meta-level learning）</strong>：在元层次上，模型学习到一些高层次的知识，如如何快速适应不同的任务、如何优化模型参数、或者如何选择最适合当前任务的模型架构。这个阶段的学习是在大量不同任务上进行的，使模型能够积累“学习如何学习”的经验。</li><li><strong>任务层次学习（Task-level learning）</strong>：在任务层次上，模型使用元层次学到的知识快速适应某个具体任务，甚至只需少量样本（少样本学习）。这种任务通常是新出现的、之前未见过的任务。</li></ol><h2 id="MMQA流程概述"><a href="#MMQA流程概述" class="headerlink" title="MMQA流程概述"></a>MMQA流程概述</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410171342859.png"></p><ol><li>这包括我们方法的五个主要部分：(1)筛选出显著和非显著区域；(2)提取不同输入的特征；(3)特征融合和线性预测；(4)对未知扭曲进行微调；(5)通过元学习总结先验知识。为了获得先验知识，需要对当前任务训练一个高质量的先验模型。在训练过程中，MF学习深度多流网络如何更新参数，然后总结先验知识。在处理新类型任务时，该方法利用先验知识对质量模型进行快速微调。</li></ol><h2 id="显著区域和非显著区域"><a href="#显著区域和非显著区域" class="headerlink" title="显著区域和非显著区域"></a>显著区域和非显著区域</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410171409042.png"></p><ol><li>我们将显著性分数最大的图像块作为显著区域，最小的图像块作为非显著区域。为了可视化该方法在筛选显著区域和非显著区域方面的效果，我们选择KADID中的一些图像作为示例，分别使用红色和绿色背景框定位显著区域和非显著区域。具体情况如图2所示。</li></ol><h3 id="显著性分数计算"><a href="#显著性分数计算" class="headerlink" title="显著性分数计算"></a>显著性分数计算</h3><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410171412385.png"></p><ol><li>为了突出不同块与整个图像之间的关系，该方法使用显著分数来表示局部变化与全局信息之间的相关性。数字i块的显著性分数由式3计算。</li></ol><h2 id="Extracting-features-of-different-inputs"><a href="#Extracting-features-of-different-inputs" class="headerlink" title="Extracting features of different inputs"></a>Extracting features of different inputs</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410171419845.png"></p><ol><li>该方法将显著区域、非显著区域和整幅图像作为输入。通过兼顾整体特征，网络可以增强对关键要素的关注。在特征提取过程中，感受野发生变化，逐渐呈现出不同大小的特征表征。随着网络的发展，浅层特征和深层特征之间的关系将逐渐减弱。为了更好地连接不同深度的特征信息，我们设计了一个层间交互更频繁的网络，其中每个分支交替由三个块组成，交替由三个过渡组成。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="合成失真和真实失真"><a href="#合成失真和真实失真" class="headerlink" title="合成失真和真实失真"></a>合成失真和真实失真</h2><p><strong>合成失真</strong>：这是通过人为的方式在图像上引入的失真，常见于传统的IQA数据集。常见的合成失真包括：</p><ul><li><strong>压缩失真</strong>：如JPEG压缩、JPEG2000压缩等，会导致图像出现块效应或模糊。</li><li><strong>模糊</strong>：通过添加高斯模糊、运动模糊等来模拟相机抖动或对焦不准等情况。</li><li><strong>噪声</strong>：添加高斯噪声、椒盐噪声等来模拟图像的传输或传感器噪声。</li><li><strong>色偏</strong>：通过改变图像的色彩通道来模拟光线或色彩失真。</li></ul><p><strong>真实失真</strong>：这些失真是由于实际拍摄或传输过程中的各种不可控因素造成的，不是人为合成的。真实失真通常更加复杂且多样化，难以精确模拟，常见于实际应用场景中。真实失真包括：</p><ul><li><strong>传感器噪声</strong>：由成像设备本身的限制引起的噪声，例如在低光照环境下拍摄的图像。</li><li><strong>透视失真</strong>：由于拍摄角度或镜头畸变引起的失真。</li><li><strong>光线失真</strong>：如过曝或欠曝的情况。</li><li><strong>其他复杂失真</strong>：如多种失真同时存在，或由于设备、传输、环境等综合因素导致的失真。</li></ul><h2 id="可以参考的图例"><a href="#可以参考的图例" class="headerlink" title="可以参考的图例"></a>可以参考的图例</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410171619132.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410171620626.png"></p><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><h3 id="关于NR-IQA"><a href="#关于NR-IQA" class="headerlink" title="关于NR-IQA"></a>关于NR-IQA</h3><ul><li>早期基于深度学习的NR-IQA使用深度卷积神经网络（DCNNs）捕获深度特征，并在IQA任务中表现良好。Kang等人[39]描述了一种基于卷积神经网络（CNN）的方法，其中整个网络由一个最大池化层、两个完全连接的卷积层和一个输出节点组成。该网络将特征学习和回归结合到一个优化过程中，形成一个更有效的IQA模型进行训练。Bosse等人[40]设计的特征提取网络具有明显的深度。它采用端到端训练模式，由10个卷积层、5个特征提取池化层和2个全连接回归层组成。该模型在学习图像的局部信息方面更加稳定和准确。这些方法旨在提高特征提取的能力。因此，他们可以捕捉失真和质量之间的关系，在合成失真。但这些方法在处理较为复杂的真实失真时表现不佳，泛化程度有待提高。这使得它们无法处理具有多类型任务的应用程序场景。</li><li>一些多尺度方法通过丰富特征信息来提高性能。Zhang等[41]提出了一种利用不同深度网络提取特征的方法，使用不同的浅层特征作为输入，增强对语义信息的分析。然而，多流网络的作用并不能得到合理的解释。Hu等[22]利用图像的空间信息和梯度域信息综合表达畸变信息。然而，模型的鲁棒性有待提高。此外，Zhang等人[42]利用文本信息作为新的尺度来帮助模型捕捉失真细节并基于余弦相似度计算联合概率。虽然可以增强模型定位重要区域畸变的能力，但该方法依赖于大量的标签数据。Karimi等人[26]检测了显著区域是如何被保留的，以及相关特征产生了多少几何畸变。Feng等人[25]引入了一种高效的特征学习框架。使用从未标记图像的显著区域提取的图像块来学习稀疏编码字典。这些方法通过添加显著区域来提高模型的性能，但无法分析显著区域、非显著区域与全局特征之间的相关性。由于这个缺点，语义分析不太完整。</li><li>为了更好地研究真实失真中的语义信息，更多的方法注重分析图像之间的深层关系。Su等[43]提出了一种自适应超网络架构来盲评估野外图像质量。在提取图像语义后，使用超网络自适应构建感知规则，然后使用质量预测网络进行感知。该方法通过增强语义分析和参数共享，在真实失真方面取得了进一步的效果。随着Transformer的广泛应用，结合Transformer[44] -[48]的NR-IQA方法有了新的发展。Ke等[45]提出了一种多尺度图像质量转换器（MUSIQ）来处理不同尺度的图像失真。此外，针对图像块位置嵌入问题，他们提出了一种新的基于哈希的二维空间嵌入和基于哈希的尺度嵌入，以支持多尺度表示中的位置嵌入。Golestaneh等[46]提出了一种基于CNN和自注意机制的NR-IQA模型，可用于从图像中提取局部特征和全局语义信息。Qin等人[47]引入了一种Transformer Decoder，可以从不同角度对感知到的信息进行细化。同时，引入了一种新的注意面板机制来模拟人的主观评价行为。Saha等人[48]提出了一种混合方法，训练两个独立的编码器，在无监督环境中学习图像的高级语义和浅层特征。</li></ul><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Multi-type Image Quality Assessment based on Multi-region Deep Feature Fusion under Meta-learning</p><p>期刊：无</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：朱顺(南京师范大学)</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：暂无</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年10月17日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：<a href="https://github.com/dart-into/MMQA">https://github.com/dart-into/MMQA</a> (作者)</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像质量评价(IQA) </tag>
            
            <tag> 元学习 </tag>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>光学笔记</title>
      <link href="/2024/10/09/%E5%85%89%E5%AD%A6%E7%AC%94%E8%AE%B0/"/>
      <url>/2024/10/09/%E5%85%89%E5%AD%A6%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="光栅"><a href="#光栅" class="headerlink" title="光栅"></a>光栅</h1><h2 id="一维光栅"><a href="#一维光栅" class="headerlink" title="一维光栅"></a>一维光栅</h2><ul><li><strong>定义</strong>：一维光栅具有单一方向的周期性结构，通常表现为一系列平行的线条或条纹。在光栅的平面上，这些条纹仅在一个方向上具有周期性，而在垂直方向上没有周期性。</li><li><strong>结构</strong>：典型的结构包括均匀间隔的平行条纹、狭缝或凹槽。光栅常见的参数包括条纹的间距、深度和宽度。</li><li><strong>衍射特性</strong>：由于仅在一个方向上具有周期性，一维光栅的衍射通常只在一个平面内发生（例如，沿条纹方向的垂直平面），使得入射光被分成不同的衍射角度。</li><li><strong>应用</strong>：一维光栅广泛用于光谱分析、激光器调谐和光通信系统中，用来分光和改变光束的方向。它们可以根据不同波长的光产生特定的衍射角度，从而实现波长分离。</li></ul><h2 id="二维光栅"><a href="#二维光栅" class="headerlink" title="二维光栅"></a>二维光栅</h2><ul><li><strong>定义</strong>：二维光栅具有在两个正交(<strong>垂直</strong>或<strong>互相成 90 度角</strong>)方向上均呈现周期性分布的结构，形成类似网格或点阵的结构。光栅在两个方向上都可以引起光的衍射。</li><li><strong>结构</strong>：二维光栅常表现为方形或六边形的网格、点阵、孔阵等。两个方向上的周期和形状可以相同，也可以不同，以满足特定的光学需求。</li><li><strong>衍射特性</strong>：二维光栅可以在两个方向上同时引起衍射，这意味着入射光会在二维平面上被分散。它们能产生更复杂的衍射图样，使光线分散到不同的方向上。二维光栅可以使光的分离更有效，适合在更复杂的光学系统中使用。</li><li><strong>应用</strong>：二维光栅常用于显微成像、3D 显示、衍射仪、光学数据存储、全息影像等领域，用于生成特定的光学图案、增强显微成像效果或实现空间上的多角度波长分离。</li></ul><h2 id="一维、二维光栅的区别"><a href="#一维、二维光栅的区别" class="headerlink" title="一维、二维光栅的区别"></a>一维、二维光栅的区别</h2><ul><li><strong>周期性方向</strong>：一维光栅只有一个方向上的周期性，而二维光栅则在两个方向上都有周期性。</li><li><strong>衍射图样</strong>：一维光栅的衍射图样通常是一维的（如一条或多条衍射线），而二维光栅的衍射图样更为复杂，通常是二维点阵或网格。</li><li><strong>应用领域</strong>：一维光栅更常用于光谱分离和方向控制，而二维光栅则更常用于生成复杂光学图样、增强空间光学系统中的成像效果等。</li></ul><h2 id="衍射级数"><a href="#衍射级数" class="headerlink" title="衍射级数"></a>衍射级数</h2><p>衍射级数表示光在通过光栅后相对于入射光束的偏转情况。</p><p><strong>0级衍射（零级衍射）</strong>：</p><ul><li>指光在通过光栅后没有发生方向变化，继续沿着入射光方向传播。即对应的是没有衍射的情况，光只通过光栅而不发生任何偏转。</li><li>通常0级衍射光的强度较大，但没有光谱分离能力，所以在光谱分析中不具备作用。</li><li>0级衍射不带频域信息。</li><li>0级是我们不想要的，会把它遮挡掉。正负一级的光会移动物体的频谱，将原先无法获得的高频信息移到通带以低频强度表示出来，通过算法再移频回到正确的位置从而获得高分辨率。</li></ul><p><strong>+1级衍射和-1级衍射</strong>：</p><ul><li>+1级和-1级衍射是光栅在两侧方向上产生的衍射模式。</li><li>+1级衍射表示光在通过光栅后向一个特定方向偏转，偏转角与入射光方向成正角度。</li><li>-1级衍射则表示光偏转到与+1级相反的方向，偏转角为负。</li><li>这些级次的衍射能够分离不同波长的光，即同一入射角度下，不同波长的光会以不同的角度偏转出来。</li></ul><h1 id="样品"><a href="#样品" class="headerlink" title="样品"></a>样品</h1><h2 id="一维样品"><a href="#一维样品" class="headerlink" title="一维样品"></a>一维样品</h2><p>一维样品通常是指在某个方向上具有显著变化的样品，而在其他方向上的变化可以忽略不计。典型的一维样品包括：</p><ul><li><strong>纳米线</strong>：在长度方向上很长，但宽度和厚度很小，因此可以看作是一维结构。</li><li><strong>光纤</strong>：具有很长的长度，横截面较小，通常用作传输光信号。</li><li><strong>电路导线</strong>：长度比宽度和厚度大很多，可以被近似为一维导体。</li><li>在数据科学中，一维数据可以指只有一个变量的时间序列，例如股价时间序列。</li></ul><h2 id="二维样品"><a href="#二维样品" class="headerlink" title="二维样品"></a>二维样品</h2><p>二维样品通常是指在两个方向上具有显著变化的样品，而在第三个方向上的变化较小或可以忽略。例如：</p><ul><li><strong>薄膜</strong>：具有长和宽的平面结构，而厚度非常薄，比如在纳米或微米量级，因此可以看作是二维结构。</li><li><strong>平面图案</strong>：如纸张上的印刷电路板（PCB），其在两个维度（长和宽）上展现显著结构，而厚度可忽略不计。</li><li><strong>图像</strong>：在图像处理中，图像数据通常是二维的，因为它由宽和高组成的像素点表示。</li><li>在数据科学和机器学习中，二维数据通常可以表示为矩阵或二维数组，例如灰度图像、二维坐标数据等。</li></ul><h2 id="两者的区别"><a href="#两者的区别" class="headerlink" title="两者的区别"></a>两者的区别</h2><p>一根纳米线可以被视为一维结构，当许多纳米线交错在一起形成一个结构时，这种集合通常会成为一个二维样品，甚至可能被视为三维样品。</p><h1 id="杨氏双缝干涉"><a href="#杨氏双缝干涉" class="headerlink" title="杨氏双缝干涉"></a>杨氏双缝干涉</h1><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202502261406853.png"></p>]]></content>
      
      
      <categories>
          
          <category> 笔记资料 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>未经训练的神经网络实现快速和通用的结构照明显微镜</title>
      <link href="/2024/10/07/%E6%9C%AA%E7%BB%8F%E8%AE%AD%E7%BB%83%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E5%BF%AB%E9%80%9F%E5%92%8C%E9%80%9A%E7%94%A8%E7%9A%84%E7%BB%93%E6%9E%84%E7%85%A7%E6%98%8E%E6%98%BE%E5%BE%AE%E9%95%9C/"/>
      <url>/2024/10/07/%E6%9C%AA%E7%BB%8F%E8%AE%AD%E7%BB%83%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E5%BF%AB%E9%80%9F%E5%92%8C%E9%80%9A%E7%94%A8%E7%9A%84%E7%BB%93%E6%9E%84%E7%85%A7%E6%98%8E%E6%98%BE%E5%BE%AE%E9%95%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><ol><li>SIM需要多个原始结构照明(SI)帧来重建超分辨率(SR)图像，特别是散斑SIM的重建需要数百个SI帧，耗时较长。</li><li>本文提出了一种具有已知照明模式的未经训练的结构化照明重建神经网络(USRNN)，以将散斑SIM重建所需的原始数据量减少20倍，从而提高其时间分辨率。利用无监督优化策略和CNN的结构先验，在不需要数据集的情况下从网络中获取高频信息;</li><li>SIM使用空间调制的结构照明(SI)模式来照亮样品，样品的高频成分被不均匀的照明模式转移到显微镜的可检测域。为了恢复重叠的频率分量并获得最终的SR结果，需要相机依次捕获样品的一系列原始图像帧。多帧采集降低了成像速度，增加了曝光剂量，导致时间分辨率下降和光漂白。</li><li>未经训练的神经网络在数据集独立性方面具有优越的性能，通常用于正弦SIM重构、帧减少和分辨率增强等领域。</li></ol><h2 id="USRNN网络架构"><a href="#USRNN网络架构" class="headerlink" title="USRNN网络架构"></a>USRNN网络架构</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410081029869.png"></p><ol><li>USRNN的框架。在前向传播过程中，首先将随机噪声图像输入到网络中生成初始输出图像(y)，然后将已知的照明模式(patterni, i &#x3D; 1)输入到网络中。N)，测得的图像帧(xi, i &#x3D; 1…N)，初始y输入目标函数。在接下来的反向传播过程中，根据目标函数的偏导数更新网络的权值，完成一次迭代。这个迭代一直持续，直到得到最优的输出图像。整个优化过程采用梯度下降法求解，求出目标函数的最小值。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410091056950.png"></p><ul><li>其中y为网络的输出，PSF为光学成像系统的点扩展函数，pattern i (i &#x3D; 1…N)是第i个SI模式， x i (i &#x3D; 1…N)为第i个SI图像帧。损失矩阵L2 和Hessian矩阵R Hessian分别对应于物理约束和连续性约束，并通过权重β来平衡它们以保证最优输出。前者计算真实SI帧x i (i &#x3D; 1…N)之间的均方误差。和模拟输入帧，后者可表示为</li></ul><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410091106936.png"></p><ul><li>式中，Dxx、Dyy、Dxy分别是图像y在水平、垂直、对角三个轴上的离散二阶导数。给定N幅SI图像帧和相应的模式，可以确定目标函数的初始值，经过迭代下降过程，得到最优SR结果。利用物理和网络结构先验，采用无监督迭代优化过程，可以用较少的帧数恢复原始高频信息。</li></ul><h2 id="不同模式照射下模拟星状样品的重建"><a href="#不同模式照射下模拟星状样品的重建" class="headerlink" title="不同模式照射下模拟星状样品的重建"></a>不同模式照射下模拟星状样品的重建</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410081101888.png"></p><ol><li>不同模式照射下模拟星状样品的重建。(a)地真星样和模拟宽视场图像。(b) USRNN重建的散斑图案和SIM图像。(c) USRNN重构的正弦模式和SIM图像。(d)傅里叶变换值与半径R的对比，黑色虚线表示瑞利准则。</li></ol><h2 id="散斑照明下100-nm荧光纳米粒子的实验结果"><a href="#散斑照明下100-nm荧光纳米粒子的实验结果" class="headerlink" title="散斑照明下100 nm荧光纳米粒子的实验结果"></a>散斑照明下100 nm荧光纳米粒子的实验结果</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410091311686.png"></p><ul><li>散斑照明下100 nm荧光纳米粒子的实验结果。(a) USRNN重构的宽视场图像和SIM图像。(b)图(a)宽视场和USRNN下感兴趣区域(白虚线框)的放大图像。(c)图(b)中沿白线的强度分布图。(d)宽视场与USRNN的统计分辨率比较，分别通过计算半最大值时的全宽度(FWHM) (n &#x3D; 10)得到各自的值。比例尺，2µm in (a)和400 nm in (b)</li></ul><h2 id="斑点光照下COS-7细胞线粒体的实验结果"><a href="#斑点光照下COS-7细胞线粒体的实验结果" class="headerlink" title="斑点光照下COS-7细胞线粒体的实验结果"></a>斑点光照下COS-7细胞线粒体的实验结果</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202410091313889.png"></p><ul><li>斑点光照下COS-7细胞线粒体的实验结果。(a)未经预处理&#x2F;预处理后USRNN重构的宽视场图像和SIM图像。(b)图(a)中感兴趣区域(白虚线框)在不同模式下的放大图像。(c)面板(b)中沿白线的强度分布图。比例尺，2µm in (a)和300 nm in (b)。</li></ul><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="半最大值平均全宽度"><a href="#半最大值平均全宽度" class="headerlink" title="半最大值平均全宽度"></a>半最大值平均全宽度</h2><ul><li>半最大值平均全宽度（Full Width at Half Maximum，简称 FWHM）是用于描述函数或曲线特征宽度的一种度量。具体来说，它表示曲线从最大值下降到一半最大值处的两个点之间的距离，通常用于表示曲线的“宽度”或“扩展程度”。FWHM 常用于分析各种科学领域中的峰形信号，如光谱学、信号处理、生物学成像等。</li><li>在 FWHM 中，“半最大值”指的是峰值的一半，“全宽度”是指在这个半最大值处的宽度。因此，FWHM 提供了一个标准化的方式来比较不同曲线的宽度，无论它们的形状是否相同。例如，在光谱分析中，可以用 FWHM 来比较不同信号峰的锐度或分辨率。</li></ul><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><p>无。</p><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Untrained neural network enabling fast and universal structured-illumination microscopy</p><p>未经训练的神经网络实现快速和通用的结构照明显微镜</p><p>期刊：OL</p><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ul><li>Ye, Zitong, et al. “Untrained Neural Network Enabling Fast and Universal Structured-Illumination Microscopy.” Optics Letters 49.9 (2024): 2205-08. Print.</li></ul><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Zitong Ye, Xiaoyan Li, Yile Sun</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2024年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年10月7日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：<a href="https://github.com/ZJUOPTKuangLab/USRNN">https://github.com/ZJUOPTKuangLab/USRNN</a> (作者)</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> SIM </tag>
            
            <tag> U-Net </tag>
            
            <tag> 无监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用于远场超分辨率成像的二维斐波那契光栅</title>
      <link href="/2024/09/24/%E7%94%A8%E4%BA%8E%E8%BF%9C%E5%9C%BA%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%88%90%E5%83%8F%E7%9A%84%E4%BA%8C%E7%BB%B4%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E5%85%89%E6%A0%85/"/>
      <url>/2024/09/24/%E7%94%A8%E4%BA%8E%E8%BF%9C%E5%9C%BA%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%88%90%E5%83%8F%E7%9A%84%E4%BA%8C%E7%BB%B4%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E5%85%89%E6%A0%85/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>通过在自由空间中一次性检测光场穿过二维斐波那契光栅前物体的远场强度分布，可以检索到空间分辨率超过λ&#x2F;7的物体图像。</li><li>空间分辨率表征成像系统的最小可分辨能力，一般认为二维(2D)系统为0.61λ&#x2F;NA，一维(1D)系统为0.5λ&#x2F;NA。</li><li>亚波长结构用于将前方物体的倏逝波转换为远场探测所需的传播波。此外，这种亚波长结构还可以产生倏逝波，照亮分辨率以下尺度的物体，从而进一步从物体的散射光或衍射光中获取超分辨率图像。</li><li>周期亚波长光栅的一个关键缺陷，它只能产生固定的相移，使频域不连续。为了克服这一缺点，可以使用一种准周期亚波长光栅在频域内产生准连续位移。由于空间频域是连续的，在一定空间频率范围内丢失的亚波长信息可以被携带到远场区域。因此，可以利用远场检测信息重建超分辨率图像。</li><li>本文提出了一种用于自由空间远场超分辨率成像的二维斐波那契光栅辅助显微镜。采用严格耦合波分析(RCWA)方法对二维斐波那契光栅进行设计和优化，将二维物体的倏逝波转化为传播波。通过一次测量远场光强分布，我们可以通过傅里叶光学方法和时域有限差分(FDTD)数值模拟方法重建空间分辨率超过λ&#x2F;7的样品物体图像。与用于扩展一维空间频率轴连续的一维斐波那契光栅辅助显微镜相比，我们的二维斐波那契光栅可以使二维空间频率圆平面上的频域连续。因此，所有低于该分辨率的具有不同亚波长细节的二维图像都可以被分辨出来。</li></ol><h2 id="基础理论"><a href="#基础理论" class="headerlink" title="基础理论"></a>基础理论</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409241005367.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409241915169.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409241937411.png"></p><h2 id="二维斐波那契光栅理论设计"><a href="#二维斐波那契光栅理论设计" class="headerlink" title="二维斐波那契光栅理论设计"></a>二维斐波那契光栅理论设计</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409241940636.png"></p><ol><li>每个光栅单元都是亚波长块，因此光栅将显示出与偏振无关的正常入射响应20。在每一行和每一列中，沿着x´和y´方向，两个基本单位以斐波那契级数的形式排列为“…abaababababababab…”。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409242013188.png"></p><h2 id="相干光照条件下仿真结果"><a href="#相干光照条件下仿真结果" class="headerlink" title="相干光照条件下仿真结果"></a>相干光照条件下仿真结果</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409242013803.png"></p><ol><li>相干光照下二维斐波那契光栅超分辨率成像的仿真结果。(A、D)分别间隔90nm和70nm的3×3-point-array物体灰度分布。(B)和(E)分别为A和D波长为632nm的相干光照射下的模拟原始图像。(C、F)分别计算A、D重建过程后的分辨图像光强分布。</li></ol><h2 id="非相干光照条件下仿真结果"><a href="#非相干光照条件下仿真结果" class="headerlink" title="非相干光照条件下仿真结果"></a>非相干光照条件下仿真结果</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409242020516.png"></p><ol><li>非相干光照条件下二维斐波那契光栅超分辨率成像的仿真结果。(A、C)分别间隔90nm、170nm的3×3-point-array物体模拟关节原始图像。(B、D)分别计算A、D重建过程后的分辨图像光强分布。</li></ol><h2 id="反卷积过程"><a href="#反卷积过程" class="headerlink" title="反卷积过程"></a>反卷积过程</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409242037027.png"></p><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><p>无。</p><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Two-dimensional Fibonacci grating for far-field super-resolution imaging</p><p>用于远场超分辨率成像的二维斐波那契光栅</p><p>期刊：Sci Rep</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：K. Wu , G. P. Wang</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2016年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年9月24日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：无</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 光栅 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>远场超透镜用于亚衍射光学成像的实验研究</title>
      <link href="/2024/09/23/%E8%BF%9C%E5%9C%BA%E8%B6%85%E9%80%8F%E9%95%9C%E7%94%A8%E4%BA%8E%E4%BA%9A%E8%A1%8D%E5%B0%84%E5%85%89%E5%AD%A6%E6%88%90%E5%83%8F%E7%9A%84%E5%AE%9E%E9%AA%8C%E7%A0%94%E7%A9%B6/"/>
      <url>/2024/09/23/%E8%BF%9C%E5%9C%BA%E8%B6%85%E9%80%8F%E9%95%9C%E7%94%A8%E4%BA%8E%E4%BA%9A%E8%A1%8D%E5%B0%84%E5%85%89%E5%AD%A6%E6%88%90%E5%83%8F%E7%9A%84%E5%AE%9E%E9%AA%8C%E7%A0%94%E7%A9%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>远场超级透镜(FSL)由一个传统的超级透镜和一个纳米级耦合器组成。从物体发出的倏逝场经过FSL增强后转化为传播场。通过测量远场的传播场，可以实现亚波长分辨率的物像重建。</li><li>设计并制造了一个银结构的一维FSL，实验结果表明，采用现有的FSL设计，可以实现50nm以上的特征分辨率。</li><li>远场超透镜(FSL)利用平面几何特性和纳米光栅的倏逝-传播波转换，不仅可以在远场获得亚衍射限制图像，而且具有非常大的视场尺寸</li></ol><h2 id="利用近场莫尔效应提高远场分辨率"><a href="#利用近场莫尔效应提高远场分辨率" class="headerlink" title="利用近场莫尔效应提高远场分辨率"></a>利用近场莫尔效应提高远场分辨率</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409231400461.png"></p><ol><li>(a)物体的近场光学图像。(b)亚波长耦合光栅。(c)由(a)和(b)叠加而成的远场可观测的莫尔条纹。从莫尔条纹可以得到亚波长分辨率的原始物象。</li><li>FSL由平板超透镜和纳米级光栅耦合器组成。如果将耦合器带到超透镜附近，从而与近场图像重叠，则可以构造一个莫尔条纹图(见图1中的示意图)。由于叠加的近场图像和光栅之间的倏逝波“混频”，可以通过近场莫尔条纹效应来理解该条纹图。通过将不可分辨的高空间频率转换为低空间频率的莫尔条纹，可以在远场检测到目标的高分辨率信息。利用远场测量数据可以重建亚衍射极限图像。</li></ol><h2 id="简化一维远场超透镜-FSL-的概念"><a href="#简化一维远场超透镜-FSL-的概念" class="headerlink" title="简化一维远场超透镜(FSL)的概念"></a>简化一维远场超透镜(FSL)的概念</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409231413468.png"></p><ol><li>传统光学透镜在傅里叶空间中的可达区域由半径为kobs的中心圆定义(kobs仅限于传播波)。FSL可以利用其衍射阶将原消去波转换为传播波。如果物体也是一维的，并且相对于FSL的光栅存在角度失调α，则经过不同衍射顺序的物体信息将在空间上分离。通过这种方式，可以检测和恢复的最大空间频率为Kobs+KΛ。</li></ol><h2 id="结构银FSL的光学振幅传递函数-ATF"><a href="#结构银FSL的光学振幅传递函数-ATF" class="headerlink" title="结构银FSL的光学振幅传递函数(ATF)"></a>结构银FSL的光学振幅传递函数(ATF)</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409231420615.png"></p><ol><li>在TM模式下，结构银FSL(仅显示传播波)的光学振幅传递函数(ATF)。FSL的几何形状如图所示。周围介质的折射率设为1.52，真空条件下工作光波长为377nm。</li><li>以一个银结构FSL为例，采用严格耦合波分析(RCWA)方法计算OTF，如图3所示。在大的空间频率(波矢量)下，OTF表现出显著的增强，而在小的空间频率下，OTF表现出抑制，这将大大提高条纹和最终恢复的高分辨率图像的对比度。</li></ol><h2 id="银结构FSL的扫描电镜图像"><a href="#银结构FSL的扫描电镜图像" class="headerlink" title="银结构FSL的扫描电镜图像"></a>银结构FSL的扫描电镜图像</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409231426814.png"></p><h2 id="单线物体定向FSL图像"><a href="#单线物体定向FSL图像" class="headerlink" title="单线物体定向FSL图像"></a>单线物体定向FSL图像</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409231440507.png"></p><ol><li>(a)线宽50nm的单线物体定向FSL图像;(b)线宽50nm、边距70nm的双线物体;(c)线宽50nm、边距70nm的三线物体。(d)、(e)和(f)分别对(a)、(b)和(c)进行二维傅里叶变换。白色圆圈内的区域表示光学显微镜的远场可探测信息(NA≤1.4)。(g)、(h)、(i)分别由(d)、(e)、(f)重建图像。(j)、(k)、(l)分别为(g)、(h)、(i)中虚线白色矩形区域的放大图。</li><li>p偏光直接光学显微镜成像如图5(a)、5(b)、5(c)所示，分别为单缝、双缝和三缝。我们对这些图像进行了二维傅里叶变换，其光谱如图5(d)、5(e)和5(f)所示。</li><li>由于线对对象被故意定向与相对于FSL的光栅方向小角度。通过FSL产生的不同阶的传输在傅里叶谱中被分离。从图5(d)、5(e)、5(f)可以清楚地看到FSL衍射顺序为0、+1、-1，这与我们在图2中的预期一致。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409231448323.png"></p><ol><li>线宽50nm， (a) 60、 (b) 80nm、(c) 100nm边缘距离的三线物体定向FSL图像。(d)、(e)、(f)分别为物体(a)、(b)、(c)重建图像的放大视图。</li><li>为了进一步证明FSL的成像能力，我们还测试了一个线宽为50nm、边到边距离(60nm、80nm和100nm)不同的三线物体。定向光学图像和重构图像如图6所示。显然，物体的三条线都被精确地分辨出来了。物体之间狭缝距离的差异，小到20纳米，已经被清楚地区分出来。</li></ol><h2 id="分离和重新定位频率分量获得高分辨率图像"><a href="#分离和重新定位频率分量获得高分辨率图像" class="headerlink" title="分离和重新定位频率分量获得高分辨率图像"></a>分离和重新定位频率分量获得高分辨率图像</h2><ol><li>为了获得高分辨率图像，必须将三个重叠的频率分量分离并重新定位到其原始位置。这可以通过在物体和光栅之间进行多次相对相移测量，然后进行数据处理程序来完成。由于物体和光栅处于彼此的近场，因此需要非常复杂的仪器在物体上以纳米精度平移光栅。此外，在平移过程中，目标必须保持不变，这使得该方法不适合实时高速成像。为了在没有平移过程的情况下解决波矢量混合问题，具有特定波矢量相关光学传递函数(OTF)的FSL提供了一种全新的方法。根据期望的OTF，只有一个衍射级可以通过FSL，这使得可探测的传播波与它的倏逝源之间具有“一对一”的关系。</li><li><strong>传统方法</strong>：通常是通过物体和光栅之间的多个相对位置变化来实现高分辨率。光栅就像是一个精细的网格，当它与物体产生相对移动时，会测量多个角度下的光线。这些数据随后经过复杂的处理，能够提升图像的分辨率。这种方法的问题在于，光栅的移动需要非常精确，达到纳米级别。而且物体必须在这个过程中保持静止，这让它不适合快速成像，因为移动和测量的时间都很长。</li><li><strong>新的FSL方法</strong>：为了解决这个问题，有一种新的方法叫<strong>FSL</strong>，它利用了一种特定的“光学传递函数”（OTF），使得只需要一个衍射波级通过即可。这样可以避免复杂的平移过程，并直接通过波矢量之间的关系来获得高分辨率。换句话说，FSL的方法避免了光栅的移动，简化了设备的要求，使得更高效地获得清晰的图像成为可能。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><ol><li>由亚波长光栅衍射耦合的波是倏逝的。物体与耦合光栅之间的任何距离都可能导致显著的场振幅损失。此外，对于自然物体而言，较高空间频率的幅值通常较小。</li></ol><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Experimental studies of far-field superlens for sub-diffractional optical imaging</p><p>远场超透镜用于亚衍射光学成像的实验研究</p><p>期刊：Optics Express(OE)</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Zhaowei Liu，Stéphane Durant</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2007年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年9月23日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：无</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 超透镜 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用于远场超分辨率成像的一维斐波那契光栅</title>
      <link href="/2024/09/22/%E7%94%A8%E4%BA%8E%E8%BF%9C%E5%9C%BA%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%88%90%E5%83%8F%E7%9A%84%E4%B8%80%E7%BB%B4%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E5%85%89%E6%A0%85/"/>
      <url>/2024/09/22/%E7%94%A8%E4%BA%8E%E8%BF%9C%E5%9C%BA%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%88%90%E5%83%8F%E7%9A%84%E4%B8%80%E7%BB%B4%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E5%85%89%E6%A0%85/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>利用一维斐波那契光栅将倏逝波转换为传播波，实现远场超分辨成像。</li><li>通过在自由空间中检测光穿过斐波那契光栅前物体的远场强度分布，我们可以观察到接近λ∕9的空间分辨率的物体。</li><li>利用 Fibonacci 光栅模拟了光栅对频谱的调制过程，然后通过反卷积重构物体信息，该研究成果理论论证了光栅能对频谱进行调制，然后通过反卷积重构物体信息研究方案的可行性</li></ol><h2 id="远场超分辨成像理论"><a href="#远场超分辨成像理论" class="headerlink" title="远场超分辨成像理论"></a>远场超分辨成像理论</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409221455509.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409221602285.png"></p><h2 id="斐波那契光栅成像系统方案"><a href="#斐波那契光栅成像系统方案" class="headerlink" title="斐波那契光栅成像系统方案"></a>斐波那契光栅成像系统方案</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409221606330.png"></p><ol><li><p>(a)斐波那契光栅成像系统方案。</p><p>(b)计算斐波那契光栅0(红色虚线)−1(蓝色实线)和1(绿色虚线)阶的衍射效率。</p><p>(c)和(d)分别计算光通过放置在斐波那契光栅前和自由空间的两线物体的灰度强度分布。</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409221630769.png"></p></li></ol><h2 id="其余图示"><a href="#其余图示" class="headerlink" title="其余图示"></a>其余图示</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409231009924.png"></p><ol><li>图2(a)显示了三个物体的强度分布，其对应的截面如图2(d)所示(黑色虚线)。在检索过程中，我们假设光的场强是非负的，并且被观察的物体被放置在光栅的中心，以避免场分布的不对称，这可能导致反卷积操作的误差。</li><li>图2(b)为检索到的目标图像的灰度分布。我们看到，物体的两条线的间距分别为70、105和145 nm。其对应截面如图2(d)(红色实线)所示。</li><li>图2(c)分别使用斐波那契光栅系统和周期光栅系统重建物体的灰度分布。</li><li>图2(d)物体强度分布的对应截面(黑色虚线)和它们的检索图像分别为斐波那契光栅(红色实线)和周期光栅(蓝色圆点线)。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409231012104.png"></p><ol><li>图3(a)采样间隔为5 nm(红色实线)、300 nm(绿色实圆)时检测平面光场强度截面及其拟合曲线(蓝色虚线)。</li><li>图3(b)以λ&#x2F;9空间为采样间隔，分别为5 nm(红色实线)和300 nm(蓝色圆点线)的双线物体重构图像的横截面。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="周期性光栅的缺陷"><a href="#周期性光栅的缺陷" class="headerlink" title="周期性光栅的缺陷"></a>周期性光栅的缺陷</h2><ol><li>超出衍射极限的高空间分辨率光学成像需要由倏逝波携带目标信息，倏逝波随距离目标呈指数衰减，仅在近场可探测到。</li><li>周期光栅辅助远场超分辨成像系统存在一个关键缺陷，即物体某些空间频率的精细特征无法被提取。这是由于周期光栅在频域只能产生固定的、不连续的频移，导致无法获取对象的部分特征。此外，多角度照明或多方向采集也影响了成像速度。与基于周期光栅的显微镜相比，我们的准周期结构可以产生频谱的准连续位移，从而避免了丢失部分亚波长信息的情况。最后，讨论了采样误差对检索图像分辨率的影响。</li></ol><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：One-dimensional Fibonacci grating for far-field super-resolution imaging</p><p>用于远场超分辨率成像的一维斐波那契光栅</p><p>期刊：Opt Lett(OL)</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：K. Wu , G. P. Wang</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2013年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年9月21日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：无</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 光栅 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>锯齿相位衍射光栅光子射流阵列的首次实验观察</title>
      <link href="/2024/09/20/%E9%94%AF%E9%BD%BF%E7%9B%B8%E4%BD%8D%E8%A1%8D%E5%B0%84%E5%85%89%E6%A0%85%E5%85%89%E5%AD%90%E5%B0%84%E6%B5%81%E9%98%B5%E5%88%97%E7%9A%84%E9%A6%96%E6%AC%A1%E5%AE%9E%E9%AA%8C%E8%A7%82%E5%AF%9F/"/>
      <url>/2024/09/20/%E9%94%AF%E9%BD%BF%E7%9B%B8%E4%BD%8D%E8%A1%8D%E5%B0%84%E5%85%89%E6%A0%85%E5%85%89%E5%AD%90%E5%B0%84%E6%B5%81%E9%98%B5%E5%88%97%E7%9A%84%E9%A6%96%E6%AC%A1%E5%AE%9E%E9%AA%8C%E8%A7%82%E5%AF%9F/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>利用球形微粒紧密堆积产生的<span style="color: red;">光子纳米射流(PNJ)</span>阵列的超聚焦效应。为了控制光子射流阵列的关键参数，可以采用相位光栅。</li><li>本文首次在405,532和761 nm三种不同波长下对锯齿相衍射光栅阴影表面附近PNJ系综形成的空间和振幅特征进行了实验直接成像，结果表明，由这种相位衍射光栅形成的PNJs阵列具有可控制的特性。</li></ol><h2 id="锯齿状相位衍射光栅"><a href="#锯齿状相位衍射光栅" class="headerlink" title="锯齿状相位衍射光栅"></a>锯齿状相位衍射光栅</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409201926982.png"></p><ol><li>图1(a)描述了用于光子射流的相位衍射光栅的尺寸和示意图。</li><li>长度(L)、焦距(F)、宽度(R)和归一化强度是光子射流的关键参数。波长分别为405 nm、532 nm和671nm的激光束从相位衍射光栅底部入射。</li><li><span style="color: red;">锯齿形光栅用于增强和收集特定角度的光辐射。</span>对于凹槽距离d &lt; 1 μm的介质光栅，光子射流从光栅的阴影侧突出。当光栅尺寸与入射波长相当时，由光栅形成的光子射流具有较小的尺寸。然而，在标准光学显微镜下很难观察到纳米光栅[33]。随着凹槽距离的增加，光子射流开始出现并远离光栅。考虑到相衍射光栅形成的光子射流，需要注意的是，在光学显微镜下，沟槽距离d &#x3D; 5 μm更适合实际观察。</li></ol><h2 id="扫描光学显微镜系统的实验配置"><a href="#扫描光学显微镜系统的实验配置" class="headerlink" title="扫描光学显微镜系统的实验配置"></a>扫描光学显微镜系统的实验配置</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409201933667.png"></p><h2 id="功率流分布"><a href="#功率流分布" class="headerlink" title="功率流分布"></a>功率流分布</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409201939332.png"></p><ul><li>具有锯齿形凹槽的相位衍射光栅的功率流分布的原始实验图像(右)和FDTD模拟(左)。</li></ul><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：First experimental observation of array of photonic jets from saw-tooth phase diffraction grating</p><p>锯齿相位衍射光栅光子射流阵列的首次实验观察</p><p>期刊：EPL (Europhysics Letters)</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Cheng Yang Liu</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2018年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年9月20日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：无</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 光栅 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>光学远场超透镜超越衍射极限成像的传输特性理论</title>
      <link href="/2024/09/19/%E5%85%89%E5%AD%A6%E8%BF%9C%E5%9C%BA%E8%B6%85%E9%80%8F%E9%95%9C%E8%B6%85%E8%B6%8A%E8%A1%8D%E5%B0%84%E6%9E%81%E9%99%90%E6%88%90%E5%83%8F%E7%9A%84%E4%BC%A0%E8%BE%93%E7%89%B9%E6%80%A7%E7%90%86%E8%AE%BA/"/>
      <url>/2024/09/19/%E5%85%89%E5%AD%A6%E8%BF%9C%E5%9C%BA%E8%B6%85%E9%80%8F%E9%95%9C%E8%B6%85%E8%B6%8A%E8%A1%8D%E5%B0%84%E6%9E%81%E9%99%90%E6%88%90%E5%83%8F%E7%9A%84%E4%BC%A0%E8%BE%93%E7%89%B9%E6%80%A7%E7%90%86%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>超衍射极限成像的传统光学超透镜仅在超透镜的近场区产生图像。相比之下，光学远场超透镜(FSL)器件具有显著的传输特性，导致远场和近场角光谱之间的一对一关系。</li><li>这种特殊的FSL是由一个合理设计的周期性波纹金属板基超透镜组成的。通过在物体的近场使用FSL，并通过测量远场散射的角光谱(例如，使用衍射显微镜)，可以以低于衍射极限的超高分辨率重建物体的图像。</li></ol><h2 id="远场超透镜成像理论"><a href="#远场超透镜成像理论" class="headerlink" title="远场超透镜成像理论"></a>远场超透镜成像理论</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409201439095.png"></p><ul><li>(a)传统超透镜与(b)超透镜的透射特性。假设波是由z &#x3D; z0的物体辐射的。入射倏逝波通过传统的超透镜(a)传输时得到增强，并且在近场区域仍然迅速消失，这限制了超透镜对近场的成像能力。相比之下，FSL (b)对传播波中的原始倏逝波进行了增强和转换。在后一种情况下，入射传播波在远场以较低的振幅传播，对远场角谱的主要贡献是入射倏逝波。利用这一特性，可以从远场角谱测量中得到近场角谱。</li><li>如图1(a)所示，倏逝波在传输过程中得到增强，但仍局限于近场，这意味着它们的振幅在超透镜的法线方向上呈指数衰减，这使得远场的亚波长成像不可能。相比之下，FSL不仅大大增强了倏逝波，而且在传输中将其转换为传播波，如图1(b)所示。FSL是基于一个合理设计的周期性波纹超透镜。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409201439205.png"></p><ul><li>选择一个亚波长周期，使得入射传播波仅通过0阶传输到远场，而具有正(负)横波数的入射倏逝波仅通过−1阶(阶+1)传输到远场。该结构的设计使得入射倏逝波通过利用超透镜效应得到增强，并通过高效的- 1阶衍射过程转化为传播波。相比之下，入射传播波以较小的振幅在0阶上传播。</li><li>因此，虽然物体辐射的近场角光谱包含传播波和倏逝波，但通过超透镜光栅传输到远场的波的主要贡献来自入射倏逝波。透射和入射横波数之间也存在一对一的关系。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409201439065.png"></p><h2 id="光学远场超透镜装置的设计"><a href="#光学远场超透镜装置的设计" class="headerlink" title="光学远场超透镜装置的设计"></a>光学远场超透镜装置的设计</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409201439700.png"></p><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="传播波与倏逝波"><a href="#传播波与倏逝波" class="headerlink" title="传播波与倏逝波"></a>传播波与倏逝波</h2><ul><li>衍射极限以下分辨率成像的关键在于对物体近场辐射的传播波和倏逝波的采集和检测自由空间中折射率为n的单色平流层波U。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409201439939.png"></p><h2 id="表面等离子体激元-SPP"><a href="#表面等离子体激元-SPP" class="headerlink" title="表面等离子体激元(SPP)"></a>表面等离子体激元(SPP)</h2><ol><li>表面等离子体激元(SPP)在贵金属光学超透镜两个界面的激发对倏逝波的增强起着关键作用。</li><li>表面等离子体激元（SPP）是一种由光与金属表面自由电子相互作用产生的电磁波，沿着金属-介质界面传播。SPP 在贵金属光学超透镜的两个界面处的激发，能够极大地增强倏逝波（evanescent wave）的强度，从而提升光的空间分辨率。这种增强效应是因为 SPP 可以将光场限制在亚波长尺度，并使光能够穿透传统衍射极限。</li><li>在超透镜中，倏逝波携带着物体细节信息，但通常会迅速衰减，无法直接检测。通过在贵金属界面激发 SPP，可以有效地增强这些倏逝波的能量，使其传播距离更长，并被超透镜捕捉和利用。这一现象对于实现光学超分辨率成像和纳米尺度光学应用具有重要意义。</li><li>贵金属如金和银，因其优越的导电性和对电磁波的响应特性，常被用作激发 SPP 的材料。它们能够在可见光或近红外范围内高效地支持 SPP 的传播和增强。</li></ol><h2 id="横向分辨率与纵向分辨率"><a href="#横向分辨率与纵向分辨率" class="headerlink" title="横向分辨率与纵向分辨率"></a>横向分辨率与纵向分辨率</h2><ol><li>横向分辨率是指成像系统在水平方向或垂直方向上能分辨的最小距离，也就是两个物体或物体细节在图像中能够被区分开的最小间距。它是用来衡量图像清晰度和细节精度的重要指标，通常以距离单位（如微米）表示。</li><li>纵向分辨率（也称为轴向分辨率）是光学或成像系统中沿光轴方向（深度方向）能够分辨的最小距离。与横向分辨率不同，纵向分辨率描述的是系统在深度方向上区分两个相邻物体层的能力。在实际应用中，纵向分辨率对于三维成像（如共聚焦显微镜和光学相干断层扫描等）至关重要，因为它决定了系统在深度方向上能够清晰区分物体不同层次的能力。</li></ol><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><ol><li>传播波与倏逝波部分。</li><li>远场超透镜成像理论。</li></ol><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Theory of the transmission properties of an optical far-field superlens for imaging beyond the diffraction limit</p><p>光学远场超透镜超越衍射极限成像的传输特性理论</p><p>期刊：Optica Publishing Group(OPG)</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Stéphane Durant, Zhaowei Liu</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2006年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年9月19日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：无</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 超透镜 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>纳米颗粒超分辨率成像的深度学习增强显微镜</title>
      <link href="/2024/09/08/%E7%BA%B3%E7%B1%B3%E9%A2%97%E7%B2%92%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%88%90%E5%83%8F%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A2%9E%E5%BC%BA%E6%98%BE%E5%BE%AE%E9%95%9C/"/>
      <url>/2024/09/08/%E7%BA%B3%E7%B1%B3%E9%A2%97%E7%B2%92%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%88%90%E5%83%8F%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A2%9E%E5%BC%BA%E6%98%BE%E5%BE%AE%E9%95%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>应用深度学习框架来提高具有规则形状但排列不同的金属纳米结构的光学成像的空间分辨率。</li><li>以随机分布的金纳米粒子的光学图像作为输入，相应的扫描电镜图像作为真值，构建卷积神经网络(CNN)并对其进行预训练。</li><li>提出了一种深度非局部U-net，它将非局部去噪模块融入到众所周知的U-net体系结构中。这种集成使我们的网络能够隐式地将跨多个尺度的自相似先验与U-net结构结合起来，并通过非局部去噪模块显式地揭示跨单个尺度的非局部自相似。我们提出的网络对于将衍射受限的光学图像转换为类似SEM的超分辨率图像的特定任务特别有效。</li></ol><h2 id="图像采集原理图"><a href="#图像采集原理图" class="headerlink" title="图像采集原理图"></a>图像采集原理图</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409201426792.png"></p><ol><li>以金纳米粒子(AuNPs)为例，展示了用于光学图像超分辨率重建的深度学习反卷积模型。通过安装在暗视场光学显微镜上的科学相机收集AuNPs的光学显微照片，如图1(a)所示。从AuNP的衍射图(图1(b))可以看出，如果粒子间距离低于瑞利准则(∆&#x3D; 0.61λ&#x2F;NA)，则无法区分紧密分布的AuNP二聚体。然而，这些衍射模式的强度分布仍然存在许多差异，为纳米颗粒排列的反向恢复提供了很多信息。</li></ol><h2 id="深层卷积神经网络的架构设计"><a href="#深层卷积神经网络的架构设计" class="headerlink" title="深层卷积神经网络的架构设计"></a>深层卷积神经网络的架构设计</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409201427495.png"></p><ol><li>设计了一个CNN来建立光学衍射图像与粒子尺寸之间的非线性投影。</li><li>所提出的网络架构结合了由非局部去噪模块构成的显式非局部先验和由U-net结构构成的隐式非局部先验，如图2(a)所示，U-net建立在四尺度编解码U-net上，每个尺度都有一个跳过连接。在第三个尺度中，我们为编码器和解码器插入非局部去噪模块。</li></ol><h2 id="数据集的分配"><a href="#数据集的分配" class="headerlink" title="数据集的分配"></a>数据集的分配</h2><ol><li>将光学和扫描电镜图像裁剪成480对400 × 400像素的不重叠子图，其中360对用于训练，其余用于测试。</li></ol><h2 id="纳米粒子多聚体和纳米线的超分辨率"><a href="#纳米粒子多聚体和纳米线的超分辨率" class="headerlink" title="纳米粒子多聚体和纳米线的超分辨率"></a>纳米粒子多聚体和纳米线的超分辨率</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409201427277.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409201427747.png" alt="image-20240908213629560"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409201428630.png"></p><h2 id="网络训练时的损失曲线"><a href="#网络训练时的损失曲线" class="headerlink" title="网络训练时的损失曲线"></a>网络训练时的损失曲线</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409201428387.png"></p><ol><li>所提出的CNN算法的通用性需要进一步验证用于其他类型的纳米结构。为此，我们还利用深度学习模型从模糊的光学图像中重建随机交联银纳米线的形态(图7)。银纳米线的直径为~ 100 nm，长度为~ 10 μ m。</li><li>如图7(a)所示，银纳米线随机分布在衬底表面。纳米线网表现出许多相互作用，这些相互作用不可避免地影响了所得到的光学衍射图。</li><li>很明显，复杂的纳米线网的大部分部分都被准确地恢复了。然而，在纳米线网相交区域附近，微小的差异是明显的。此外，在某些密集的区域内，较细的纳米线容易产生误判，导致输出纳米线产生微小的波纹。为了更清楚地说明这些挑战，我们提供了两个特定局部区域的放大图片，如图7(d)和7(e)所示。</li><li>此外，我们给出了在网络训练过程中观察到的损失曲线，表明损失随着epoch的增加逐渐减小并收敛(图7(f))。</li><li>需要注意的是，在这项工作中设计CNN算法的动机是为了提高光学显微镜对常见的规则形状的人工纳米结构(如圆盘、球体和线等)的分辨率，因为这些几何形状经常用于在纳米材料、纳米光子学、纳米电子学甚至纳米力学领域构建更复杂的纳米图案。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><ol><li>过去几十年中，光学超分辨率显微镜的研究主要集中于突破衍射极限，尤其是远场荧光显微镜（如PALM、STORM、STED、SIM）在生物成像中取得了几纳米的分辨率。然而，这些方法依赖荧光标记，限制了其在无机样品中的应用。当前的挑战是为人工纳米结构等无机样品开发一种简单、非侵入式的超分辨率成像技术。</li><li>传统远场光学成像受到阿贝衍射极限的限制，这主要是由于样品表面的指数衰减倏逝波所携带的图像信息的损失。一些光学“硬件”创新，如能够收集全波信息的超级透镜，有望打破分辨率的限制。然而，目前这些超材料光学元件距离实际应用还有很长的路要走，许多技术难题尚未解决，如准近场工作距离等。</li><li>计算方法在光学显微照片的超分辨率重建中也发挥了重要作用。例如，荧光图像的光学模糊可以通过样品散射场与非相干成像系统的点扩展函数的卷积来表述。应用适当的反卷积算法，可以从物体的模糊图像中反向恢复物体的精细结构。对于相干成像系统，基于傅里叶光学和信息理论，已经提出了许多反卷积算法，如带宽外推和压缩感知，用于亚波长结构的超分辨率认知。</li><li>用于逆图像恢复的网络必须使用广泛的已知数据进行预训练。因此，数据驱动模型缺乏通用性。此外，深度学习网络的“黑箱”性质引发了人们对网络可以得出的基础物理完整性的担忧。因此，更多的研究旨在将物理约束纳入深度神经架构，以减少对大量训练数据集的依赖。</li><li>除了网络训练的数据采集问题外，设计良好的CNN对于将计算图像解卷积为高分辨率图像也起着至关重要的作用。直接应用常见的完整CNN架构可能会遇到通用性问题，即训练好的CNN模型严重依赖数据并且缺乏模型可解释性。解决这个问题的一种方法是在CNN设计过程中考虑专家领域知识。这种基于知识的CNN在使用稀疏训练数据的同时，期望具有更高的通用性，以已知的物理先验作为约束训练的手段。在各种领域知识中，非局部自相似是大多数图像恢复任务的通用先验，因此得到了广泛的研究。最近，有研究指出，具有跳过连接的U-net架构自然地在多个尺度上施加自相似性。另一方面，为了利用这种跨空间域的非局部自相似的固有特性，提出了非局部去噪模块，该模块可以融合到现有的CNN中进行端到端训练。</li></ol><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Deep-learning-augmented microscopy for super-resolution imaging of nanoparticles</p><p>纳米颗粒超分辨率成像的深度学习增强显微镜</p><p>期刊：Optics Express</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：XIN HU，XIXI JIA</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2023年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年9月8日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：无</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> U-Net </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多模态结构照明显微镜的自监督去噪使长期超分辨率活细胞成像成为可能</title>
      <link href="/2024/09/05/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%BB%93%E6%9E%84%E7%85%A7%E6%98%8E%E6%98%BE%E5%BE%AE%E9%95%9C%E7%9A%84%E8%87%AA%E7%9B%91%E7%9D%A3%E5%8E%BB%E5%99%AA%E4%BD%BF%E9%95%BF%E6%9C%9F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%B4%BB%E7%BB%86%E8%83%9E%E6%88%90%E5%83%8F%E6%88%90%E4%B8%BA%E5%8F%AF%E8%83%BD/"/>
      <url>/2024/09/05/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%BB%93%E6%9E%84%E7%85%A7%E6%98%8E%E6%98%BE%E5%BE%AE%E9%95%9C%E7%9A%84%E8%87%AA%E7%9B%91%E7%9D%A3%E5%8E%BB%E5%99%AA%E4%BD%BF%E9%95%BF%E6%9C%9F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%B4%BB%E7%BB%86%E8%83%9E%E6%88%90%E5%83%8F%E6%88%90%E4%B8%BA%E5%8F%AF%E8%83%BD/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文解决的问题"><a href="#论文解决的问题" class="headerlink" title="论文解决的问题"></a>论文解决的问题</h2><ol><li>噪声会明显降低结构照明显微镜（SIM）图像的质量，尤其是在光线很弱的时候。虽然一些使用监督学习（也就是通过已经标记的数据来进行训练）的去噪方法在去除噪声引起的图像伪影方面取得了很大进展，但这些方法需要大量高质量的训练数据。这就成了一个主要的限制，因为获取这些数据并不容易。</li></ol><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li><span style="color: red;">为SIM开发了一个基于像素重新对齐的自监督去噪框架(PRS-SIM)，该框架仅使用噪声数据训练SIM图像去噪器，重建无伪影的SR-SIM图像，荧光比常规SIM成像条件少20倍。</span></li></ol><h2 id="PRS-SIM原理图"><a href="#PRS-SIM原理图" class="headerlink" title="PRS-SIM原理图"></a>PRS-SIM原理图</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409051553042.png"></p><ol><li>PRS-SIM的自监督训练策略。a）通过对噪声低分辨率(LR)原始SIM图像组y进行像素对齐操作，生成4个匹配的图像组yA、yB、yC和yD，然后使用常规SIM算法重构4个超分辨率(SR)图像，并将其随机排列作为神经网络训练的输入和目标。</li><li>b）PRS-SIM推理管道。首先利用传统的SIM算法将带噪声的原始SIM图像组重构为带噪声的SR图像。然后将该带噪SR图像输入到预训练的PRS-SIM模型中，生成相应的无噪SR SIM图像。</li></ol><h2 id="PRS-SIM与其它现有方法的比较"><a href="#PRS-SIM与其它现有方法的比较" class="headerlink" title="PRS-SIM与其它现有方法的比较"></a>PRS-SIM与其它现有方法的比较</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409051553074.png"></p><ol><li>用Conv. SIM、Sparse-SIM和PRS-SIM对网格蛋白包被的凹坑(CCPs)、微管(MTs)和内质网(ER)进行重建和处理。提供了相应的WF和GT-SIM图像供参考。</li><li>b）MT样品的WF、PRS-SIM和GT-SIM图像的傅里叶光谱。虚线圈为截止频率。(对应94 nm的空间频率)</li><li>c）PRS-SIM、Conv. SIM和Sparse-SIM的定量比较。参考GT-SIM图像计算PSNR和SSIM值(每个数据点N&#x3D;40)。</li><li>d）b中WF、PRS-SIM和GT-SIM图像的Fourier环相关曲线中Conv. SIM(蓝色)、Sparse-SIM(绿色)、PRS-SIM(红色)和GT-SIM(棕色)沿黄色箭头所示直线的强度分布。根据截止频率计算分辨率，FRC阈值为0.24。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><h3 id="基于深度学习的SIM算法的现有挑战"><a href="#基于深度学习的SIM算法的现有挑战" class="headerlink" title="基于深度学习的SIM算法的现有挑战"></a>基于深度学习的SIM算法的现有挑战</h3><ol><li>一些现有技术采用“端到端”方案，直接将宽场或原始SIM图像转换为SR-SIM图像，而不充分利用由照明模式调制的高频信息，即摩尔条纹。因此，整个框架退化为SR推理任务(称为“图像超分辨率”)，而不是分析SR重建，这可能会受到光谱偏差问题的影响，并导致分辨率受损。</li><li>构建训练数据集需要大量匹配良好的低信噪比和高信噪比图像对，这对于低荧光效率或高动态的生物标本来说是费力的，甚至是不可行的。</li><li>神经网络的泛化能力受到限制，因为在监督训练方案中，预训练好的去噪模型不能可靠地转移到只有噪声数据的未知领域，这抑制了对前所未有的生物结构和生物过程的发现。</li></ol><h2 id="傅里叶环相关"><a href="#傅里叶环相关" class="headerlink" title="傅里叶环相关"></a>傅里叶环相关</h2><ol><li>FRC（Fourier Ring Correlation，傅里叶环相关）是一种用来评估图像分辨率的方法，特别是在显微镜成像和结构照明显微镜（SIM）等领域中经常使用。</li><li>它的基本原理是将同一对象的两幅独立图像（通常是同一样本的两次不同采集）转换到傅里叶空间，然后比较它们在不同频率下的相关性。通过计算傅里叶空间中不同频率环（圆环状区域）之间的相关性，FRC可以评估图像在不同分辨率下的一致性，从而确定图像的分辨率极限。</li><li>简单来说，FRC就是一种评估图像质量和分辨率的方法，通过比较两幅图像在不同频率上的一致性，来判断图像能达到的最佳分辨率。</li></ol><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Self-supervised denoising for multimodal structured illumination microscopy enables long-term super-resolution live-cell imaging</p><p>多模态结构照明显微镜的自监督去噪使长期超分辨率活细胞成像成为可能</p><p>期刊：PhotoniX</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：X. Chen，Chang Qiao</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2024年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年9月5日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub：<a href="https://github.com/IntellM-THUIBCS/PRS-SIM">https://github.com/IntellM-THUIBCS/PRS-SIM</a> (作者)</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> SIM </tag>
            
            <tag> 自监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>零射击学习实现了光学荧光显微镜的即时去噪和超分辨率</title>
      <link href="/2024/09/03/%E9%9B%B6%E5%B0%84%E5%87%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0%E4%BA%86%E5%85%89%E5%AD%A6%E8%8D%A7%E5%85%89%E6%98%BE%E5%BE%AE%E9%95%9C%E7%9A%84%E5%8D%B3%E6%97%B6%E5%8E%BB%E5%99%AA%E5%92%8C%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
      <url>/2024/09/03/%E9%9B%B6%E5%B0%84%E5%87%BB%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0%E4%BA%86%E5%85%89%E5%AD%A6%E8%8D%A7%E5%85%89%E6%98%BE%E5%BE%AE%E9%95%9C%E7%9A%84%E5%8D%B3%E6%97%B6%E5%8E%BB%E5%99%AA%E5%92%8C%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>提出了一个零镜头反卷积深度神经网络(ZS-DeconvNet)框架，该框架能够以无监督的方式训练DLSR网络，只使用一个低分辨率和低信噪比的单个平面图像或体图像堆栈，从而实现零镜头实现。</li><li>与最先进的DLSR方法相比，ZS-DeconvNet可以适应各种生物成像环境。</li><li>ZS-DeconvNet可以在衍射极限上提高1.5倍以上的分辨率，具有高保真度和可量化性，即使在单个低信噪比输入图像上进行训练，也不需要对图像进行特定参数调优。</li><li>引入了Hessian正则化项，减轻显微镜图像中的重建伪像，调节网络收敛。</li></ol><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409051552368.png"></p><ol><li>采用了由两个顺序连接的U-Nets29组成的双级DNN架构，作为ZS-DeconvNet的简单但有效的骨干(图1a, b和补充图2a)。第一阶段作为去噪器，根据去噪损失生成无噪声图像(方法)，第二阶段根据上述无监督反卷积损失增强图像分辨率。我们的经验发现，双阶段架构和物理模型调节损失函数稳定了训练过程，并赋予了整个网络模型的可解释性。</li><li>a）ZSDeconvNet的双阶段架构及其训练阶段示意图。</li><li>b） ZS-DeconvNet推理阶段示意图。</li><li>c） RL反褶积(第二列)、稀疏反褶积(第三列)和ZS-DeconvNet(第四列)重建的Lyso和MTs代表性SR图像。显示清晰的WF图像供参考。</li><li>d） RL反卷积、稀疏反卷积和ZS-DeconvNet在PSNR和分辨率方面的统计比较(n &#x3D; 100个感兴趣的区域)</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="论文中可以引用的部分"><a href="#论文中可以引用的部分" class="headerlink" title="论文中可以引用的部分"></a>论文中可以引用的部分</h2><h3 id="计算SR的两类方法"><a href="#计算SR的两类方法" class="headerlink" title="计算SR的两类方法"></a>计算SR的两类方法</h3><p>现有的计算SR方法可以分为两类：基于分析模型的方法和基于深度学习的方法。(基于分析模型的方法依赖于传统的数学和统计模型，而基于深度学习的方法则通过数据驱动的方式学习图像恢复的规律，通常在图像超分辨率任务中表现更为优异。)</p><h3 id="传统SIM的两个局限性"><a href="#传统SIM的两个局限性" class="headerlink" title="传统SIM的两个局限性"></a>传统SIM的两个局限性</h3><p>传统的SIM有两个关键的局限性：首先，进一步提高分辨率需要相当多的原始数据，即非线性SIM至少需要25张原始图像才能获得低于80纳米的分辨率；其次，SIM图像的后期构建通常需要具有高信噪比的原始图像来消除噪声引起的重建伪影，从而影响快速、低光和长期的活细胞成像。</p><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Zero-shot learning enables instant denoising and super-resolution in optical fluorescence microscopy</p><p>零射击学习实现了光学荧光显微镜的即时去噪和超分辨率</p><p>期刊：Nature Communications</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Chang Qiao</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2024年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年9月3日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>Github：<a href="https://github.com/TristaZeng/ZS-DeconvNet">https://github.com/TristaZeng/ZS-DeconvNet</a> (作者)</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> U-Net </tag>
            
            <tag> Zero-Shot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>U-Net++:重新设计跳跃连接以利用图像分割中的多尺度特征</title>
      <link href="/2024/09/01/U-Net-%E9%87%8D%E6%96%B0%E8%AE%BE%E8%AE%A1%E8%B7%B3%E8%B7%83%E8%BF%9E%E6%8E%A5%E4%BB%A5%E5%88%A9%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%B8%AD%E7%9A%84%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%89%B9%E5%BE%81/"/>
      <url>/2024/09/01/U-Net-%E9%87%8D%E6%96%B0%E8%AE%BE%E8%AE%A1%E8%B7%B3%E8%B7%83%E8%BF%9E%E6%8E%A5%E4%BB%A5%E5%88%A9%E7%94%A8%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%B8%AD%E7%9A%84%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%89%B9%E5%BE%81/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>提出了一种用于语义和实例分割的神经结构U-Net++。</li><li>通过不同深度的U-Net的有效集成来缓解未知的网络深度，使不同大小对象的分割性能得到改善，这是对固定深度U-Net的改进。这些U-Net部分共享编码器并使用深度监督同时共同学习。 </li><li>重新设计U-Net++中的跳跃连接，使解码器能够灵活地融合特征，聚合译码子网络中不同语义尺度的特征，形成高度灵活的特征融合方案，这是对U-Net中只需要融合相同尺度特征图的限制性跳过连接的改进。</li><li>设计一种剪枝方案，在保持其性能的同时加快其推理速度。</li><li>我们发现同时训练嵌入在U-Net++体系结构中的多深度U-Net可以促进组成U-Nets之间的协作学习，从而比单独训练具有相同体系结构的孤立U-Nets获得更好的性能。</li><li><span style="color: red;">跳跃连接在残差网络和密集网络等现代神经结构中得到了进一步的应用，有利于梯度流，让信息更容易地在网络中流动，从而帮助训练过程中的梯度（就是误差的反馈）更好地传递下去。这种方法最终提高了分类任务中网络的整体表现，使得网络能够更准确地进行分类。</span></li><li>U-Net++性能的提高归功于其嵌套结构和重新设计的跳跃连接，其目的是<span style="color: red;">解决U-Net的两个关键挑战: 1) 未知的最佳架构深度。2) 不必要的跳跃连接限制设计。</span>我们使用六种不同的生物医学成像应用程序对U-Net++进行了评估，并展示了在各种最先进的语义分割骨干和实例分割元框架上的一致性能改进。</li><li>六种不同的医学图像分割数据集对U-Net++进行了评估，涵盖了多种成像模式，如计算机断层扫描(CT)、磁共振成像(MRI)和电子显微镜(EM)，并证明了(1)U-Net++在不同数据集和骨干架构的语义分割任务中始终优于基线模型;(2)在固定深度U-Net的基础上，U-Net++提高了不同大小对象的分割质量;(3) Mask RCNN++ (Un-Net++设计的Mask R-CNN)在实例分割任务上优于原始的Mask R-CNN;(4)修剪后的U-Net++模型获得了显著的加速，同时只显示出适度的性能下降。</li></ol><h2 id="U-Net和全卷积网络FCN的局限性"><a href="#U-Net和全卷积网络FCN的局限性" class="headerlink" title="U-Net和全卷积网络FCN的局限性"></a>U-Net和全卷积网络FCN的局限性</h2><p>医学图像分割的最新模型是U-Net和全卷积网络(FCN)的变体，尽管这些模型取得了成功，但是它们有两个局限性：</p><ol><li><strong>模型的深度难以确定</strong>：<br>想象一下，模型的深度就像是楼层的数量。模型越深，可能在某些任务上表现得更好，但是我们并不事先知道多少层楼（即多深的模型）是最好的选择。因此，研究人员需要花费很多时间和资源去尝试不同深度的模型，或者进行大量的“架构搜索”，就像不断地测试不同高度的楼房，看哪个高度最合适。</li><li><strong>跳过连接的限制</strong>：<br>在U-Net这样的模型中，有一种称为“跳过连接”的技术，可以理解为在建楼的过程中，有些地方需要用桥梁连接不同楼层。问题在于，这些桥梁只能在建筑的相同高度之间搭建（比如第3层与第3层之间），而不能在不同高度之间连接。这种限制可能导致我们不能充分利用模型的所有信息。</li></ol><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409051550499.png"></p><ul><li>如图1(g)所示，U-Net++由不同深度的U-Nets组成，这些U-Nets的解码器通过重新设计的跳过连接以相同的分辨率紧密连接。</li><li>首先，U-Net++不容易选择网络深度，因为它在其架构中嵌入了不同深度的U-Net。所有这些U-Net部分共享一个编码器，而它们的解码器相互交织在一起。通过深度监督训练U-Net++，所有组成U-Net同时训练，同时受益于共享的图像表示。这种设计不仅提高了整体分割性能，而且可以在推理时间内对模型进行修剪。在训练 U-Net++ 时，通过“深度监督”方法，所有这些 U-Net 结构会同时进行训练，并且因为它们共享相同的图像表示（编码器部分的输出），所以可以互相帮助，从而提高整体的分割效果。此外，这种设计还允许在推理时（即模型实际应用时）对模型进行“修剪”，也就是根据需要调整或简化模型，以减少计算量或提高效率。</li><li>其次，U-Net++不受不必要的限制性跳过连接的限制，只有来自编码器和解码器的相同比例的特征映射才能融合。U-Net++中引入的重新设计的跳跃连接在解码器节点上呈现不同尺度的特征图，允许聚合层决定沿着跳跃连接携带的各种特征图应该如何与解码器特征图融合。重新设计的跳线连接在U-Net++中通过密集连接解码器来实现。</li><li>从U-Net到U-Net++的演变。图中的每个节点表示一个卷积块，向下的箭头表示向下采样，向上的箭头表示向上采样，点箭头表示跳过连接。(a-d)不同深度的U型网。(e)集成体系结构，即将不同深度的U-Net组合成一个统一的体系结构。所有U-Net(部分)共享相同的编码器，但有自己的解码器。(f) U-Net+是在U-Net e的基础上，去掉原有的跳线连接，每两个相邻的节点都用一个短跳线连接，使较深的解码器向较浅的解码器发送监督信号。(g) U-Net++由U-Net e构建，通过连接解码器，形成密集连接的跳跃连接，使得特征沿跳跃连接密集传播，从而在解码器节点更灵活地融合特征。因此，U-Net++解码器中的每个节点，从水平角度来看，在相同分辨率下结合其所有前节点的多尺度特征，从垂直角度来看，从其前节点集成不同分辨率的多尺度特征，如式1所示。U-Net++的这种多尺度特征聚合逐渐综合了分割，从而提高了准确性和更快的收敛速度，正如我们在第四节中的经验结果所证明的那样。请注意，训练U-Net e需要明确的深度监督(粗体链接)，但对于U-Net+和U-Net++来说是可选的(淡色链接)。</li></ul><h2 id="数据集语义分割"><a href="#数据集语义分割" class="headerlink" title="数据集语义分割"></a>数据集语义分割</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202409051551667.png"></p><h2 id="深度监督"><a href="#深度监督" class="headerlink" title="深度监督"></a>深度监督</h2><h3 id="什么是深度监督？"><a href="#什么是深度监督？" class="headerlink" title="什么是深度监督？"></a>什么是深度监督？</h3><p><strong>深度监督（Deep Supervision）</strong> 是一种在深度学习中用于改进模型训练的方法，特别是针对深层神经网络。这种方法通过在网络的中间层（而不仅仅是在最后一层）添加额外的监督信号（也就是额外的损失函数），来帮助模型在训练过程中更好地学习特征。通常在深度神经网络中，只有网络的最后一层会输出预测结果，并根据这些结果计算损失（误差）。这种设置可能导致前几层的训练不充分，特别是在非常深的网络中，前面层的梯度会变得很小，从而影响网络的学习能力。这就是“梯度消失”问题。</p><h3 id="关于深度监督的研究"><a href="#关于深度监督的研究" class="headerlink" title="关于深度监督的研究"></a>关于深度监督的研究</h3><p>​        He等人认为网络的深度d可以作为一个正则化器。Lee等人证明深度监督层可以提高隐藏层的学习能力，强制中间层学习判别特征，实现网络的快速收敛和正则化。Dense Net以隐式的方式执行类似的深度监督。深度监督也可以用在类似U-Net的架构中。deep Layer aggregation——一项同步但独立的研究，发表在CVPR-2018上。</p><p>​        通过结合不同分辨率特征图的预测引入深度监督，可以克服潜在的优化困难，从而达到更快的收敛速度和更强大的判别能力。Zhu等人在他们提出的体系结构中使用了八个额外的深度监督层。然而，我们的嵌套网络更适合深度监督下的训练：1)多个解码器自动生成全分辨率分割图；2)网络嵌入不同深度的U-Net，掌握多分辨率特征；3)紧密连接的特征图有助于平滑梯度流，给出相对一致的预测掩码;4)高维特征通过反向传播对每个输出产生影响，使我们能够在推理阶段对网络进行修剪。</p><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><p>无</p><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：UNet++: 重新设计跳跃连接以利用图像分割中的多尺度特征</p><p>UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation</p><p>期刊：arXiv e-prints</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Z. Zhou</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2019年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年9月1日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>Github：<a href="https://github.com/MrGiovanni/UNetPlusPlus">https://github.com/MrGiovanni/UNetPlusPlus</a> (作者)</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> U-Net </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用于医学图像分割的嵌套U-Net体系结构(U-Net++)</title>
      <link href="/2024/08/22/%E7%94%A8%E4%BA%8E%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%9A%84%E5%B5%8C%E5%A5%97U-Net%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
      <url>/2024/08/22/%E7%94%A8%E4%BA%8E%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%9A%84%E5%B5%8C%E5%A5%97U-Net%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>提出了一种基于嵌套和密集跳跃连接的分割架构U-Net++，背后的基本假设是，当来自编码器网络的高分辨率特征图在与来自解码器网络的相应语义丰富的特征图融合之前逐渐丰富时，该模型可以更有效地捕获前景对象的细粒度细节。我们认为，当来自解码器和编码器网络的特征映射在语义上相似时，网络将更容易处理学习任务。</li><li>作者提出的架构本质上是一个深度监督的编码器-解码器网络，其中编码器和解码器子网络通过一系列嵌套的、密集的跳过路径连接。重新设计的跳过路径旨在减少编码器和解码器子网络特征映射之间的语义差距。</li><li>将U-Net++与U-Net和宽U-Net架构在胸部低剂量CT扫描中的结节分割，显微镜图像中的细胞核分割，腹部CT扫描中的肝脏分割，结肠镜视频中的息肉分割等数据集上进行了对比评估。</li><li>这与U-Net中常用的普通跳过连接形成对比，后者直接将高分辨率特征图从编码器快速推进到解码器网络，从而导致语义上不同的特征图融合。</li></ol><h2 id="U-Net-网络架构"><a href="#U-Net-网络架构" class="headerlink" title="U-Net++网络架构"></a>U-Net++网络架构</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408221312855.png"></p><ul><li><span style="color: red;">U-Net++背后的主要思想是在融合之前弥合编码器和解码器的特征映射之间的语义差距。</span></li><li>Net++与U-Net++(图1a中的黑色部分)的区别在于重新设计的连接两个子网的跳过路径(绿色和蓝色)以及深度监督的使用(红色)。</li><li>重新设计的跳过路径改变了编码器和解码器子网络的连通性。在U-Net中，编码器的特征映射直接在解码器中接收；然而，在U-Net++中，它们经历一个密集的卷积块，其卷积层的数量取决于金字塔级别。</li><li>在U-Net++中使用深度监督，使模型能够在两种模式下运行：1)精确模式，其中所有分割分支的输出都是平均的；2)快速模式，仅从一个分割分支中选择最终的分割映射，其选择决定了模型修剪的程度和速度增益。图1c显示了快速模式下分割分支的选择如何导致不同复杂性的架构。</li><li>U-Net++与原始的U-Net在三个方面不同：1)在跳过路径(绿色显示)上具有卷积层，它弥合了编码器和解码器特征映射之间的语义差距；2)箕斗路径上有密集的箕斗连接(蓝色部分)，改善了梯度流；3)具有深度监督(用红色表示)，它可以进行模型修剪，并提高或在最坏的情况下达到与仅使用一个损失层相当的性能。</li></ul><h2 id="不同数据集上的分隔结果"><a href="#不同数据集上的分隔结果" class="headerlink" title="不同数据集上的分隔结果"></a>不同数据集上的分隔结果</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408221325328.png"></p><h2 id="复杂度，速度，准确性分析"><a href="#复杂度，速度，准确性分析" class="headerlink" title="复杂度，速度，准确性分析"></a>复杂度，速度，准确性分析</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408221331623.png"></p><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="特征图，密集跳跃，语义相似性"><a href="#特征图，密集跳跃，语义相似性" class="headerlink" title="特征图，密集跳跃，语义相似性"></a>特征图，密集跳跃，语义相似性</h2><ol><li><strong>特征图和网络结构</strong>：在 U-Net++ 中，网络分为两个主要部分：编码器（负责提取图像的特征）和解码器（负责将这些特征转化为最终的分割结果）。特征图就是这些提取的特征数据的“图像”。</li><li><strong>嵌套和密集跳跃连接</strong>：U-Net++ 使用了一些特别的连接方式来帮助这些特征图更好地融合。嵌套连接就是把编码器和解码器的特征图以一种更复杂的方式结合在一起，而密集跳跃连接则是将来自不同层的特征图直接连接起来。这样做可以让网络更好地利用不同层的信息。</li><li><strong>细粒度细节</strong>：网络的设计假设是，如果编码器提取的高分辨率细节图（即包含细节的特征图）在与解码器的特征图融合之前，经过更多的处理和丰富，那么网络就能更好地捕捉到前景对象的细节。这意味着网络能够更准确地识别和分割出图像中的细小部分。</li><li><strong>语义上的相似性</strong>：当解码器和编码器的特征图在语义上更相似（即它们包含的信息对解决问题更一致）时，网络更容易学习和处理任务。这就像是把两个相似的特征图拼接在一起，网络就能更容易地理解和利用这些信息来做出正确的分割。</li></ol><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：UNet++: A Nested U-Net Architecture for Medical Image Segmentation</p><p>用于医学图像分割的嵌套U-Net体系结构</p><p>期刊：arXiv</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Zongwei Zhou</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2018年7月</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年8月22日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>Github：<a href="https://github.com/MrGiovanni/UNetPlusPlus">https://github.com/MrGiovanni/UNetPlusPlus</a> (作者)</p><p><a href="https://github.com/bigmb/Unet-Segmentation-Pytorch-Nest-of-Unets">https://github.com/bigmb/Unet-Segmentation-Pytorch-Nest-of-Unets</a></p><p><a href="https://github.com/LeeJunHyun/Image_Segmentation">https://github.com/LeeJunHyun/Image_Segmentation</a></p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> U-Net </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>注意力U-Net:学习在哪里寻找胰腺(Attention U-Net)</title>
      <link href="/2024/08/20/%E6%B3%A8%E6%84%8F%E5%8A%9BU-Net-%E5%AD%A6%E4%B9%A0%E5%9C%A8%E5%93%AA%E9%87%8C%E5%AF%BB%E6%89%BE%E8%83%B0%E8%85%BA/"/>
      <url>/2024/08/20/%E6%B3%A8%E6%84%8F%E5%8A%9BU-Net-%E5%AD%A6%E4%B9%A0%E5%9C%A8%E5%93%AA%E9%87%8C%E5%AF%BB%E6%89%BE%E8%83%B0%E8%85%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408201416966.png"></p><ol><li>提出了一种Attention Gate(AG)模型的医学成像，自动学习聚焦在不同形状和大小的目标结构。经过AGs训练的模型隐式学习<span style="color: red;">抑制输入图像中的不相关区域，同时突出显示对特定任务有用的显著特征。</span>这使我们能够消除使用级联卷积神经网络(CNN)的显式外部组织&#x2F;器官定位模块的必要性。</li><li>AGs可以很容易地集成到标准的CNN架构中，如U-Net模型，以最小的计算开销，同时提高模型的灵敏度和预测精度。</li><li>在两个大型CT腹部数据集上对所提出的注意力U-Net架构进行了多类图像分割的评估。</li></ol><h2 id="Attention-U-Net框架"><a href="#Attention-U-Net框架" class="headerlink" title="Attention U-Net框架"></a>Attention U-Net框架</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408201403056.png"></p><ol><li>在网络的编码部分，输入图像在每个尺度上以2的倍数逐步滤波和下采样(例如H4 &#x3D; H1&#x2F;8)。Nc表示类的数量。注意门(AGs)过滤通过跳过连接传播的特征。AGs的示意图如图2所示。AGs中的特征选择性是通过使用在较粗尺度上提取的上下文信息(门控)来实现的。</li><li>Attention U-Net的优势主要与图像多尺度特征提取有关。粗糙的特征地图捕获上下文信息，并突出前景对象的类别和位置。多尺度提取的特征图通过跳跃连接进行合并，将粗层次和细层次的密集预测结合起来，如图1所示。</li></ol><h2 id="注意力门控-Attention-Gates"><a href="#注意力门控-Attention-Gates" class="headerlink" title="注意力门控(Attention Gates)"></a>注意力门控(Attention Gates)</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408201414860.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408201438447.png"></p><ol><li>在UNet中，可将收缩路径视为编码器，而将扩展路径视为解码器。UNet的有趣之处在于，<span style="color: red;">跳跃连接允许在解码器期间直接使用由编码器提取的特征。</span>这样，在“重建”图像的掩模时，网络就学会了使用这些特征，因为收缩路径的特征与扩展路径的特征是连接在一起的。<span style="color: red;">在此连接之前应用一个注意力块，可以让网络对跳转连接相关的特征施加更多的权重。它允许直接连接专注于输入的特定部分，而不是输入每个特征。将注意力分布乘上跳转连接特征图，只保留重要的部分。</span>这种注意力分布是从所谓的query(输入)和value(跳跃连接)中提取出来的。注意力操作允许有选择地选择包含在值中的信息。此选择基于query。总结：输入和跳跃连接用于决定要关注跳跃连接的哪些部分。然后，我们使用skip连接的这个子集，以及标准展开路径中的输入。</li></ol><h2 id="胰腺和腹部图像分隔"><a href="#胰腺和腹部图像分隔" class="headerlink" title="胰腺和腹部图像分隔"></a>胰腺和腹部图像分隔</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408201428777.png"></p><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><p>无</p><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Attention U-Net: Learning Where to Look for the Pancreas</p><p>注意力U-Net：学习在哪里寻找胰腺</p><p>期刊：arXiv</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Ozan Oktay</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2018年4月</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年8月20日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>Github：<a href="https://github.com/bigmb/Unet-Segmentation-Pytorch-Nest-of-Unets">https://github.com/bigmb/Unet-Segmentation-Pytorch-Nest-of-Unets</a></p><p><a href="https://github.com/LeeJunHyun/Image_Segmentation">https://github.com/LeeJunHyun/Image_Segmentation</a></p><p><a href="https://github.com/ozan-oktay/Attention-Gated-Networks?tab=readme-ov-file">https://github.com/ozan-oktay/Attention-Gated-Networks?tab=readme-ov-file</a> (作者)</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> U-Net </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于U-Net的递归残差卷积神经网络(R2U-Net)用于医学图像分割</title>
      <link href="/2024/08/17/%E5%9F%BA%E4%BA%8EU-Net%E7%9A%84%E9%80%92%E5%BD%92%E6%AE%8B%E5%B7%AE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-R2U-Net-%E7%94%A8%E4%BA%8E%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
      <url>/2024/08/17/%E5%9F%BA%E4%BA%8EU-Net%E7%9A%84%E9%80%92%E5%BD%92%E6%AE%8B%E5%B7%AE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-R2U-Net-%E7%94%A8%E4%BA%8E%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>本文提出了一种基于U-Net的<mark>循环卷积神经网络(RCNN)</mark>和一种基于U-Net模型的<mark>循环残差卷积神经网络(RRCNN)</mark>，分别命名为RU-Net和R2U-Net。</li><li>实验采用基于patch的视网膜血管分割、皮肤癌分割和肺分割三种不同的医学成像方式，对所提模型进行了性能评估。</li></ol><h2 id="医学图像分隔"><a href="#医学图像分隔" class="headerlink" title="医学图像分隔"></a>医学图像分隔</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408171523591.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408191439244.png"></p><h2 id="U-Net基本架构"><a href="#U-Net基本架构" class="headerlink" title="U-Net基本架构"></a>U-Net基本架构</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408171525564.png"></p><ol><li>分类任务的CNN模型需要一个编码单元，并提供类概率作为输出。在分类任务中，我们使用激活函数执行卷积操作，然后进行子采样层，从而降低特征图的维数。当输入样本遍历网络各层时，特征映射的数量增加，但特征映射的维数减少。如图2中模型的第一部分(绿色部分)所示。由于特征映射的数量在更深的层中增加，网络参数的数量也相应增加。最后，在网络的末端应用Softmax运算来计算目标类的概率。</li><li>与分类任务相反，分割任务的架构需要卷积编码和解码单元。编码单元用于将输入图像编码成大量的低维地图。解码单元用于执行上卷积(反卷积)操作，以产生与原始输入图像具有相同维数的分割映射。因此，与分类任务的体系结构相比，分割任务的体系结构通常需要几乎两倍的网络参数。因此，为分割任务设计高效的<mark>深度卷积神经网络(DCNN)</mark>架构是非常重要的，它可以在较少的网络参数下保证更好的性能。</li><li>基本U-Net模型示意图如图2所示。从结构上看，网络主要由卷积编码单元和卷积解码单元两部分组成。执行基本的卷积操作，然后在网络的两个部分激活ReLU。对于编码单元中的下采样，执行2×2最大池操作。在解码阶段，执行卷积转置(表示上卷积或反卷积)操作来对特征映射进行上采样。U-Net的第一个版本被用来从编码单元裁剪和复制特征映射到解码单元。U-Net模型为分段任务提供了几个优势:首先，该模型允许同时使用全局位置和上下文。其次，它可以使用很少的训练样本，为分割任务提供更好的性能。第三，端到端流水线在前向通道中处理整个图像并直接生成分割图。这确保了U-Net保留了输入图像的完整背景，与基于补丁的分割方法相比，这是一个主要优势。</li></ol><h2 id="基于循环卷积层-RCL-的U-Net架构"><a href="#基于循环卷积层-RCL-的U-Net架构" class="headerlink" title="基于循环卷积层(RCL)的U-Net架构"></a>基于循环卷积层(RCL)的U-Net架构</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408191426179.png"></p><h2 id="卷积和循环卷积单元不同的变体"><a href="#卷积和循环卷积单元不同的变体" class="headerlink" title="卷积和循环卷积单元不同的变体"></a>卷积和循环卷积单元不同的变体</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408191417581.png"></p><ol><li>采用具有前向卷积层和特征连接的U-Net作为替代U-Net初级版本中的裁剪和复制方法。如图4(a)所示。</li><li>采用带残差连通性的前向卷积层U-Net，通常称为残差U-Net (ResU-Net)，如图4(c)所示。</li><li>第三种架构是U-Net，具有正向循环卷积层，如图4(b)所示，命名为RU-Net。</li><li>如图4(d)所示具有残差连通性的递归卷积层的U-Net，命名为R2U-Net。</li><li>残差单元有助于训练深度架构，使用循环残差卷积层进行特征积累，确保更好地表示分割任务的特征。</li></ol><h2 id="循环卷积层-RCL"><a href="#循环卷积层-RCL" class="headerlink" title="循环卷积层(RCL)"></a>循环卷积层(RCL)</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408191424145.png"></p><ol><li>在编码和解码单元中，使用<mark>循环卷积层(RCL)</mark>和带有残差单元的RCL来代替常规的前向卷积层。带有RCL的剩余单元有助于开发更有效的深层模型。其次，在两种模型的RCL单元中都加入了有效的特征积累方法。基于CNN的医学图像分割方法显示了从网络的一部分到另一部分特征积累的有效性。</li><li>在该模型中，元素特征求和在U-Net模型之外执行。该模型只在训练过程中表现出更好的收敛性。然而，由于模型内部的特征积累，我们提出的模型在训练和测试阶段都显示出好处。相对于不同时间步长的特征积累确保了更好更强的特征表示。因此，它有助于提取非常低层次的特征，这些特征对于不同模式的医学成像(如血管分割)的分割任务至关重要。</li><li>我们从基本的U-Net模型中删除了裁剪和复制单元，只使用连接操作，从而产生了一个非常复杂的体系结构，从而获得了更好的性能。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="U-Net中的裁剪与复制方法"><a href="#U-Net中的裁剪与复制方法" class="headerlink" title="U-Net中的裁剪与复制方法"></a>U-Net中的裁剪与复制方法</h2><ol><li><strong>裁剪 (Cropping)</strong>: 在U-Net中，编码器部分通过多次卷积和池化操作逐渐减少特征图的尺寸。然而，解码器部分则通过上采样逐渐恢复特征图的尺寸。由于池化操作可能导致特征图的尺寸与解码器中的上采样特征图不匹配，因此需要对编码器的特征图进行裁剪，使其尺寸与解码器对应层的特征图一致。</li><li><strong>复制 (Copying)</strong>: 裁剪后的特征图会被复制，并与解码器部分对应层的特征图连接（即进行特征图的拼接）。这种跳跃连接可以帮助网络在解码阶段恢复更精确的空间信息，从而提高分割的精度。</li></ol><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation</p><p>基于U-Net的递归残差卷积神经网络(R2U-Net)用于医学图像分割</p><p>期刊：arXiv</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Md Zahangir Alom</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2018年2月</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年8月17日—2024年8月19日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>Github：<a href="https://github.com/bigmb/Unet-Segmentation-Pytorch-Nest-of-Unets">https://github.com/bigmb/Unet-Segmentation-Pytorch-Nest-of-Unets</a></p><p><a href="https://github.com/LeeJunHyun/Image_Segmentation">https://github.com/LeeJunHyun/Image_Segmentation</a></p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> U-Net </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>U-Net:生物医学图像分割的卷积网络</title>
      <link href="/2024/08/17/U-Net-%E7%94%9F%E7%89%A9%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
      <url>/2024/08/17/U-Net-%E7%94%9F%E7%89%A9%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>以<mark>FCN全卷积神经网络</mark>(Fully Convolution Networks for Semantic Segmentation)为基础设计了Unet，包含两条串联的路径：contracting path用来提取图像特征，捕捉context信息，将图像压缩为由特征组成的feature maps；expanding path用来精准定位，precise localization，将提取的特征解码为与原始图像尺寸一样的分割后的预测图像。</li><li>与FCN不同的是，在上采样过程中保留了大量的特征通道(Feature Channels)，从而使更多的信息能流入最终复原的分割图像中。另外，为了降低在压缩路径上损失的图像信息，还将Contracting Path和Expanding Path同尺寸的Feature Map进行叠加，再继续进行卷积和上采样工作，以此整合更多信息进行图像分割。</li></ol><h2 id="U-Net网络结构"><a href="#U-Net网络结构" class="headerlink" title="U-Net网络结构"></a>U-Net网络结构</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408170912300.png"></p><h2 id="Overlap-Tile策略"><a href="#Overlap-Tile策略" class="headerlink" title="Overlap-Tile策略"></a>Overlap-Tile策略</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408170925360.png"></p><ol><li>该策略的思想是：对图像的某一块像素点（黄框内部分）进行预测时，需要该图像块周围的像素点（蓝色框内）提供上下文信息(context)，以获得更准确的预测。简单地说，就是在预处理中，对输入图像进行Padding，通过Padding扩大输入图像的尺寸，使得最后输出的结果正好是原始图像的尺寸， 同时， 输入图像块（黄框）的边界也获得了上下文信息从而提高预测的精度，本文用的是Mirror Padding。</li><li>医学图像是一般相当大，但是分割时候不可能将原图太小输入网络，所以必须切成一张一张的小Patch，在切成小Patch的时候，Unet由于网络结构的原因，因此适合用Overlap的切图 Overlap部分可以为分割区域边缘部分提供文理等信息, 并且分割结果并没有受到切成小Patch而造成分割情况不好。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><p>无</p><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：U-Net: Convolutional Networks for Biomedical Image Segmentation</p><p>U-Net:生物医学图像分割的卷积网络</p><p>期刊：MICCAI</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Olaf Ronneberger</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2015年5月</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年8月17日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>Github：<a href="https://github.com/bigmb/Unet-Segmentation-Pytorch-Nest-of-Unets">https://github.com/bigmb/Unet-Segmentation-Pytorch-Nest-of-Unets</a></p><p><a href="https://github.com/LeeJunHyun/Image_Segmentation">https://github.com/LeeJunHyun/Image_Segmentation</a></p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> U-Net </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于深度学习的超分辨率无标签暗场显微镜</title>
      <link href="/2024/08/16/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%97%A0%E6%A0%87%E7%AD%BE%E6%9A%97%E5%9C%BA%E6%98%BE%E5%BE%AE%E9%95%9C/"/>
      <url>/2024/08/16/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%97%A0%E6%A0%87%E7%AD%BE%E6%9A%97%E5%9C%BA%E6%98%BE%E5%BE%AE%E9%95%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>提出了一个基于CNN的U_Net框架，用于在不修改标准<mark>暗场显微镜(DFM)</mark>设置的情况下，通过检索斜照射获得的高空间频率信息来实时提高单帧DFM图像的分辨率。</li><li>利用已知的DFM正演过程，我们在不需要实验图像的情况下，根据光学装置的参数，数值生成训练数据集。在模拟数据集上训练网络，检索编码在暗场图像中的高空间频率信息。(一种基于合成孔径思想的仿真方法，在了解物镜NA、照明角度和工作波长等光学知识的情况下，生成低分辨率和超分辨率DFM图像对)</li><li>训练完网络后，评估了它在未见过的模拟图像上的性能。用50×&#x2F;0.55 NA物镜获得的100 nm聚苯乙烯珠对暗场图像进行了超分辨率重建实验。此外，分别对相位目标和HeLa细胞的暗场图像进行超分辨率图像重建。</li></ol><h2 id="暗场显微镜工作原理"><a href="#暗场显微镜工作原理" class="headerlink" title="暗场显微镜工作原理"></a>暗场显微镜工作原理</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408161338852.png"></p><ol><li>在DFM中，照明的中心部分被暗场环遮挡，如图1(a)所示。照射光通过聚光镜后，光线变得倾斜，以一定角度照射在样品上，此时物镜只能收集来自物体的散射光。</li><li>根据合成孔径的思路，傅立叶空间中环形对应的角度照明模式将物体的高空间频率信息转移到物镜的检测带宽中，如图1(b)所示。</li><li>光学系统可获得的最高空间频率信息受照明和检测物镜的数值孔径(NA)的共同限制。高空间频率信息被编码在衍射极限暗场图像中。因此，通过检索编码的高空间频率信息，可以从检测到的低分辨率图像重构出超分辨率图像(见图1(c))。</li></ol><h2 id="模拟生成数据集"><a href="#模拟生成数据集" class="headerlink" title="模拟生成数据集"></a>模拟生成数据集</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408161351925.png"></p><ol><li>在非相干光照明下，暗场图像可以看作是每个点源照明在照明暗场环上产生的多个离轴图像的叠加。每个角度照明将物体对应的高空间频率信息转移到物镜的检测带宽中，该带宽为图2(a)的子集所示的傅立叶平面低通滤波器。考虑非相干光照，最终的低分辨率暗场图像ILR可视为各角度光照下的图像之和。近似计算为：</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408161357916.png"></p><ol start="2"><li>同样，包含目标所有可探测高频信息的超分辨率暗场图像ISR可以计算为：</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408161359368.png"></p><h2 id="不同数据集上的复原效果"><a href="#不同数据集上的复原效果" class="headerlink" title="不同数据集上的复原效果"></a>不同数据集上的复原效果</h2><h3 id="聚苯乙烯珠"><a href="#聚苯乙烯珠" class="headerlink" title="聚苯乙烯珠"></a>聚苯乙烯珠</h3><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408161409951.png"></p><h3 id="HeLa细胞"><a href="#HeLa细胞" class="headerlink" title="HeLa细胞"></a>HeLa细胞</h3><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408161410993.png"></p><h2 id="U-Net网络"><a href="#U-Net网络" class="headerlink" title="U_Net网络"></a>U_Net网络</h2><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408161433088.png"></p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">double_conv</span>(<span class="params">in_channels, out_channels</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    )   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_class</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">                </span><br><span class="line">        <span class="variable language_">self</span>.dconv_down1 = double_conv(<span class="number">1</span>, <span class="number">64</span>)</span><br><span class="line">        <span class="variable language_">self</span>.dconv_down2 = double_conv(<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">        <span class="variable language_">self</span>.dconv_down3 = double_conv(<span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line">        <span class="variable language_">self</span>.dconv_down4 = double_conv(<span class="number">256</span>, <span class="number">512</span>)        </span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.maxpool = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.upsample = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>)        </span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.dconv_up3 = double_conv(<span class="number">256</span> + <span class="number">512</span>, <span class="number">256</span>)</span><br><span class="line">        <span class="variable language_">self</span>.dconv_up2 = double_conv(<span class="number">128</span> + <span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        <span class="variable language_">self</span>.dconv_up1 = double_conv(<span class="number">128</span> + <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.conv_last = nn.Conv2d(<span class="number">64</span>, n_class, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        conv1 = <span class="variable language_">self</span>.dconv_down1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.maxpool(conv1)</span><br><span class="line"></span><br><span class="line">        conv2 = <span class="variable language_">self</span>.dconv_down2(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.maxpool(conv2)</span><br><span class="line">        </span><br><span class="line">        conv3 = <span class="variable language_">self</span>.dconv_down3(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.maxpool(conv3)   </span><br><span class="line">        </span><br><span class="line">        x = <span class="variable language_">self</span>.dconv_down4(x)</span><br><span class="line">        </span><br><span class="line">        x = <span class="variable language_">self</span>.upsample(x)        </span><br><span class="line">        x = torch.cat([x, conv3], dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        x = <span class="variable language_">self</span>.dconv_up3(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.upsample(x)        </span><br><span class="line">        x = torch.cat([x, conv2], dim=<span class="number">1</span>)       </span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.dconv_up2(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.upsample(x)        </span><br><span class="line">        x = torch.cat([x, conv1], dim=<span class="number">1</span>)   </span><br><span class="line">        </span><br><span class="line">        x = <span class="variable language_">self</span>.dconv_up1(x)</span><br><span class="line">        </span><br><span class="line">        out = <span class="variable language_">self</span>.conv_last(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="暗场显微镜"><a href="#暗场显微镜" class="headerlink" title="暗场显微镜"></a>暗场显微镜</h2><ol><li><strong>光线照射方式</strong>：在暗场显微镜中，光线不是直接通过样本照射到观察者的眼睛，而是以一定角度斜射入样本。只有被样本散射或反射的光进入观察视野，直接光线则被阻挡。这使得背景变暗，而样本的边缘和细节由于散射光而明亮。</li><li><strong>高对比度</strong>：由于背景是黑暗的，而样本在黑暗的背景上发光，暗场显微镜提供了高对比度的图像。这有助于观察微小和透明的生物样本，如细菌、血液样本中的细胞、以及其他微生物。</li><li><strong>无染色观察</strong>：暗场显微镜可以在无需染色的情况下观察样本，适用于不想改变或破坏样本的实验。</li></ol><h2 id="无标签成像技术"><a href="#无标签成像技术" class="headerlink" title="无标签成像技术"></a>无标签成像技术</h2><p>无标签成像技术是指在不使用荧光或其他标记物标记样本的情况下，对生物样本或材料进行成像的技术，可以在不改变样本化学成分或结构的情况下，提供样本的高分辨率和高对比度图像。以下是几种常见的无标签成像技术：</p><ol><li><strong>相差显微镜（Phase Contrast Microscopy）</strong>：这项技术通过将样本不同区域的光线相位差转换为亮度差异，从而增强样本中细微结构的对比度。这使得研究者可以在不使用染料的情况下观察活体细胞和组织。</li><li><strong>差分干涉对比显微镜（Differential Interference Contrast, DIC）</strong>：DIC显微镜使用两束偏振光，经过样本后再相互干涉，产生三维立体效果。这种方法可以提供高分辨率的无染色样本的清晰图像。</li><li><strong>共聚焦显微镜（Confocal Microscopy）</strong>：通过使用光学切片技术和激光扫描，共聚焦显微镜能够在不使用荧光染料的情况下获得样本的高分辨率图像。这对于观察活体样本的细微结构非常有用。</li><li><strong>相干反斯托克斯拉曼散射（Coherent Anti-Stokes Raman Scattering, CARS）显微镜</strong>：CARS显微镜利用拉曼散射技术，根据样本分子振动特性生成图像。由于该技术依赖于样本本身的化学特性，因此无需染色或标记。</li><li><strong>光学相干断层扫描（Optical Coherence Tomography, OCT）</strong>：OCT是一种使用近红外光来获取样本微观结构的成像技术。它在医学领域中广泛应用，特别是眼科和心脏病学。</li><li><strong>电子显微镜技术</strong>：扫描电子显微镜（SEM）和透射电子显微镜（TEM）能够在超高分辨率下观察样本的表面和内部结构，而不需要对样本进行化学标记。</li></ol><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Super resolution label-free dark-field microscopy by deep learning</p><p>基于深度学习的超分辨率无标签暗场显微镜</p><p>期刊：Nanoscale(二区)</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Ming Lei</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2024年1月23日</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年8月16日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>地址：<a href="https://github.com/ML-UCSD/DLDFM">https://github.com/ML-UCSD/DLDFM</a></p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 超分辨率成像 </tag>
            
            <tag> 显微镜 </tag>
            
            <tag> 无标签成像 </tag>
            
            <tag> U-Net </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于条件对抗网络的图像到图像转换(Pix2Pix)</title>
      <link href="/2024/08/14/%E5%9F%BA%E4%BA%8E%E6%9D%A1%E4%BB%B6%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%B0%E5%9B%BE%E5%83%8F%E7%BF%BB%E8%AF%91/"/>
      <url>/2024/08/14/%E5%9F%BA%E4%BA%8E%E6%9D%A1%E4%BB%B6%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%B0%E5%9B%BE%E5%83%8F%E7%BF%BB%E8%AF%91/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>研究了条件对抗网络作为图像到图像翻译问题的通用解决方案。该网络不仅学习了从输入图像到输出图像的映射，还学习了一个损失函数来训练这个映射，这使得使用一个通用框架就能解决很多不同类型的问题成为可能。</li><li>当我们使用一种简单的方法<span style="color: red;">让卷积神经网络（CNN）最小化它对图像预测与真实图像之间的欧几里得距离时，结果往往会变得模糊。这是因为欧几里得距离损失函数会使CNN在所有可能的预测中求平均，从而导致模糊的结果。找到合适的损失函数，迫使CNN生成我们真正想要的输出—例如清晰、逼真的图像—是一个尚未解决的问题，需要专业知识和经验。换句话说，设计一个能够指导CNN生成更符合预期的清晰图像的损失函数是一项具有挑战性的任务。</span></li><li>GAN（生成对抗网络）通过一种独特的学习方式来生成更逼真的图像。它包括两个部分：一个是“判别器”，它的任务是判断一张图像是真实的还是由模型生成的；另一个是“生成器”，它试图生成足够逼真的图像以骗过判别器。模糊的图像很容易被判别器识别为假，因为它们不像真实的图像那么清晰。这种逼迫生成器必须输出清晰、细节丰富的图像。由于GAN学习过程中会根据数据自适应损失函数，它能够应用于很多传统上需要不同类型损失函数的任务，比如图像修复、风格迁移等。这样，GAN可以在很多不同的图像生成任务中表现出色。</li><li>条件GAN（条件生成对抗网络）与普通GAN的不同之处在于它的损失是通过学习得来的。这意味着条件GAN可以根据不同的任务，动态地调整对生成图像的要求。具体来说，条件GAN可以识别并惩罚生成的图像与目标图像之间的任何差异，无论是颜色、形状还是纹理。换句话说，它可以根据我们提供的条件，灵活地适应并生成我们想要的特定类型的图像。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408141320113.png"></p><h2 id="Conditonal-GANs"><a href="#Conditonal-GANs" class="headerlink" title="Conditonal GANs"></a>Conditonal GANs</h2><ol><li>对于生成器，使用了基于U-Net的架构，对于鉴别器，使用了卷积的PatchGAN分类器，它只在图像补丁的尺度上惩罚结构。</li><li><span style="color: red;">GAN是一种生成模型，它学习从随机噪声向量z到输出图像y的映射(G：z—&gt;y)，相比之下，条件GAN学习从观测图像x和随机噪声向量z到y(G：{x,z}—&gt;y), 生成器G经过训练，产生的输出不能被对抗训练的鉴别器D与“真实”图像区分开来，而鉴别器D被训练得尽可能地检测生成器的“伪造”图像。</span>这个训练过程如图2所示。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408141250791.png"></p><h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><p>​        评价合成图像的质量是一个复杂的问题。传统的评价方法，比如逐像素的均方误差（MSE），只能简单地计算每个像素之间的差异。这种方法不能理解图像整体的结构、细节或者纹理，因为它没有考虑像素之间的关系或分布。简单来说，MSE只看单个像素的差异，而不看图像整体的结构和细节变化。因此，这种方法无法有效地衡量合成图像中更高级别的特征，比如图像的整体形状、纹理或者视觉上的一致性。</p><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="欧几里得距离"><a href="#欧几里得距离" class="headerlink" title="欧几里得距离"></a>欧几里得距离</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408141057621.png"></p><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Image-to-Image Translation with Conditional Adversarial Networks</p><p>基于条件对抗网络的图像到图像转换</p><p>期刊：CVPR</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Phillip Isola</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2017年</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年8月14日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>GitHub地址：<a href="https://github.com/phillipi/pix2pix">https://github.com/phillipi/pix2pix</a></p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>光学显微镜图像超分辨深度神经网络的评价与发展</title>
      <link href="/2024/08/10/%E5%85%89%E5%AD%A6%E6%98%BE%E5%BE%AE%E9%95%9C%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AF%84%E4%BB%B7%E4%B8%8E%E5%8F%91%E5%B1%95/"/>
      <url>/2024/08/10/%E5%85%89%E5%AD%A6%E6%98%BE%E5%BE%AE%E9%95%9C%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AF%84%E4%BB%B7%E4%B8%8E%E5%8F%91%E5%B1%95/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>首先使用多模态结构照明显微镜(SIM)，提供了一个广泛的LR-SR图像对数据集BioSR，从结构复杂性、信噪比和升级因子等方面评估了深度学习SR模型。</li><li>为了进一步提高基于深度学习的超分辨率(DLSR)成像性能，<span style="color: red;">设计了DFCAN和DFGAN架构，它们利用不同特征之间的频率内容差异，在通过网络传播时自适应地重新调整其权重。这种策略使网络能够学习从LR到SR图像的精确层次映射。</span>(该网络利用不同特征之间的频率内容差异来学习不同生物结构高频信息的精确分层表示)</li><li>自制的多模态结构照明显微镜(SIM)系统，该系统集成了全内反射荧光(TIRF-SIM)、放牧发生率(GI-SIM)13和非线性SIM14(Methods)，在输入LR图像的广泛信噪比(SNR)水平、观察到的生物结构的复杂性和所需的放大因子范围内获得匹配良好的LR-SR图像对。该数据集被命名为BioSR，并已公开提供。</li><li>众所周知，输入LR图像的功率谱被限制在衍射限制频率以下，因此我们推测，利用傅里叶域中不同特征之间的频率含量差异，而不是空间域中的结构差异，可能使<mark>基于深度学习的超分辨率(DLSR)</mark>网络能够更精确、更有效地学习高频信息的分层表示。</li></ol><h2 id="实验数据的采集"><a href="#实验数据的采集" class="headerlink" title="实验数据的采集"></a>实验数据的采集</h2><ol><li>我们采用多模态SIM系统获取网格蛋白包覆凹坑(ccp)、内质网(ER)、微管(mt)和f -肌动蛋白丝的数据集。</li><li>WF图像被用作DLSR网络的输入LR图像。</li><li>对于每种类型的标本，我们在10个不断升级的激发光强度下获得了大约50组原始SIM图像。特别是，在最高激励水平下，我们确保所有原始图像的信噪比足够高，以重建高质量的SIM图像。将每组原始SIM图像平均为衍射限宽视场(WF)图像，并重构为SR-SIM图像。</li></ol><h2 id="具有代表性的DLSR模型"><a href="#具有代表性的DLSR模型" class="headerlink" title="具有代表性的DLSR模型"></a>具有代表性的DLSR模型</h2><ul><li>SRCNN—SR卷积神经网络，这是最早的具有轻量级配置的SR网络。</li><li>增强型深度SR (EDSR)网络—残差卷积块构建深度CNN的最新发展，以实现极大扩展的网络深度。</li><li>使用条件GAN(称为Pix2Pix)进行图像到图像的转换，和跨模态GAN (CMGAN)。</li><li>为了定量评价不同SR方法的性能，我们将归一化均方根误差(NRMSE)、多尺度结构相似性指数(MS-SSIM)和分辨率这三个指标整合到评估矩阵中，以衡量每种成像条件下SR图像的质量。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408101923226.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408101923745.png"></p><h2 id="深傅立叶通道注意网络-RCAN-DFCAN"><a href="#深傅立叶通道注意网络-RCAN-DFCAN" class="headerlink" title="深傅立叶通道注意网络(RCAN,DFCAN)"></a>深傅立叶通道注意网络(RCAN,DFCAN)</h2><ol><li><span style="color: red;">设计了DFCAN和DFGAN，它们利用了傅里叶域中不同特征映射的功率谱特征。在每个残差块中，傅里叶通道注意(FCA)机制(图2a)使网络能够根据其功率谱中包含的所有频率分量的综合贡献自适应地重新缩放每个特征映射。相反，空间通道注意(SCA)机制仅利用特征映射的平均强度，相当于零频率(即直流分量)来计算重新缩放因子。</span></li><li>为了测试FCA在SISR任务中是否优于SCA，我们分别在ResNet5、U-net18和DenseNet19的不同网络架构中实现了FCA和SCA机制。我们发现基于fca的网络通常比基于SCA的相应网络提供更少的NRMSE，并且更精确地推断出精细结构(扩展数据图4和方法)。此外，我们证明，尽管DFCAN的参数比RCAN少7倍(补充表1)，但DFCAN更精确地解析f -肌动蛋白细胞骨架的密集交叉区域(图2b)，并且DFCAN图像中的f -肌动蛋白的线扫描轮廓比RCAN图像中的轮廓更接近tg - sim的轮廓(图2c)。统计分析量化了DFCAN在所有三个指标方面通常具有更好的DLSR成像性能，特别是对于高结构复杂性的标本，例如MTs和F-actin(图2d)。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408101935998.png"></p><h2 id="BioSR数据集"><a href="#BioSR数据集" class="headerlink" title="BioSR数据集"></a>BioSR数据集</h2><p>BioSR数据集由广泛的LR-SR图像对组成，涵盖了广泛的信噪比水平，结构复杂性和升级因子。</p><p>链接：<a href="https://figshare.com/articles/dataset/BioSR/13264793">BioSR数据集</a></p><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><p>无</p><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Evaluation and development of deep neural networks for image super-resolution in optical microscopy</p><p>光学显微镜图像超分辨深度神经网络的评价与发展</p><p>期刊：Nature Methods(一区)</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Chang Qiao(清华大学自动化系)</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2021年2月</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年8月10日—2024年8月12日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>地址：<a href="https://github.com/qc17-THU/DL-SR">https://github.com/qc17-THU/DL-SR</a></p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> SIM </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>可见光远场超透镜的二维成像</title>
      <link href="/2024/08/09/%E5%8F%AF%E8%A7%81%E5%85%89%E8%BF%9C%E5%9C%BA%E8%B6%85%E9%80%8F%E9%95%9C%E7%9A%84%E4%BA%8C%E7%BB%B4%E6%88%90%E5%83%8F/"/>
      <url>/2024/08/09/%E5%8F%AF%E8%A7%81%E5%85%89%E8%BF%9C%E5%9C%BA%E8%B6%85%E9%80%8F%E9%95%9C%E7%9A%84%E4%BA%8C%E7%BB%B4%E6%88%90%E5%83%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>新的超材料远场超透镜理论上可以通过旋转超材料FSL获得二维(2D)亚衍射限制图像。</li><li>这种由金属介电多层和一维亚波长光栅组成的超材料远场超透镜本质上可以在很宽的可见波长范围内工作。</li><li>二维亚衍射极限成像能力使远场超透镜在光学纳米成像和传感方面有了更多的应用。</li></ol><h2 id="通过旋转FSL获得二维衍射限制图像"><a href="#通过旋转FSL获得二维衍射限制图像" class="headerlink" title="通过旋转FSL获得二维衍射限制图像"></a>通过旋转FSL获得二维衍射限制图像</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408091602397.png"></p><ol><li>图1a中，红色区域表示二维物体的传播带，可以在远场检测到。(这里我们只考虑一半的传播带。另一半的情况也可以类似处理。)插图是超材料FSL的原理图，它由金属介电多层和一维亚波长金属介电光栅组成。</li><li>当亚波长光栅的周期方向沿x方向时，如图1a所示，光栅只提供Kx方向的波矢。因此，FSL后的传播波有三个贡献:入射波的0阶、-1阶和+1阶衍射波，其矢量分别在图1a的红色、绿色和黄色区域内。(入射波是在FSL之前的平面上的二维物体的散射波。)</li><li>由于衍射效率小，忽略了较高的衍射阶。在FSL的设计中，经过FSL后的绕射波的传播部分主要来自入射波与图1a中绿色区域内的波矢量的-1阶衍射。然后，通过测量FSL后传播波的频谱，可以检索到图1a中绿色区域内的入射波的信息。</li><li>为了重建二维高分辨率图像，需要各个方向的信息。自然，<span style="color: red;">一个直接的解决方案是旋转FSL以获得所有方向的信息。例如，可以将FSL旋转30°(沿着图1a中的z轴)，然后可以从FSL之后在远场测量的传播波的频谱中检索到另一个绿色区域的信息。通过累积FSL六个方向的测量，可以得到如图1b所示的绿色环中的信息(在每次测量中，当考虑两个半传播带时，都会检索到两个对称绿色区域中的信息)。</span>通过传统光学显微镜的反射方式，可以获得图1b中红色区域内入射波的传播信息。结果，从检索到的光谱中可以重建出二维亚衍射限制图像。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="主流的显微镜技术"><a href="#主流的显微镜技术" class="headerlink" title="主流的显微镜技术"></a>主流的显微镜技术</h2><ol><li>近场扫描光学显微镜(NSOM)通过扫描物体近场的尖锐尖端并收集倏逝波信息来突破衍射极限。</li><li>受激发射耗尽(STED)显微镜是利用受激发射锐化荧光焦点来克服衍射极限的另一种技术，NSOM与STED都需要耗时的逐点扫描过程，这阻碍了它们的实时成像。</li><li>非线性结构照明显微镜成像速度较快，但对材料的非线性响应要求较高。</li></ol><h2 id="光学远场超透镜-FSL"><a href="#光学远场超透镜-FSL" class="headerlink" title="光学远场超透镜(FSL)"></a>光学远场超透镜(FSL)</h2><ol><li>光学远场超透镜(FSL)，在远场中成功地展示了一维(1D)亚衍射限制图像。<span style="color: red;">FSL由银板和一维亚波长光栅组成。银板增强了倏逝波，其亚波长光栅将增强的倏逝波转换为传播波，从而在远场收集高空间频率信息，重建亚衍射受限图像。</span></li></ol><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Two-Dimensional Imaging by Far-Field Superlens at Visible Wavelengths</p><p>可见光远场超透镜的二维成像</p><p>期刊：Nano Letters(一区TOP)</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Yi Xiong</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>时间：2007年10月</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>时间：2024年8月9日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>无</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 超透镜 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像超分辨率残差密集网络(RDN)</title>
      <link href="/2024/08/08/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%AE%8B%E5%B7%AE%E5%AF%86%E9%9B%86%E7%BD%91%E7%BB%9C/"/>
      <url>/2024/08/08/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%AE%8B%E5%B7%AE%E5%AF%86%E9%9B%86%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="论文主要贡献"><a href="#论文主要贡献" class="headerlink" title="论文主要贡献"></a>论文主要贡献</h2><ol><li>提出了残差密集网络(RDN)。该网络充分利用了原始低分辨率(LR)图像的所有分层特征。</li><li>对于一个非常深的网络来说，直接提取LR空间中每个卷积层的输出是非常困难和不切实际的。因此提出残差密集块(RDB)作为RDN的构建模块，RDB由密集连接层和带有局部残差学习的局部特征融合(LFF)组成。我们的RDB还支持RDB之间的连续内存。一个RDB的输出可以直接访问下一个RDB的每一层，从而产生连续的状态传递。RDB中的每个卷积层都可以访问所有后续层，并传递需要保留的信息。LFF将当前RDB的前一层状态与当前RDB的所有前一层状态连接起来，通过自适应保留信息提取局部密集特征。此外，LFF通过稳定更广泛的网络的训练，允许非常高的增长率。在提取多层次局部密集特征后，进一步进行全局特征融合(global feature fusion, GFF)，从全局角度自适应地保留层次特征。</li><li>RDB中的局部特征融合的使用可以减少多余的特征，使网络训练更稳定。在充分得到稠密的局部特征后，采用全局特征融合的方法对全局多层次特征进行联合自适应学习。在具有不同退化模型的基准数据集上的实验表明，与现有的state-of-the art方法相比，RDN取得了更佳的性能。</li><li>提出了全局特征融合来自适应融合LR空间中所有RDBs的层次特征。通过全局残差学习，我们将浅特征和深特征结合在一起，得到来自原始LR图像的全局密集特征。</li></ol><h2 id="现有的SR方法"><a href="#现有的SR方法" class="headerlink" title="现有的SR方法"></a>现有的SR方法</h2><p>单图像超分辨(SISR)旨在通过低分辨图像(LR)，生成视觉效果更好的高分辨图像(HR)，实际上SR是一个病态的问题，因为对于任何LR输入都有多种解决方案(一对多)。已有的SR方法主要有三种：</p><ul><li>基于插值</li><li>基于重建</li><li>基于学习</li></ul><h3 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h3><ul><li>Dong 等人在ECCV2014提出的SRCNN(DL应用在SR领域的开山之作)。</li><li>Kim 等人根据SRCNN的不足在CVPR2016分别提出了VDSR(增大感受野、残差学习和高学习率、mutil-scale)和DRCN(递归)，使网络更好训练。</li><li>Lim 等人在CVPR2017中，使用残差块构建了EDSR(very wide)和MDSR(very deep)，该方法还赢得了当年CVPR Workshops的超分辨比赛——NTIRE2017的冠军。</li><li>Tai 等人提出了包含递归单元和门控单元的记忆模块，构建了MemNet。</li></ul><h3 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h3><ol><li>尽管MemNet中的门控单元可以提供短期记忆，但是局部卷积层不能直接访问后续层，很难说记忆模块充分利用了里面所有层的信息，没有充分利用每一个卷积层中的信息。</li><li>忽视了多层次特征，图像中的物体有不同的尺寸、视角、纵横比，一个非常深的网络的多层次特征（hierarchical features）能为重建提供更多线索。然而，大多数当前的DL方法（VDSR、LapSRN、EDSR等）都忽视了这种多层次特征。</li></ol><h2 id="RDN网络结构"><a href="#RDN网络结构" class="headerlink" title="RDN网络结构"></a>RDN网络结构</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408081516547.png"></p><ol><li>RDN主要由四个部分组成:浅层特征提取网络(SFENet，开始的两个卷积层)、独立密集块(RDB)、密集特征融合(DFF)和上采样网络(UPNet)。</li></ol><h3 id="RDB块-Residual-Dense-Block"><a href="#RDB块-Residual-Dense-Block" class="headerlink" title="RDB块(Residual Dense Block)"></a>RDB块(Residual Dense Block)</h3><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408081521657.png"></p><ol><li>残差密集块RDB &#x3D; 密集连接层 + 局部特征融合(LFF) + 局部残差，形成了连续记忆机制(CM，Contiguous Memory)。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="DIV2K数据集"><a href="#DIV2K数据集" class="headerlink" title="DIV2K数据集"></a>DIV2K数据集</h2><p>Timofte等人发布了用于图像恢复应用的高质量(2K分辨率)数据集DIV2K[27]。DIV2K由800张训练图像、100张验证图像和100张测试图像组成。</p><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Residual Dense Network for Image Super-Resolution</p><p>图像超分辨率残差密集网络</p><p>期刊：CVPR(顶会)</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Yulun Zhang</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>时间：2018年3月</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年8月8日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>地址：<a href="https://github.com/yulunzhang/RDN">https://github.com/yulunzhang/RDN</a></p><p>相关文章：<a href="https://blog.csdn.net/weixin_41804998/article/details/108363229">【论文笔记1_超分辨】（RDN）Residual Dense Network for Image Super-Resolution</a></p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 超分辨率成像 </tag>
            
            <tag> RDN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>利用极深残差通道注意网络的图像超分辨率(RCAN)</title>
      <link href="/2024/08/03/%E5%88%A9%E7%94%A8%E6%9E%81%E6%B7%B1%E6%AE%8B%E5%B7%AE%E9%80%9A%E9%81%93%E6%B3%A8%E6%84%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/"/>
      <url>/2024/08/03/%E5%88%A9%E7%94%A8%E6%9E%81%E6%B7%B1%E6%AE%8B%E5%B7%AE%E9%80%9A%E9%81%93%E6%B3%A8%E6%84%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><h2 id="本文主要贡献"><a href="#本文主要贡献" class="headerlink" title="本文主要贡献"></a>本文主要贡献</h2><ol><li><p>提出了深度残差通道注意网络(RCAN)来实现高精度的图像SR。</p></li><li><p>提出了由几个具有长跳跃连接的残差组构成的residual in residual（RIR）结构来构建深度网络。每个残差组中包含一些具有短跳跃连接的残差块。整个RIR结构通过运用多个跳跃连接，让低频信息绕过网络，使主干网络只学习到高频信息。</p></li><li><p>提出了通道注意(CA)机制，通过考虑特征通道之间的相互依赖性，来自适应地调整不同通道的权重。</p></li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408031005075.png" alt="主流的超分辨率方法"></p><h2 id="RCAN网络结构"><a href="#RCAN网络结构" class="headerlink" title="RCAN网络结构"></a>RCAN网络结构</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408031018055.png"></p><ol><li><p>网络由四个模块构成：</p><p>(1).浅层特征提取：由1个卷积层<code>head</code>构成：<code>conv(args.n_colors, n_feats, kernel_size)</code>；</p><p>(2).深层特征提取：即文章提出的RIR结构；</p><p>(3).上采样：上采样用的是PixelShuffle；</p><p>(4).重构部分：用1个卷积层把通道数恢复到输入图片的通道数：<code>conv(n_feats, args.n_colors, kernel_size)</code>；</p></li><li><p>从上面的结构图中可以看出，residual in residual（RIR）结构的最外层由G个残差组以及一个长跳跃连接构成，从而形成了一个粗粒度的残差学习。在每一个残差组的内部，则是由B个残差通道注意力块（RCAB）以及一个小的跳跃连接构成。简单来说，这个residual in residual就是大残差内部再套娃小残差。</p></li><li><p><span style="color: red;">长跳跃连接可以使网络在更加粗粒度的层次上学习到残差信息。而短跳跃连接则是一种细粒度的identity-based的跳跃连接，使得大量网络不需要的低频信息得到过滤</span>。</p></li><li><p>为了进一步实现自适应的辨别学习（discriminative learning），<span style="color: red;">作者提出了通道注意力机制（CA）并在RCAB中进行了运用，其目的是给更有价值的通道更高的权重。</span></p></li></ol><h3 id="通道注意力机制"><a href="#通道注意力机制" class="headerlink" title="通道注意力机制"></a>通道注意力机制</h3><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408031033823.png"></p><ol><li><p>为了使网络专注于更多信息特征，我们利用特征通道之间的相互依赖性，从而产生通道关注(CA)机制。</p></li><li><p>LR空间中的信息具有丰富的低频和有价值的高频成分。低频部分似乎更平稳。高频成分通常是充满边缘、纹理和其他细节的区域。</p></li><li><p>HGP(·)为全局池化函数。</p></li></ol><h2 id="测试集效果"><a href="#测试集效果" class="headerlink" title="测试集效果"></a>测试集效果</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408031355235.png"></p><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Image Super-Resolution Using Very Deep Residual Channel Attention Networks</p><p>​            利用极深残差通道注意网络的图像超分辨率</p><p>期刊：ECCV(顶会)</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Yulun Zhang</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>时间：2018年10月6日</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年8月3日</p><h2 id="开源代码及其它"><a href="#开源代码及其它" class="headerlink" title="开源代码及其它"></a>开源代码及其它</h2><p>地址：<a href="https://github.com/yulunzhang/RCAN">https://github.com/yulunzhang/RCAN</a></p><p>其他博主的笔记：<a href="https://blog.csdn.net/weixin_41804998/article/details/109209721">【论文笔记4_超分辨】（RCAN）Image Super-Resolution Using Very Deep Residual Channel Attention Networks</a></p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 超分辨率成像 </tag>
            
            <tag> RCAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>通过注意力通道生成对抗网络的三维结构照明显微镜</title>
      <link href="/2024/08/02/%E9%80%9A%E8%BF%87%E6%B3%A8%E6%84%8F%E5%8A%9B%E9%80%9A%E9%81%93%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%89%E7%BB%B4%E7%BB%93%E6%9E%84%E7%85%A7%E6%98%8E%E6%98%BE%E5%BE%AE%E9%95%9C/"/>
      <url>/2024/08/02/%E9%80%9A%E8%BF%87%E6%B3%A8%E6%84%8F%E5%8A%9B%E9%80%9A%E9%81%93%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%89%E7%BB%B4%E7%BB%93%E6%9E%84%E7%85%A7%E6%98%8E%E6%98%BE%E5%BE%AE%E9%95%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><ol><li>提出了残差网络与通道注意力结合的生成对抗网络(caGAN)，提高低信噪比(SNR)条件下3D-SIM的重建质量，能使用更少的原始图像进行重建。</li><li>利用活细胞中微管和溶酶体之间的动态相互作用的例子，证明了caGAN-SIM在各种亚细胞结构中的优越性能及其在长期多色3D超分辨率成像中的能力。</li></ol><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408021419063.png"></p><h2 id="3D-SIM理论部分"><a href="#3D-SIM理论部分" class="headerlink" title="3D-SIM理论部分"></a>3D-SIM理论部分</h2><ol><li>三维(3D)结构照明显微镜(SIM)具有双倍的横向和轴向分辨率和光学切片能力(与宽视场(WF)荧光显微镜相比)，与其他体积成像方式(如光片显微镜)相比，3D-SIM具有更多的光漂白和光毒性，因为它每个轴向切片需要15张原始图像，阻碍了其在活细胞成像中的广泛应用。(每个原始3D-SIM图像是五种不同频率成分的混合物，为了分离它们，使用相同横向照明模式的五个不同阶段获得五张原始图像。此外，为了使横向分辨率加倍，是各向异性的，使用了三个方向，因此是15张图片，3D-SIM需要至少数百次曝光和典型的每体积数十秒的采集时间，导致严重的光漂白，光毒性，成像速度低，从而使其与长期活细胞成像不兼容)</li><li>二维(2D)结构照明显微镜(SIM)在细胞内动力学的超分辨率活细胞成像中脱颖而出，因为2D-SIM只需要9张原始图像来重建SR图像，从而实现高成像速度和低光子预算。</li><li>传统的SIM算法通过在傅里叶域中进行波段分离和重组，重建出超分辨图像体，分别达到100 nm和300 nm的横向和轴向分辨率。</li><li>在傅里叶域中实现通道注意机制优于在空间域中，但傅里叶变换的多次运算需要大量的计算资源和时间，在处理三维数据时会更加严重。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408021441210.png"></p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202408021442579.png"></p><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：3D Structured Illumination Microscopy via Channel Attention Generative Adversarial Network</p><p>​            通过通道注意生成对抗网络的三维结构照明显微镜</p><p>期刊：IEEE JOURNAL OF SELECTED TOPICS IN QUANTUM ELECTRONICS(二区)</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Chang Qiao (清华大学自动化系)</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2021年2月22日</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年8月2日</p><h2 id="开源代码"><a href="#开源代码" class="headerlink" title="开源代码"></a>开源代码</h2><p>地址：<a href="https://github.com/qc17-THU/caGAN-SIM">https://github.com/qc17-THU/caGAN-SIM</a></p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> SIM </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于深度学习的超透镜亚衍射极限图案的超分辨率成像</title>
      <link href="/2024/07/29/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%B6%85%E9%80%8F%E9%95%9C%E4%BA%9A%E8%A1%8D%E5%B0%84%E6%9E%81%E9%99%90%E5%9B%BE%E6%A1%88%E7%9A%84%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%88%90%E5%83%8F/"/>
      <url>/2024/07/29/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%B6%85%E9%80%8F%E9%95%9C%E4%BA%9A%E8%A1%8D%E5%B0%84%E6%9E%81%E9%99%90%E5%9B%BE%E6%A1%88%E7%9A%84%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%88%90%E5%83%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><ol><li>通过时域有限差分法(FDTD)仿真生成随机样本的近场数据。然后，利用傅里叶光学得到远场衍射图。采用深度学习方法从远场强度重建近场强度。</li><li>利用条件生成对抗网络(cGAN)在带宽非相干照明下获得任意纳米尺度图案的超分辨率图像。</li></ol><h2 id="超透镜理论"><a href="#超透镜理论" class="headerlink" title="超透镜理论"></a>超透镜理论</h2><ol><li><p>超透镜是一种基于超材料的透镜，它可以利用表面等离子体现象增强纳米级物体产生的倏逝波。</p></li><li><p>局域等离子体结构照明显微镜是一种利用超透镜增强的倏逝波随照明位移成像的超分辨率显微镜。</p></li><li><p>传统光学显微镜的分辨率受限于衍射极限，原因是高频空间信息（例如样品的细小结构）会以倏逝波（evanescent waves）的形式存在，这些波在传播过程中迅速衰减，无法被普通透镜收集到。超透镜通过使用负折射率材料，可以增强这些倏逝波，使它们重新传播，从而捕获更多的高频信息，实现比衍射极限更高的分辨率。</p></li><li><p>超透镜是一种将亚波长光栅集成到银薄膜中的装置。远场超透镜(FSL)选择性地放大近场亚波长物体的倏逝波，将倏逝波转换为传播波，并被物镜收集。</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202407291402212.png"></p><ol start="5"><li><p>周期光栅可以通过衍射过程将倏逝波耦合为传播波，公式如下：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202407291408055.png"></p></li><li><p>透射光栅上方和透射前的电场角谱关系表达式：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202407291417238.png"></p></li><li><p>周期物体的投影远场条纹可以用下面的公式来解释：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202407291429026.png"></p></li></ol></li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="衍射极限"><a href="#衍射极限" class="headerlink" title="衍射极限"></a>衍射极限</h2><ol><li>衍射极限（Diffraction Limit）是光学成像系统中的一个基本物理限制，主要由物镜的数值孔径（Numerical Aperture, NA）和倏逝波（evanescent wave）无法传播的性质决定的。(衍射极限是物镜的数值孔径和携带样品高频信息的倏逝分量的不传播问题所造成的物理约束)</li><li>d &#x3D; λ&#x2F;2NA，数值孔径由物镜的孔径和光在介质中的折射率决定。较大的数值孔径和较短的波长可以提高分辨率。</li></ol><h2 id="零均值归一化互相关系数-ZNCC"><a href="#零均值归一化互相关系数-ZNCC" class="headerlink" title="零均值归一化互相关系数(ZNCC)"></a>零均值归一化互相关系数(ZNCC)</h2><ul><li>零均值归一化互相关(Zero Mean Normalized Cross-Correlation, ZNCC)值用于证明生成的数据(通过训练后的网络重建)与真实空间中真实值的相似度。</li><li>ZNCC用于评估两个图像块（通常是模板和搜索区域）之间的相似度，通过消除亮度和对比度的影响来提高匹配的鲁棒性。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202407291351074.png"></p><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Super‑Resolution Imaging of Sub‑diffraction‑Limited Pattern with Superlens Based on Deep Learning</p><p>​            基于深度学习的超透镜亚衍射极限图案的超分辨率成像</p><p>期刊：Regular Paper(二区)</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Yizhao Guan</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2024年3月5日</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年7月29日</p><h2 id="开源代码"><a href="#开源代码" class="headerlink" title="开源代码"></a>开源代码</h2><p>无</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 超透镜 </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于深度学习的光学像差估计实现离线数字自适应光学和超分辨率成像</title>
      <link href="/2024/07/26/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%85%89%E5%AD%A6%E5%83%8F%E5%B7%AE%E4%BC%B0%E8%AE%A1%E5%AE%9E%E7%8E%B0%E7%A6%BB%E7%BA%BF%E6%95%B0%E5%AD%97%E8%87%AA%E9%80%82%E5%BA%94%E5%85%89%E5%AD%A6%E5%92%8C%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%88%90%E5%83%8F/"/>
      <url>/2024/07/26/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%85%89%E5%AD%A6%E5%83%8F%E5%B7%AE%E4%BC%B0%E8%AE%A1%E5%AE%9E%E7%8E%B0%E7%A6%BB%E7%BA%BF%E6%95%B0%E5%AD%97%E8%87%AA%E9%80%82%E5%BA%94%E5%85%89%E5%AD%A6%E5%92%8C%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E6%88%90%E5%83%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="论文主体思路"><a href="#论文主体思路" class="headerlink" title="论文主体思路"></a>论文主体思路</h1><ol><li>首先设计了空间频率编码网络(SFE-Net)，可直接从单个显微镜图像中提取有像差的PSF，并且利用估计的PSF通过反卷积算法可以基本消除相应的像差。</li><li>设计了空间特征转换引导(SFT)深傅立叶通道注意网络(SFT-DFCAN)。利用SFE-Net估计的PSF输入到SFT-DFCAN模型。证明了SFE-Net和SFT-DFCAN的结合可以实现活细胞成像的即时数字AO和光学像差感知超分辨率重建。</li><li>提出了一种新的空频编码网络（SFE-Net），它可以直接从生物图像中估计像差点扩展函数（psf），从而实现快速、高精度的光学像差估计，而无需额外的光学器件和图像采集。结果表明，利用估计的psf，通过反卷积算法可以计算地去除光学像差。</li></ol><h2 id="自适应光学-AO"><a href="#自适应光学-AO" class="headerlink" title="自适应光学(AO)"></a>自适应光学(AO)</h2><ol><li>AO的实现一般包括两个主要部分：像差检测和像差校正。</li><li>为了测量光学或样品引起的像差，开发了直接和间接波前传感方法。直接波前传感方法利用专用波前传感器，主要是Shack-Hartmann传感器，以及用于像差检测的额外光路。相比之下，间接波前传感方法在没有特定波前传感器的情况下表征像差，而是通过区域或模态方法通过重复采集计算确定像差。</li></ol><h2 id="深度学习部分"><a href="#深度学习部分" class="headerlink" title="深度学习部分"></a>深度学习部分</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ul><li>使用BioSR(<a href="https://figshare.com/articles/dataset/BioSR/13264793">开源数据集</a>)的GT-SIM图像作为生物荧光标本。根据光学成像模型，这些图像被有意地降级，可以表示为：</li></ul><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411200955935.png"></p><p>其中I为光学成像系统捕获的像差宽视场（WF）图像；S为生物标本，即GT-SIM图像；*表示卷积算子；NPoisson(.)表示泊松重构；G(0， σ2)为均值为0，方差为σ2的高斯白噪声；PSFZernike为像差点扩散函数，其瞳孔函数由Zernike多项式4-18 （Wyant排序）加权求和构造。这些函数可以用数学公式表示如下：</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411201000990.png"></p><p>式中，Zn和an分别表示泽尼克多项式和n阶系数。A(.)为圆形apodiization函数，其半径由发射波长和探测数值孔径（NA）决定。F−1{.}表示快速傅里叶反变换算子。</p><p>每一阶的系数an随机抽样于均值为零、标准差为0.125的正态分布。我们为所有采样an设置了[−1,1 ]的上限和下限，以避免可能破坏训练过程稳定的极高或极低的值。由于a服从正态分布N(0, 0.125)，边界较松，因此产生的像差的均方根（RMS）近似服从重新标化的chi分布，其可表示为:</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411201002424.png"></p><p>其中λ为发射波长。因此，训练和测试数据集的总均方根范围为[0,7.38 λ]。</p><ul><li>在训练过程中，将像差PSF图像PSFZernike作为PSF估计网络模型（SFE-Net）的目标，将GT-SIM图像S作为SFT-DFCAN等单图像超分辨率（SISR）网络模型的目标。</li></ul><h3 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h3><h4 id="空间频率编码网络-SFE-Net"><a href="#空间频率编码网络-SFE-Net" class="headerlink" title="空间频率编码网络(SFE-Net)"></a>空间频率编码网络(SFE-Net)</h4><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202407261402161.png" alt="图1"></p><ol><li>SFE-Net的架构如图所示，由双支路编码器和基于unet的解码器组成。</li><li><font color="RED">编码器网络由空间分支(SB)和频率分支(FB)两个并行分支组成，分别在空间和频率域提取深度特征。</font>在这两个分支中，都采用了改进的残差通道注意网络(RCAN)，该网络有4个残差组*4个残差通道注意块(residual channel attention block)。FB分支经快速傅里叶变换层后，依次进行模算子和对数算子，从而将图像特征编码到傅里叶域中。</li><li>该解码器主要由U-net特征提取器和降阶模块两部分组成。我们采用了一个比较深的U-net模型[11]，它以一个双卷积块（图1(c)）开始，然后是5个低尺度块（图1(d)）和5个高尺度块（图1(e)），其中5个跳跃连接桥接相同尺度的特征。在每个降尺度块中，使用最大池化层和双卷积块进行降尺度和特征提取。在Up模块中，<font color="RED">使用Pixel Shuffle来提升特征通道。</font>(Pixel Shuffle的核心思想是将低分辨率特征图中的多个通道的数值重新排列到高分辨率图像的空间位置上)。</li><li>U-Net网络的输出到downscale模块中，该模块由两个2x向下洗牌层和四个卷积- relu块组成。在每个卷积- relu块中，将卷积层的stride参数设置为2，使downscale模块能够将输入的132 × 132像素的特征映射转换为33 × 33像素的PSF图像。</li></ol><h4 id="空间特征变换引导的深傅立叶通道注意网络-SFT-DFCAN"><a href="#空间特征变换引导的深傅立叶通道注意网络-SFT-DFCAN" class="headerlink" title="空间特征变换引导的深傅立叶通道注意网络(SFT-DFCAN)"></a>空间特征变换引导的深傅立叶通道注意网络(SFT-DFCAN)</h4><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202407261439904.png" alt="图2"></p><ol><li>SFT-DFCAN的整体架构如图2(a)所示，它是根据我们之前提出的最先进的DLSR模型DFCAN[9]进行修改的，DFCAN[9]经过训练可以直接将WF图像转换为SR图像。</li><li>这里，为了将PSF和像差信息传递给图像SR处理，受SFTMD模型[12]的启发，我们将原始的傅立叶通道注意块（FCAB）更新为空间特征转换引导的FCAB (SFT-FCAB)，该FCAB可以利用嵌入的PSF信息自适应地重新缩放SFT-FCAB中的空间特征。具体来说，我们使用主成分分析将PSF投影到维度为b的线性空间上。然后将该投影的PSF拉伸成大小为b × H × W的PSF嵌入，该嵌入作为每个SFT-FCAB的输入。在每个SFT-FCAB中，PSF嵌入通过两个con - relu块与生物结构的空间特征映射相结合，对输入特征映射进行缩放和移位。随后，实现FCA层[图2(c)]进行深度特征提取和聚合。最后，重构的SR图像由上采样模块生成，该上采样模块依次由卷积- gelu块[13]、像素shuffle层[14]和最后的卷积层组成。</li></ol><h3 id="SFE-Net的训练"><a href="#SFE-Net的训练" class="headerlink" title="SFE-Net的训练"></a>SFE-Net的训练</h3><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411201042682.png"></p><ol><li><p>对于SFE-Net的训练，我们在每次迭代中随机生成对像差WF （132 × 132像素）及其对应的PSF （33 × 33像素）。(1)和(2)利用多个生物标本的~ 200张原始GT-SIM图像，包括空心网格蛋白包覆坑（CCPs）、内质网（ER）和交叉微管（MTs），以赋予训练模型良好的泛化能力。SFE-Net的整体数据增强工作流程和训练过程如图3所示。对于SFT-DFCAN等SISR模型，我们在每次迭代中随机生成像差WF图像（132 × 132像素）、真地psf图像（33 × 33像素）和相应的GTSIM图像（264 × 264像素）的三组图像作为训练数据集。SFE-Net和SISR模型的目标函数都定义为均方误差，它量化了网络输出与目标图像之间的差异。</p></li><li><p>在训练过程中，我们使用初始学习为5 × 10−5的Adam优化器。SFT-Net的学习率在每~ 10,000个小批量迭代后衰减0.5倍，而SFT-DFCAN的学习率遵循余弦退火计划，每12500个小批量迭代重新启动。我们对SFE-Net和SFT-DFCAN分别采用了4和8个批大小。通常情况下，SFE-Net和SFT-DFCAN的总训练迭代次数为15万次和50万次，在RTX 3090 GPU下分别耗时约16小时和30小时。</p><p>在推理阶段，SFE-Net通过将输入图像（512 × 512）分割成若干块来捕获光学像差的空间变化，通常需要不到1.5 s（单个图像patch为30 ms）来生成PSF矩阵（7 × 7 × 33 × 33）。以WF图像和估计的PSF作为输入，训练良好的SFT-DFCAN模型可以在1 s内重建出1024 × 1024像素的无像差SR图像。</p></li></ol><h3 id="验证SFE-Net网络能高精度估计像差PSF"><a href="#验证SFE-Net网络能高精度估计像差PSF" class="headerlink" title="验证SFE-Net网络能高精度估计像差PSF"></a>验证SFE-Net网络能高精度估计像差PSF</h3><ul><li><p>与其他模型估计的PSF进行比较说明SFE-Net网络的高精度性。</p><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202407261504253.png"></p></li></ul><h3 id="PSF和光学像差的先验知识可以提高SISR的性能"><a href="#PSF和光学像差的先验知识可以提高SISR的性能" class="headerlink" title="PSF和光学像差的先验知识可以提高SISR的性能"></a>PSF和光学像差的先验知识可以提高SISR的性能</h3><p><img src="https://cdn.jsdelivr.net/gh/YuJiang-LongHoliday/Blog_Picture/Article/202411201059595.png"></p><ol><li>利用估计的PSF进行像差感知图像的超分辨率重建。(a) DFCAN和SFT-DFCAN利用KernelGAN、IKC、MANet和SFE-Net获得的psf重构的代表性SR图像。提供了低分辨率图像和高分辨率GT图像供参考。在每张重建的SR图像的右上角显示相应的估计PSF图像。比例尺、1 μm、0.5 μm（放大区域）。(b) DFCAN和SFT-DFCAN输出的SR图像的PSNR值与KernelGAN、IKC、MANet和SFE-Net估计的psf值的统计比较（n  30）。</li><li><font color="RED">SISR网络已经被开发出来，以端到端方式即时增强生物图像的分辨率，而不考虑图像形成模型[9]。最近的研究表明，结合物理先验知识，如PSF，可以提高超分辨率网络的性能。鉴于所提出的SFE-Net从低分辨率图像中识别PSF的卓越能力，我们认为结合PSF和光学像差的先验知识可以提高SISR的性能。</font>为了验证这一假设，我们将SFT层[12]与我们之前提出的DFCAN模型[9]结合起来设计了SFT-DFCAN，该模型利用了像差的PSF信息和傅里叶通道注意机制来增强图像超分辨率的性能。</li><li>特别是在SFT-DFCAN中，特征映射是仿射变换的，通过缩放和移位操作，这些操作取决于估计的PSF和像差，从而使PSF信息能够自适应编码到神经网络中。</li></ol><h1 id="附录与补遗"><a href="#附录与补遗" class="headerlink" title="附录与补遗"></a>附录与补遗</h1><h2 id="光学像差"><a href="#光学像差" class="headerlink" title="光学像差"></a>光学像差</h2><ol><li>光学像差降低了荧光显微镜的性能。</li><li>传统的自适应光学(AO)利用特定的设备(额外的硬件或复杂的成像程序)从而导致高成本和低采样速率，如Shack-Hartmann波前传感器和变形镜，来测量和纠正光学像差。(自适应光学(AO)包括两个主要部分:像差检测和像差校正。)</li><li>理想的光学成像依赖于激发光的高质量聚焦和荧光样品发射光的精确检测。显微镜中的光学元件和所研究的生物样品都会引入像差，从而导致分辨率下降，荧光光子损失和信号与背景比(SBR)恶化等。例如，成像系统中光学元件的光学制造缺陷或不对准会造成一定的像差，如球面像差和彗差，而生物标本的折射率不均匀性会带来更复杂的像差。</li><li><font color="red">高数值孔径(NA)的显微镜，特别是超分辨率显微镜，对像差更敏感，因为高NA的物镜更容易受到高阶像差的影响。</font></li><li>对于一个完善的光学成像系统，在活细胞成像实验中最常见的像差是离焦和球差。这些像差通常是由聚焦平面的漂移、样品的轴向运动以及样品与盖滑移之间的折射率不对准引起的。</li></ol><h2 id="PSF"><a href="#PSF" class="headerlink" title="PSF"></a>PSF</h2><ol><li><font color="red">在光学成像系统中，图像质量和像差通常由它们的点扩展函数(PSF)来表征，它隐含地编码在显微镜图像的任何标本斑块中。</font></li><li>显微图像PSF的盲估计技术进展有限。原因是双重的。首先，由于复杂的光学系统和样品散射，生物成像遇到的光学像差比商用相机摄影严重得多。其次，直接从生物图像中估计像差PSF本质上是一个不适定问题，使得它在直觉上是不可行的。尽管如此，基于图像的PSF和光像差估计对生物成像有很大的好处，它消除了AO系统中对波前传感器的需求，同时促进了数字像差校正和像差感知的图像超分辨率重建。</li><li>为了解决上述问题，我们首先探索了几种有代表性的有监督或无监督核估计算法来估计生物图像的核，即像差PSF。这些算法包括无监督核生成对抗网络（KernelGAN）[15,18]，迭代核校正（IKC）[12]，以及用于空间变异核估计的监督互仿射网络（MANet）[17]。</li></ol><h1 id="关于文献"><a href="#关于文献" class="headerlink" title="关于文献"></a>关于文献</h1><h2 id="题目及期刊"><a href="#题目及期刊" class="headerlink" title="题目及期刊"></a>题目及期刊</h2><p>题目：Deep Learning-based Optical Aberration Estimation Enables Offline Digital Adaptive Optics and Super-resolution Imaging</p><p>​            基于深度学习的光学像差估计实现离线数字自适应光学和超分辨率成像</p><p>期刊：Photonics Research</p><h2 id="作者信息"><a href="#作者信息" class="headerlink" title="作者信息"></a>作者信息</h2><p>作者：Chang Qiao(清华大学自动化系)</p><h2 id="发表时间"><a href="#发表时间" class="headerlink" title="发表时间"></a>发表时间</h2><p>日期：2023年11月1日</p><h2 id="阅读时间"><a href="#阅读时间" class="headerlink" title="阅读时间"></a>阅读时间</h2><p>日期：2024年7月26日-2024年7月27日</p><p>2024年11月20日重读</p><h2 id="开源代码"><a href="#开源代码" class="headerlink" title="开源代码"></a>开源代码</h2><p><a href="https://github.com/HypnosRin/SFE-Net">https://github.com/HypnosRin/SFE-Net</a> (作者)</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 光学像差 </tag>
            
            <tag> U-Net </tag>
            
            <tag> DFCAN </tag>
            
            <tag> 傅里叶 </tag>
            
            <tag> 物理先验 </tag>
            
            <tag> 点扩散函数(PSF) </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
